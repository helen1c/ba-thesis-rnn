{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(object):\n",
    "    \n",
    "    def forward(self, X_in):\n",
    "        return np.tanh(X_in)\n",
    "    \n",
    "    def backward(self, X_in):\n",
    "        #dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(X_in))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu(object):\n",
    "    \n",
    "    def forward(self, X_in):\n",
    "        return np.maximum(X_in, 0)\n",
    "    \n",
    "    def backward(self, X_in):\n",
    "        dYdX = (X_in > 0)  \n",
    "        return dYdX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "        \n",
    "        if use_bias == True:\n",
    "            self.hidden_bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "            self.input_bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.hidden_bias = np.zeros((hidden_dim))\n",
    "            self.input_bias = np.zeros((hidden_dim))\n",
    "        \n",
    "    def forward(self, X_in):        \n",
    "        #treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        #treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "        \n",
    "        #u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim)) \n",
    "        \n",
    "        for i in range(self.seq_len):\n",
    "            \n",
    "            input_part = np.einsum('ij,jk->ik', X_in[:,i,:], self.input_weights.T) + self.input_bias\n",
    "            hidden_part = np.einsum('ij,jj->ij', H[:,i,:], self.hidden_weights.T) + self.hidden_bias\n",
    "            \n",
    "            H[:,i+1,:] = self.activation.forward(input_part + hidden_part)\n",
    "       \n",
    "        return H, H[:,self.seq_len,:]\n",
    "    \n",
    "    def book_forward(self, X_in):\n",
    "        \n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim)) \n",
    "        \n",
    "        for i in range(self.seq_len):\n",
    "            #ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki',self.input_weights, X_in[:,i,:].T) + self.input_bias\n",
    "            hidden_part = np.einsum('ii,ij->ji',self.hidden_weights, H[:,i,:].T) + self.hidden_bias\n",
    "            \n",
    "            H[:,i+1,:] = self.activation.forward(input_part + hidden_part)\n",
    "       \n",
    "        return H, H[:,self.seq_len,:]\n",
    "        \n",
    "            \n",
    "    def backward(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        \n",
    "        print(f'self.hiddan_bias={self.hidden_bias}')\n",
    "        print(f'self.input_bias={self.input_bias}')\n",
    "        \n",
    "        \n",
    "        dEdB_in = np.zeros_like(self.input_bias)\n",
    "        dEdB_h = np.zeros_like(self.hidden_bias)\n",
    "        \n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:,self.seq_len,:] = dEdY[:,self.seq_len - 1,:]\n",
    "        \n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            \n",
    "            dEdW_in += np.einsum('bh,bi->hi', H_grad[:,i,:], X[:,i-1,:])\n",
    "            dEdW_hh += np.einsum('bh,bk->hk', H_grad[:,i,:], H[:,i-1,:])\n",
    "            \n",
    "            #zapeo oko aktivacijske funkcije!!!, kako to derivirati i mnoziti s matricama\n",
    "            \n",
    "            if(self.use_bias == True):\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:,i,:]) * H_grad[:,i,:], axis=(0))\n",
    "                #mislim da ovdje nije potrebno imati oba biasa, mislim na početku se random postave,\n",
    "                #ali ovdje uvijek računamo iste vrijednosti\n",
    "                dEdB_h = dEdB_in\n",
    "            #ovo pitaj !!!!\n",
    "            \n",
    "            if i > 1:\n",
    "                H_grad[:,i-1,:] = np.einsum('bh,hh->bh', H_grad[:,i,:], self.hidden_weights) * self.activation.backward(H[:,i,:]) + dEdY[:,i-2,:]\n",
    "            else:\n",
    "                H_grad[:,i-1,:] = np.einsum('bh,hh->bh', H_grad[:,i,:], self.hidden_weights) * self.activation.backward(H[:,i,:])\n",
    "        \n",
    "        return dEdW_in, dEdW_hh, dEdB_in, dEdB_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn forward checker\n",
    "\n",
    "rnn = RnnLayer(4, 5, 3, 2)\n",
    "#input dim 4\n",
    "#hidden dim 5\n",
    "#batch 2\n",
    "#timestamps 3\n",
    "\n",
    "X_in = np.array([[[1,2,1,3],[2,2,3,1],[0,2,3,1]],[[1,3,4,3],[1,2,1,1],[1,0,1,2]]])\n",
    "H, last = rnn.forward(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.hiddan_bias=[ 0.4316296  -0.1063735  -0.31191418  0.16940213  0.00493861]\n",
      "self.input_bias=[ 0.35240786  0.36911693 -0.28135108  0.06668408 -0.35253175]\n"
     ]
    }
   ],
   "source": [
    "dEdY = np.array([[[ 0.34545989,  0.07336296, -0.16346513, -0.06904482,\n",
    "          0.0458759 ],\n",
    "        [ 0.37271336,  0.07915059, -0.17636096, -0.07449179,\n",
    "          0.04949507],\n",
    "        [ 0.35166208,  0.07468007, -0.16639989, -0.07028441,\n",
    "          0.04669953]],\n",
    "\n",
    "       [[ 0.36616935,  0.07776088, -0.17326446, -0.07318388,\n",
    "          0.04862605],\n",
    "        [ 0.33954613,  0.07210709, -0.16066685, -0.06786287,\n",
    "          0.04509058],\n",
    "        [ 0.35872758,  0.07618053, -0.16974315, -0.07169654,\n",
    "          0.04763781]]])\n",
    "\n",
    "Win, Wh, Bin, Bh = rnn.backward(X_in, H, dEdY)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin=[ 1.86685287  0.46355134 -0.56416936 -0.40622725  0.        ]\n",
      "Bh=[ 1.86685287  0.46355134 -0.56416936 -0.40622725  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(f'Bin={Bin}')\n",
    "print(f'Bh={Bh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
