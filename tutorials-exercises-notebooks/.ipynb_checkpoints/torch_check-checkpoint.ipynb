{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "treated-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "standard-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        exps = np.exp(x_in-np.max(x_in, axis=-1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "class Sigmoid(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return 1./(1 + np.exp(-x_in))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        fw = Sigmoid().forward(x_in)\n",
    "        return fw * (1 - fw)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward_calculated(sigmoid_x):\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "    \n",
    "class Tanh(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(X_in):\n",
    "        return np.tanh(X_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(X_in):\n",
    "        # dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(X_in)) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward_calculated(tanh_x_in):\n",
    "        return 1 - tanh_x_in ** 2\n",
    "\n",
    "\n",
    "class ReLu(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.maximum(x_in, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        return x_in > 0\n",
    "    \n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, y, o):\n",
    "        self.y_pred = Softmax.forward(o)\n",
    "        return np.sum(-y * np.log(self.y_pred + 1e-15))/(y.shape[0])\n",
    "\n",
    "    def backward(self, y):\n",
    "        return (self.y_pred - y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "engaging-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        # treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "\n",
    "        # u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            input_part = np.einsum('ij,jk->ik', x_in[:, i, :], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', H[:, i, :], self.hidden_weights.T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def book_forward(self, x_in):\n",
    "\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            # ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki', self.input_weights, x_in[:, i, :].T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', self.hidden_weights, H[:, i, :].T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def backward(self, x, h, dEdY):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        act = self.activation.backward_calculated(h)\n",
    "\n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "            if i < seq_len - 1:\n",
    "                H_grad[:, i, :] = np.dot(H_grad[:, i + 1, :] * act[:, i+2, :], self.hidden_weights) + dEdY[:, i, :]\n",
    "            else:\n",
    "                H_grad[:, i, :] = dEdY[:, i, :]\n",
    "\n",
    "            dEdW_in += np.sum(act[:, i+1, :].reshape(batch_size, self.hidden_dim, 1) * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i, :])), axis=0)\n",
    "            dEdW_hh += np.sum(act[:, i+1, :].reshape(batch_size, self.hidden_dim, 1) * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(act[:, i+1, :] * H_grad[:, i, :], axis=0)\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_2nd(self, x, h, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, seq_len, :] = dEdY[:, seq_len - 1, :]\n",
    "\n",
    "        for i in range(seq_len, 0, -1):\n",
    "\n",
    "            activation_backward = self.activation.backward_calculated(h[:, i, :])\n",
    "            back_reshaped = activation_backward.reshape(batch_size, self.hidden_dim, 1)\n",
    "\n",
    "            dEdW_in += np.sum(back_reshaped * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i - 1, :])), axis=0)\n",
    "            dEdW_hh += np.sum(back_reshaped * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i - 1, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(activation_backward * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "            b = np.dot(H_grad[:, i, :], self.hidden_weights)\n",
    "            a = b * activation_backward\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = a + dEdY[:, i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.dot(H_grad[:, i, :], self.hidden_weights) * activation_backward\n",
    "\n",
    "            # if i > 1:\n",
    "            #    H_grad[:, i - 1, :] = ((np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * activation_backward) + dEdY[:, i - 2, :])\n",
    "            # else:\n",
    "            #    H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * activation_backward\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        seq_len = X.shape[1]\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, seq_len, :] = dEdY[:, seq_len - 1, :]\n",
    "\n",
    "        for i in range(seq_len, 0, -1):\n",
    "\n",
    "            for k in range(batch_size):\n",
    "                act_grad = np.diag(self.activation.backward_calculated(H[k, i, :]))\n",
    "                h_grad = H_grad[k, i, :].reshape(self.hidden_dim, 1)\n",
    "\n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k, i - 1, :].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k, i - 1, :].reshape(1, self.hidden_dim)))\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward_calculated(H[:, i, :]) * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward_calculated(H[:, i, :]) + dEdY[:,\n",
    "                                                                                                                         i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward_calculated(H[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "    \n",
    "    \n",
    "class DenseLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        sq = np.sqrt(1. / input_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = np.random.uniform(-sq, sq, (output_dim, input_dim))\n",
    "        if use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, output_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(output_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        return np.tensordot(x_in, self.weights.T, axes=((-1), 0)) + self.bias\n",
    "\n",
    "    def backward(self, de_dy, x_in):\n",
    "        # de_dw = de_dy * dYdW = de_dy * X\n",
    "        # dEdb = de_dy * dYdb = de_dy\n",
    "        # dEdX = de_dy * dYdX = de_dy * W\n",
    "        axis = tuple(range(len(x_in.shape) - 1))\n",
    "        de_dw = np.tensordot(de_dy, x_in, axes=(axis, axis))\n",
    "        de_db = np.sum(de_dy, axis=axis)\n",
    "        de_dx = np.tensordot(de_dy, self.weights, axes=(-1, 0))\n",
    "\n",
    "        return de_dx, de_dw, de_db\n",
    "\n",
    "    def refresh(self, de_dw, de_db, learning_rate):\n",
    "        self.weights = self.weights - learning_rate * de_dw\n",
    "        if self.use_bias:\n",
    "            self.bias = self.bias - learning_rate * de_db\n",
    "            \n",
    "            \n",
    "class LSTMLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, use_bias=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        # input weights (W_in_hi|W_fgt_hi|W_g_hi|W_out_hi)\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (4, hidden_dim, input_dim))\n",
    "        # hidden weights (W_in_hh|W_fgt_hh|W_g_hh|W_out_hh)\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (4, hidden_dim, hidden_dim))\n",
    "\n",
    "        self.tanh = Tanh\n",
    "        self.sigmoid = Sigmoid\n",
    "\n",
    "        self.gates = None\n",
    "        self.H = None\n",
    "        self.C = None\n",
    "\n",
    "        if self.use_bias:\n",
    "            # bias = (in_bias|fgt_bias|g_bias|out_bias)\n",
    "            self.bias = np.random.uniform(-sq, sq, (4, hidden_dim))\n",
    "        else:\n",
    "            self.bias = np.zeros((4, hidden_dim))\n",
    "\n",
    "    def forward(self, X_in, h_0=None, c_0=None):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        self.H = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if h_0 is not None:\n",
    "            self.H[:, 0, :] = h_0\n",
    "\n",
    "        self.C = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if c_0 is not None:\n",
    "            self.C[:, 0, :] = c_0\n",
    "\n",
    "        self.gates = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # input_gate\n",
    "            self.gates[0, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[0, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[0, :, :].T) + self.bias[0, :])\n",
    "            # forget gate\n",
    "            self.gates[1, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[1, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[1, :, :].T) + self.bias[1, :])\n",
    "            # c~ gate\n",
    "            self.gates[2, :, i, :] = self.tanh.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[2, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[2, :, :].T) + self.bias[2, :])\n",
    "            # output gate\n",
    "            self.gates[3, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[3, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[3, :, :].T) + self.bias[3, :])\n",
    "\n",
    "            self.C[:, i + 1, :] = self.gates[1, :, i, :] * self.C[:, i, :] + self.gates[0, :, i, :] * self.gates[2, :, i, :]\n",
    "            self.H[:, i + 1, :] = self.gates[3, :, i, :] * self.tanh.forward(self.C[:, i + 1, :])\n",
    "\n",
    "        return self.H, self.H[:, seq_len, :], self.C[:, seq_len, :]\n",
    "\n",
    "    def backward(self, X_in, dEdY):\n",
    "\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        C_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        X_grad = np.zeros((batch_size, seq_len, self.input_dim))\n",
    "\n",
    "        gates_grad = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                H_grad[:, i, :] = np.matmul(gates_grad[:, :, i + 1, :], self.hidden_weights).sum(axis=0) + dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad[:, i, :] * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :]) + C_grad[:, i + 1, :] * self.gates[1, :, i + 1, :]\n",
    "            else:\n",
    "                H_grad[:, i, :] = dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad[:, i, :] * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :])\n",
    "\n",
    "            gates_grad[0, :, i, :] = C_grad[:, i, :] * self.gates[2, :, i, :] * self.sigmoid.backward_calculated(self.gates[0, :, i, :])\n",
    "            gates_grad[1, :, i, :] = C_grad[:, i, :] * self.C[:, i, :] * self.sigmoid.backward_calculated(self.gates[1, :, i, :])\n",
    "            gates_grad[2, :, i, :] = C_grad[:, i, :] * self.gates[0, :, i, :] * self.tanh.backward_calculated(self.gates[2, :, i, :])\n",
    "            gates_grad[3, :, i, :] = H_grad[:, i, :] * self.tanh.forward(self.C[:, i + 1, :]) * self.sigmoid.backward_calculated(self.gates[3, :, i, :])\n",
    "\n",
    "            X_grad[:, i, :] = np.matmul(gates_grad[:, :, i, :], self.input_weights).sum(axis=0)\n",
    "                \n",
    "            dEdW_in[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                dEdW_hh[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in[0, :] += np.sum(gates_grad[0, :, i, :], axis=0)\n",
    "                dEdB_in[1, :] += np.sum(gates_grad[1, :, i, :], axis=0)\n",
    "                dEdB_in[2, :] += np.sum(gates_grad[2, :, i, :], axis=0)\n",
    "                dEdB_in[3, :] += np.sum(gates_grad[3, :, i, :], axis=0)\n",
    "        \n",
    "        return dEdW_in, dEdW_hh, dEdB_in, X_grad\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-sarah",
   "metadata": {},
   "source": [
    "## Softmax, Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aging-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "\n",
    "x = torch.randn(N, num_classes) # logits\n",
    "y = torch.randint(num_classes, (N,)) # labels\n",
    "\n",
    "x_ = x.numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()] # one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regional-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax check: True\n"
     ]
    }
   ],
   "source": [
    "softmax_out = F.softmax(x, dim=-1)\n",
    "softmax_out\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax_out_ = softmax.forward(x_)\n",
    "\n",
    "print('Softmax check:', np.isclose(softmax_out, softmax_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focused-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss check: True\n"
     ]
    }
   ],
   "source": [
    "cel = nn.CrossEntropyLoss()\n",
    "loss = cel(x, y).item()\n",
    "\n",
    "cel_ = CrossEntropyLoss()\n",
    "loss_ = cel_.forward(y_, x_)\n",
    "loss_\n",
    "\n",
    "print('Cross entropy loss check:', np.isclose(loss, loss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-display",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "numerous-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer forward check:  True\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "seq_len = 3\n",
    "\n",
    "x = torch.randn(N, seq_len, requires_grad=True)\n",
    "y = torch.randint(num_classes, (N,))\n",
    "\n",
    "x_ = x.detach().numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()]\n",
    "\n",
    "linear = nn.Linear(seq_len, num_classes)\n",
    "linear_ = DenseLayer(seq_len, num_classes)\n",
    "\n",
    "linear_.weights = linear.weight.detach().numpy()\n",
    "linear_.bias = linear.bias.detach().numpy()\n",
    "\n",
    "lin_out = linear(x)\n",
    "lin_out_ = linear_.forward(x_)\n",
    "\n",
    "print('Linear layer forward check: ', np.isclose(lin_out.detach().numpy(), lin_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "minor-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7812, grad_fn=<NllLossBackward>)\n",
      "[TORCH] dE/dy:\n",
      "tensor([[ 0.0154, -0.1254,  0.0595,  0.0505],\n",
      "        [ 0.0260, -0.0758,  0.0206,  0.0292],\n",
      "        [ 0.0358,  0.0895,  0.0610, -0.1863],\n",
      "        [ 0.0995,  0.0885, -0.1909,  0.0029],\n",
      "        [ 0.0300,  0.0869,  0.0454, -0.1623]])\n",
      "\n",
      "[TORCH] dE/dW:\n",
      "tensor([[ 0.0685,  0.1054, -0.2851],\n",
      "        [ 0.4073,  0.0524, -0.2640],\n",
      "        [-0.2708, -0.4270,  0.2497],\n",
      "        [-0.2051,  0.2692,  0.2994]])\n",
      "\n",
      "[TORCH] dE/dB:\n",
      "tensor([ 0.2067,  0.0636, -0.0044, -0.2660])\n",
      "\n",
      "[TORCH] dE/dX:\n",
      "tensor([[ 0.0082, -0.0504,  0.0747],\n",
      "        [ 0.0138, -0.0136,  0.0439],\n",
      "        [ 0.0379,  0.0321, -0.1905],\n",
      "        [ 0.0463,  0.1636, -0.0177],\n",
      "        [ 0.0319,  0.0328, -0.1681]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = cel(lin_out, y)\n",
    "lin_out.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(f'[TORCH] dE/dy:\\n{lin_out.grad}\\n')\n",
    "print(f'[TORCH] dE/dW:\\n{linear.weight.grad}\\n')\n",
    "print(f'[TORCH] dE/dB:\\n{linear.bias.grad}\\n')\n",
    "print(f'[TORCH] dE/dX:\\n{x.grad}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "gothic-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = cel_.forward(y_, lin_out_)\n",
    "\n",
    "de_dy = cel_.backward(y_)\n",
    "de_dx, de_dw, de_db = linear_.backward(de_dy, x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liked-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dy:\n",
      "[[ 0.01289863 -0.12475173  0.07498912  0.03686398]\n",
      " [ 0.01184563 -0.15936113  0.11452467  0.03299084]\n",
      " [ 0.09872308  0.02475784  0.04569956 -0.16918049]\n",
      " [-0.1750032   0.03356197  0.10637606  0.03506516]\n",
      " [-0.18160434  0.03501907  0.11235602  0.03422925]]\n",
      "\n",
      "[CUSTOM] dE/dW:\n",
      "[[-0.02335379  0.34712666  0.28857261]\n",
      " [ 0.18554716  0.20977268  0.25030635]\n",
      " [ 0.01165509 -0.29133464 -0.25821118]\n",
      " [-0.17384847 -0.26556471 -0.28066779]]\n",
      "\n",
      "[CUSTOM] dE/dB:\n",
      "[-0.2331402  -0.19077397  0.45394544 -0.03003125]\n",
      "\n",
      "[CUSTOM] dE/dX:\n",
      "[[ 0.09699437 -0.03969966 -0.00161258]\n",
      " [ 0.12726112 -0.05942757 -0.00761959]\n",
      " [ 0.0481368   0.0253786   0.03034998]\n",
      " [-0.07159334 -0.10329892 -0.09504108]\n",
      " [-0.07380924 -0.1078932  -0.09902084]]\n",
      "\n",
      "Check dE/dy: True\n",
      "Check dE/dX: True\n",
      "Check dE/dW: True\n",
      "Check dE/dB: True\n"
     ]
    }
   ],
   "source": [
    "print(f'[CUSTOM] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[CUSTOM] dE/dW:\\n{de_dw}\\n')\n",
    "print(f'[CUSTOM] dE/dB:\\n{de_db}\\n')\n",
    "print(f'[CUSTOM] dE/dX:\\n{de_dx}\\n')\n",
    "\n",
    "print('Check dE/dy:', np.isclose(lin_out.grad, de_dy).all())\n",
    "print('Check dE/dX:', np.isclose(x.grad, de_dx).all())\n",
    "print('Check dE/dW:', np.isclose(linear.weight.grad, de_dw).all())\n",
    "print('Check dE/dB:', np.isclose(linear.bias.grad, de_db).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-briefs",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "coordinated-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN layer forward check:  True\n",
      "RNN layer forward check last hidden:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "rnn = nn.RNN(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "rnn_ = RnnLayer(emb_dim, hidden_dim, seq_len, N, use_bias=False)\n",
    "rnn_.input_weights = rnn.weight_ih_l0.detach().numpy()\n",
    "rnn_.hidden_weights = rnn.weight_hh_l0.detach().numpy()\n",
    "\n",
    "rnn_out, h_n = rnn(x)\n",
    "rnn_out_, h_n_ = rnn_.forward(x_)\n",
    "rnn_out__ = rnn_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('RNN layer forward check: ', np.isclose(rnn_out.detach().numpy(), rnn_out__, atol=1e-3).all())\n",
    "print('RNN layer forward check last hidden: ', np.isclose(h_n.detach().numpy(), h_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "surprised-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TORCH] dE/dWih:\n",
      "tensor([[-0.8407, -7.0685,  5.7418,  0.9003, -5.5570, -3.4874],\n",
      "        [-1.5510,  0.2180, -1.5924, -1.5642,  0.2131, -0.2486],\n",
      "        [ 0.6652, -0.6823, -2.7304,  3.6788, -0.3346, -5.8572],\n",
      "        [-1.3273,  1.0522, -0.3969, -6.4262,  1.8830,  4.0530],\n",
      "        [ 3.2290, -3.5633,  2.8654,  1.1616,  0.6940, -4.1529]])\n",
      "\n",
      "[TORCH] dE/dWhh:\n",
      "tensor([[-0.0582,  0.3691,  0.4601,  0.3695,  0.8196],\n",
      "        [-0.3268,  2.1464,  0.1138,  1.9113, -0.9562],\n",
      "        [ 0.8973, -1.4937, -0.2325,  0.0418,  0.8275],\n",
      "        [ 2.3509,  0.1792, -0.4186, -0.6692, -2.4644],\n",
      "        [ 1.0182,  0.7373,  1.3249,  0.1563,  0.8014]])\n",
      "\n",
      "[TORCH] dE/dy:\n",
      "tensor([[[-1.3503,  0.8712,  0.4761, -0.7258,  0.3794],\n",
      "         [ 0.0730, -1.7456,  0.2157,  1.2655,  0.7107],\n",
      "         [ 0.2669,  0.2185,  1.6872,  0.8179,  1.3344]],\n",
      "\n",
      "        [[ 0.8743,  1.3089, -0.2373, -0.3603,  0.4883],\n",
      "         [-0.8006, -0.1843, -0.1163,  1.8602, -1.2763],\n",
      "         [-0.5793, -2.3137,  1.3387, -0.4709,  0.3970]],\n",
      "\n",
      "        [[ 0.1361,  0.5795,  1.4143, -1.0820,  0.7761],\n",
      "         [-1.2928, -1.4015, -0.4311, -0.1458,  0.2791],\n",
      "         [ 0.8384, -0.0991, -0.5733, -0.9763,  1.2727]],\n",
      "\n",
      "        [[ 0.0631, -0.6914,  0.5707, -0.9122,  0.1855],\n",
      "         [-0.6729,  0.1066, -0.6360,  0.7578, -1.2220],\n",
      "         [ 0.4976,  0.5751,  0.4075,  1.4712,  1.3867]],\n",
      "\n",
      "        [[ 0.6538, -1.2379,  2.3431, -0.4882, -1.6707],\n",
      "         [ 0.8672, -2.2330, -0.3342,  1.1836,  0.9082],\n",
      "         [ 0.7796, -0.1918,  0.1682, -1.5971,  0.5366]]])\n",
      "\n",
      "[TORCH] dE/dH:\n",
      "tensor([[[-1.3503,  0.8712,  0.4761, -0.7258,  0.3794],\n",
      "         [ 0.0730, -1.7456,  0.2157,  1.2655,  0.7107],\n",
      "         [ 0.2669,  0.2185,  1.6872,  0.8179,  1.3344]],\n",
      "\n",
      "        [[ 0.8743,  1.3089, -0.2373, -0.3603,  0.4883],\n",
      "         [-0.8006, -0.1843, -0.1163,  1.8602, -1.2763],\n",
      "         [-0.5793, -2.3137,  1.3387, -0.4709,  0.3970]],\n",
      "\n",
      "        [[ 0.1361,  0.5795,  1.4143, -1.0820,  0.7761],\n",
      "         [-1.2928, -1.4015, -0.4311, -0.1458,  0.2791],\n",
      "         [ 0.8384, -0.0991, -0.5733, -0.9763,  1.2727]],\n",
      "\n",
      "        [[ 0.0631, -0.6914,  0.5707, -0.9122,  0.1855],\n",
      "         [-0.6729,  0.1066, -0.6360,  0.7578, -1.2220],\n",
      "         [ 0.4976,  0.5751,  0.4075,  1.4712,  1.3867]],\n",
      "\n",
      "        [[ 0.6538, -1.2379,  2.3431, -0.4882, -1.6707],\n",
      "         [ 0.8672, -2.2330, -0.3342,  1.1836,  0.9082],\n",
      "         [ 0.7796, -0.1918,  0.1682, -1.5971,  0.5366]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "rnn_out.retain_grad()\n",
    "rnn_out.backward(de_dy)\n",
    "\n",
    "print(f'[TORCH] dE/dWih:\\n{rnn.weight_ih_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dWhh:\\n{rnn.weight_hh_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[TORCH] dE/dH:\\n{rnn_out.grad}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fourth-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dWih:\n",
      "[[-0.84065264 -7.068542    5.7418056   0.9002589  -5.5570254  -3.4874358 ]\n",
      " [-1.5509931   0.21796954 -1.5923568  -1.5642024   0.2130806  -0.2485686 ]\n",
      " [ 0.6651537  -0.68229634 -2.7304165   3.6788378  -0.33462602 -5.857197  ]\n",
      " [-1.3273039   1.052206   -0.39686674 -6.426206    1.8830379   4.0530033 ]\n",
      " [ 3.228954   -3.563257    2.8654382   1.1615889   0.6939566  -4.1528783 ]]\n",
      "\n",
      "[CUSTOM] dE/dWhh:\n",
      "[[-0.05816048  0.36913016  0.46005616  0.3695405   0.81958103]\n",
      " [-0.3268031   2.1463962   0.11377817  1.9113404  -0.9561599 ]\n",
      " [ 0.89730275 -1.4937084  -0.2324918   0.04177425  0.8274532 ]\n",
      " [ 2.3509252   0.17922425 -0.41860518 -0.6692463  -2.4644444 ]\n",
      " [ 1.0182368   0.737316    1.3249016   0.15629236  0.80138135]]\n",
      "\n",
      "RNN layer gradient check dEdW_in:  True\n",
      "RNN layer gradient check dEdW_hh:  True\n"
     ]
    }
   ],
   "source": [
    "dEdW_in, dEdW_hh, _ = rnn_.backward(x_, rnn_out_, de_dy_)\n",
    "\n",
    "print(f'[CUSTOM] dE/dWih:\\n{dEdW_in}\\n')\n",
    "print(f'[CUSTOM] dE/dWhh:\\n{dEdW_hh}\\n')\n",
    "\n",
    "print('RNN layer gradient check dEdW_in: ', np.isclose(rnn.weight_ih_l0.grad.numpy(), dEdW_in).all())\n",
    "print('RNN layer gradient check dEdW_hh: ', np.isclose(rnn.weight_hh_l0.grad.numpy(), dEdW_hh).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-rebate",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "private-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer forward check:  True\n",
      "LSTM layer forward check last hidden:  True\n",
      "LSTM layer forward check last c_n:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 4\n",
    "emb_dim = 3\n",
    "seq_len = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "lstm = nn.LSTM(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "lstm_ = LSTMLayer(emb_dim, hidden_dim, use_bias=False)\n",
    "wih = lstm.weight_ih_l0.detach().numpy()\n",
    "whh = lstm.weight_hh_l0.detach().numpy()\n",
    "\n",
    "lstm_.input_weights[0,:,:] = wih[0:hidden_dim, :]\n",
    "lstm_.input_weights[1,:,:] = wih[hidden_dim: 2*hidden_dim, :]\n",
    "lstm_.input_weights[2,:,:] = wih[2*hidden_dim: 3*hidden_dim, :]\n",
    "lstm_.input_weights[3,:,:] = wih[3*hidden_dim: 4*hidden_dim, :]\n",
    "\n",
    "lstm_.hidden_weights[0,:,:] = whh[0:hidden_dim, :]\n",
    "lstm_.hidden_weights[1,:,:] = whh[hidden_dim: 2*hidden_dim, :]\n",
    "lstm_.hidden_weights[2,:,:] = whh[2*hidden_dim: 3*hidden_dim, :]\n",
    "lstm_.hidden_weights[3,:,:] = whh[3*hidden_dim: 4*hidden_dim, :]\n",
    "\n",
    "\n",
    "lstm_out, h_n = lstm(x)\n",
    "lstm_out_, h_n_, c_n_ = lstm_.forward(x_)\n",
    "lstm_out__ = lstm_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('LSTM layer forward check: ', np.isclose(lstm_out.detach().numpy(), lstm_out__, atol=1e-3).all())\n",
    "print('LSTM layer forward check last hidden: ', np.isclose(h_n[0].detach().numpy(), h_n_, atol=1e-3).all())\n",
    "print('LSTM layer forward check last c_n: ', np.isclose(h_n[1].detach().numpy(), c_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "painful-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "x.retain_grad()\n",
    "lstm_out.backward(de_dy)\n",
    "\n",
    "dEdW_in, dEdW_hh, a, X_grad = lstm_.backward(x_, de_dy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "honest-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer gradient check dEdW_in:  True\n",
      "LSTM layer gradient check dEdW_hh:  True\n",
      "LSTM layer gradient check dEdX:  True\n"
     ]
    }
   ],
   "source": [
    "print('LSTM layer gradient check dEdW_in: ', np.isclose(lstm.weight_ih_l0.grad.numpy(), dEdW_in.reshape(4*hidden_dim,emb_dim), atol=1e-3).all())\n",
    "print('LSTM layer gradient check dEdW_hh: ', np.isclose(lstm.weight_hh_l0.grad.numpy(), dEdW_hh.reshape(4*hidden_dim,hidden_dim), atol=1e-3).all())\n",
    "print('LSTM layer gradient check dEdX: ', np.isclose(x.grad.numpy(), X_grad, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-brick",
   "metadata": {},
   "source": [
    "## Pokusaj pravljenja torch modela rnn, fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "practical-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "reverse-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 3\n",
    "hidden_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "architectural-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.RNN(emb_dim, hidden_dim, bias=False, batch_first=True),\n",
    "    torch.nn.Linear(hidden_dim, 5, bias=False)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-retro",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
