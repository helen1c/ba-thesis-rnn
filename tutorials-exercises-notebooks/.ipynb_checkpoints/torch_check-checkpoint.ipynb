{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "treated-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "standard-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        exps = np.exp(x_in-np.max(x_in, axis=-1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "class Sigmoid(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return 1./(1 + np.exp(-x_in))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        fw = Sigmoid().forward(x_in)\n",
    "        return fw * (1 - fw)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward_calculated(sigmoid_x):\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "    \n",
    "class Tanh(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(X_in):\n",
    "        return np.tanh(X_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(X_in):\n",
    "        # dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(X_in)) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward_calculated(tanh_x_in):\n",
    "        return 1 - tanh_x_in ** 2\n",
    "\n",
    "\n",
    "class ReLu(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.maximum(x_in, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        return x_in > 0\n",
    "    \n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, y, o):\n",
    "        self.y_pred = Softmax.forward(o)\n",
    "        return np.sum(-y * np.log(self.y_pred + 1e-15))/(y.shape[0])\n",
    "\n",
    "    def backward(self, y):\n",
    "        return (self.y_pred - y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaging-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        # treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "\n",
    "        # u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            input_part = np.einsum('ij,jk->ik', x_in[:, i, :], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', H[:, i, :], self.hidden_weights.T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def book_forward(self, x_in):\n",
    "\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            # ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki', self.input_weights, x_in[:, i, :].T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', self.hidden_weights, H[:, i, :].T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def backward(self, x, h, dEdY):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        act = self.activation.backward_calculated(h)\n",
    "\n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "            if i < seq_len - 1:\n",
    "                H_grad[:, i, :] = np.dot(H_grad[:, i + 1, :] * act[:, i+2, :], self.hidden_weights) + dEdY[:, i, :]\n",
    "            else:\n",
    "                H_grad[:, i, :] = dEdY[:, i, :]\n",
    "\n",
    "            dEdW_in += np.sum(act[:, i+1, :].reshape(batch_size, self.hidden_dim, 1) * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i, :])), axis=0)\n",
    "            dEdW_hh += np.sum(act[:, i+1, :].reshape(batch_size, self.hidden_dim, 1) * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(act[:, i+1, :] * H_grad[:, i, :], axis=0)\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_2nd(self, x, h, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, seq_len, :] = dEdY[:, seq_len - 1, :]\n",
    "\n",
    "        for i in range(seq_len, 0, -1):\n",
    "\n",
    "            activation_backward = self.activation.backward_calculated(h[:, i, :])\n",
    "            back_reshaped = activation_backward.reshape(batch_size, self.hidden_dim, 1)\n",
    "\n",
    "            dEdW_in += np.sum(back_reshaped * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i - 1, :])), axis=0)\n",
    "            dEdW_hh += np.sum(back_reshaped * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i - 1, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(activation_backward * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "            b = np.dot(H_grad[:, i, :], self.hidden_weights)\n",
    "            a = b * activation_backward\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = a + dEdY[:, i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.dot(H_grad[:, i, :], self.hidden_weights) * activation_backward\n",
    "\n",
    "            # if i > 1:\n",
    "            #    H_grad[:, i - 1, :] = ((np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * activation_backward) + dEdY[:, i - 2, :])\n",
    "            # else:\n",
    "            #    H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * activation_backward\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        seq_len = X.shape[1]\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, seq_len, :] = dEdY[:, seq_len - 1, :]\n",
    "\n",
    "        for i in range(seq_len, 0, -1):\n",
    "\n",
    "            for k in range(batch_size):\n",
    "                act_grad = np.diag(self.activation.backward_calculated(H[k, i, :]))\n",
    "                h_grad = H_grad[k, i, :].reshape(self.hidden_dim, 1)\n",
    "\n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k, i - 1, :].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k, i - 1, :].reshape(1, self.hidden_dim)))\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward_calculated(H[:, i, :]) * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward_calculated(H[:, i, :]) + dEdY[:,\n",
    "                                                                                                                         i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward_calculated(H[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "    \n",
    "    \n",
    "class DenseLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        sq = np.sqrt(1. / input_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = np.random.uniform(-sq, sq, (output_dim, input_dim))\n",
    "        if use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, output_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(output_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        return np.tensordot(x_in, self.weights.T, axes=((-1), 0)) + self.bias\n",
    "\n",
    "    def backward(self, de_dy, x_in):\n",
    "        # de_dw = de_dy * dYdW = de_dy * X\n",
    "        # dEdb = de_dy * dYdb = de_dy\n",
    "        # dEdX = de_dy * dYdX = de_dy * W\n",
    "        axis = tuple(range(len(x_in.shape) - 1))\n",
    "        de_dw = np.tensordot(de_dy, x_in, axes=(axis, axis))\n",
    "        de_db = np.sum(de_dy, axis=axis)\n",
    "        de_dx = np.tensordot(de_dy, self.weights, axes=(-1, 0))\n",
    "\n",
    "        return de_dx, de_dw, de_db\n",
    "\n",
    "    def refresh(self, de_dw, de_db, learning_rate):\n",
    "        self.weights = self.weights - learning_rate * de_dw\n",
    "        if self.use_bias:\n",
    "            self.bias = self.bias - learning_rate * de_db\n",
    "            \n",
    "            \n",
    "class LSTMLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, use_bias=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        # input weights (W_in_hi|W_fgt_hi|W_g_hi|W_out_hi)\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (4, hidden_dim, input_dim))\n",
    "        # hidden weights (W_in_hh|W_fgt_hh|W_g_hh|W_out_hh)\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (4, hidden_dim, hidden_dim))\n",
    "\n",
    "        self.tanh = Tanh\n",
    "        self.sigmoid = Sigmoid\n",
    "\n",
    "        self.gates = None\n",
    "        self.H = None\n",
    "        self.C = None\n",
    "\n",
    "        if self.use_bias:\n",
    "            # bias = (in_bias|fgt_bias|g_bias|out_bias)\n",
    "            self.bias = np.random.uniform(-sq, sq, (4, hidden_dim))\n",
    "        else:\n",
    "            self.bias = np.zeros((4, hidden_dim))\n",
    "\n",
    "    def forward(self, X_in, h_0=None, c_0=None):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        self.H = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if h_0 is not None:\n",
    "            self.H[:, 0, :] = h_0\n",
    "\n",
    "        self.C = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if c_0 is not None:\n",
    "            self.C[:, 0, :] = c_0\n",
    "\n",
    "        self.gates = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # input_gate\n",
    "            self.gates[0, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[0, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[0, :, :].T) + self.bias[0, :])\n",
    "            # forget gate\n",
    "            self.gates[1, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[1, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[1, :, :].T) + self.bias[1, :])\n",
    "            # c~ gate\n",
    "            self.gates[2, :, i, :] = self.tanh.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[2, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[2, :, :].T) + self.bias[2, :])\n",
    "            # output gate\n",
    "            self.gates[3, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[3, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[3, :, :].T) + self.bias[3, :])\n",
    "\n",
    "            self.C[:, i + 1, :] = self.gates[1, :, i, :] * self.C[:, i, :] + self.gates[0, :, i, :] * self.gates[2, :, i, :]\n",
    "            self.H[:, i + 1, :] = self.gates[3, :, i, :] * self.tanh.forward(self.C[:, i + 1, :])\n",
    "\n",
    "        return self.H, self.H[:, seq_len, :], self.C[:, seq_len, :]\n",
    "\n",
    "    def backward(self, X_in, dEdY):\n",
    "\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        C_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        X_grad = np.zeros((batch_size, seq_len, self.input_dim))\n",
    "\n",
    "        gates_grad = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                H_grad = np.matmul(gates_grad[:, :, i + 1, :], self.hidden_weights).sum(axis=0) + dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :]) + C_grad[:, i + 1, :] * self.gates[1, :, i + 1, :]\n",
    "            else:\n",
    "                H_grad = dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :])\n",
    "\n",
    "            gates_grad[0, :, i, :] = C_grad[:, i, :] * self.gates[2, :, i, :] * self.sigmoid.backward_calculated(self.gates[0, :, i, :])\n",
    "            gates_grad[1, :, i, :] = C_grad[:, i, :] * self.C[:, i, :] * self.sigmoid.backward_calculated(self.gates[1, :, i, :])\n",
    "            gates_grad[2, :, i, :] = C_grad[:, i, :] * self.gates[0, :, i, :] * self.tanh.backward_calculated(self.gates[2, :, i, :])\n",
    "            gates_grad[3, :, i, :] = H_grad * self.tanh.forward(self.C[:, i + 1, :]) * self.sigmoid.backward_calculated(self.gates[3, :, i, :])\n",
    "\n",
    "            X_grad[:, i, :] = np.matmul(gates_grad[:, :, i, :], self.input_weights).sum(axis=0)\n",
    "\n",
    "            dEdW_in[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                dEdW_hh[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in[0, :] += np.sum(gates_grad[0, :, i, :], axis=0)\n",
    "                dEdB_in[1, :] += np.sum(gates_grad[1, :, i, :], axis=0)\n",
    "                dEdB_in[2, :] += np.sum(gates_grad[2, :, i, :], axis=0)\n",
    "                dEdB_in[3, :] += np.sum(gates_grad[3, :, i, :], axis=0)\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in, X_grad\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class GRULayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, use_bias=True):\n",
    "        \n",
    "        #r_t = sigmoid(W_r_hi.x_t + W_r_hh.h_(t-1) + b_r)\n",
    "        #z_t = sigmoid(W_z_hi.x_t + W_z_hh.h_(t-1) + b_z)\n",
    "        #c_t = tanh(W_n_hi.x_t + W_n_hh.h_(t-1) * r_t + b_c)\n",
    "        #h_t = (1-z_t) * n_t + z_t * h_(t-1)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        # input weights [W_r_hi,W_z_hi,W_c_hi]\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (3, hidden_dim, input_dim))\n",
    "        # hidden weights [W_r_hi,W_z_hi,W_c_hi]\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (3, hidden_dim, hidden_dim))\n",
    "\n",
    "        self.tanh = Tanh\n",
    "        self.sigmoid = Sigmoid\n",
    "\n",
    "        self.gates = None\n",
    "        self.H = None\n",
    "        self.C = None\n",
    "\n",
    "        if self.use_bias:\n",
    "            # bias = [r_bias,z_bias,c_bias]\n",
    "            self.bias = np.random.uniform(-sq, sq, (3, hidden_dim))\n",
    "        else:\n",
    "            self.bias = np.zeros((3, hidden_dim))\n",
    "\n",
    "    def forward(self, X_in, h_0=None, c_0=None):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        self.H = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if h_0 is not None:\n",
    "            self.H[:, 0, :] = h_0\n",
    "\n",
    "        self.gates = np.zeros((3, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # reset_gate\n",
    "            self.gates[0, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[0, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[0, :, :].T) + self.bias[0, :])\n",
    "            # z_gate\n",
    "            self.gates[1, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[1, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[1, :, :].T) + self.bias[1, :])\n",
    "            # update gate\n",
    "            self.gates[2, :, i, :] = self.tanh.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[2, :, :].T) + self.gates[0,:,i,:] * np.dot(self.H[:, i, :], self.hidden_weights[2, :, :].T) + self.bias[2, :])\n",
    "            \n",
    "            self.H[:, i + 1, :] = self.gates[1, :, i, :] * self.H[:, i, :] +  (1 - self.gates[1, :, i, :]) * self.gates[2,:,i,:]\n",
    "\n",
    "        return self.H, self.H[:, seq_len, :]\n",
    "    \n",
    "    def backward(self, X_in, dEdY):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        X_grad = np.zeros((batch_size, seq_len, self.input_dim))\n",
    "        \n",
    "        gates_grad = np.zeros((3, batch_size, seq_len, self.hidden_dim))\n",
    "        \n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                H_grad[:, i, :] = np.matmul(gates_grad[:, :, i + 1, :], self.hidden_weights).sum(axis=0) + dEdY[:, i, :]\n",
    "            else:\n",
    "                H_grad[:, i, :] = dEdY[:, i, :]\n",
    "\n",
    "            gates_grad[2, :, i, :] = (1 - self.gates[1, :, i, :]) * dEdY[:, i, :] * self.tanh.backward_calculated(self.gates[2, :, i, :])\n",
    "            gates_grad[1, :, i, :] = ((self.H[:, i, :] - self.gates[2, :, i, :]) * dEdY[:, i, :]) * self.sigmoid.backward_calculated(self.gates[1, :, i, :])\n",
    "            gates_grad[0, :, i, :] = (np.dot(gates_grad[2,:,i,:], self.hidden_weights[2,:,:].T) * self.H[:,i,:]) * self.sigmoid.backward_calculated(self.gates[0, :, i, :])\n",
    "            \n",
    "            X_grad[:, i, :] = np.dot(gates_grad[2,:,i,:], self.input_weights[2,:,:].T) + np.dot(gates_grad[1,:,i,:], self.input_weights[1,:,:].T) + np.dot(gates_grad[0,:,i,:], self.input_weights[0,:,:].T)\n",
    "                \n",
    "            h_t_T = self.H[:, i, :].T    \n",
    "                \n",
    "            dEdW_in[0, :, :] += np.dot(gates_grad[0, :, i, :].T, X_in[:, i, :])\n",
    "            dEdW_in[1, :, :] += np.dot(gates_grad[1, :, i, :].T, X_in[:, i, :])\n",
    "            dEdW_in[2, :, :] += np.dot(gates_grad[2, :, i, :].T, X_in[:, i, :])\n",
    "            \n",
    "            if i < seq_len - 1:\n",
    "                dEdW_hh[0, :, :] += np.dot(h_t_T, gates_grad[0, :, i, :])\n",
    "                dEdW_hh[1, :, :] += np.dot(h_t_T, gates_grad[1, :, i, :])\n",
    "                dEdW_hh[2, :, :] += np.dot((self.H[:, i, :] * self.gates[0, :, i, :]).T, gates_grad[2, :, i, :])\n",
    "                \n",
    "            if self.use_bias:\n",
    "                dEdB_in[0, :] += np.sum(gates_grad[0, :, i, :], axis=0)\n",
    "                dEdB_in[1, :] += np.sum(gates_grad[1, :, i, :], axis=0)\n",
    "                dEdB_in[2, :] += np.sum(gates_grad[2, :, i, :], axis=0)\n",
    "                \n",
    "        return dEdW_in, dEdW_hh, dEdB_in, X_grad\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-sarah",
   "metadata": {},
   "source": [
    "## Softmax, Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aging-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "\n",
    "x = torch.randn(N, num_classes) # logits\n",
    "y = torch.randint(num_classes, (N,)) # labels\n",
    "\n",
    "x_ = x.numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()] # one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regional-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax check: True\n"
     ]
    }
   ],
   "source": [
    "softmax_out = F.softmax(x, dim=-1)\n",
    "softmax_out\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax_out_ = softmax.forward(x_)\n",
    "\n",
    "print('Softmax check:', np.isclose(softmax_out, softmax_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "focused-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss check: True\n"
     ]
    }
   ],
   "source": [
    "cel = nn.CrossEntropyLoss()\n",
    "loss = cel(x, y).item()\n",
    "\n",
    "cel_ = CrossEntropyLoss()\n",
    "loss_ = cel_.forward(y_, x_)\n",
    "loss_\n",
    "\n",
    "print('Cross entropy loss check:', np.isclose(loss, loss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-display",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numerous-bloom",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "einstein sum subscripts string contains too many subscripts for operand 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-44e383d1bdf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mlin_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mlin_out_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Linear layer forward check: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlin_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlin_out_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-ee30d4e5dc0a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_in)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bsi,ih->bsh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_dy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\envs\\zavrsni\\lib\\site-packages\\numpy\\core\\einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspecified_out\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;31m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: einstein sum subscripts string contains too many subscripts for operand 0"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "seq_len = 3\n",
    "\n",
    "x = torch.randn(N, seq_len, requires_grad=True)\n",
    "y = torch.randint(num_classes, (N,))\n",
    "\n",
    "x_ = x.detach().numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()]\n",
    "\n",
    "linear = nn.Linear(seq_len, num_classes)\n",
    "linear_ = DenseLayer(seq_len, num_classes)\n",
    "\n",
    "linear_.weights = linear.weight.detach().numpy()\n",
    "linear_.bias = linear.bias.detach().numpy()\n",
    "\n",
    "lin_out = linear(x)\n",
    "lin_out_ = linear_.forward(x_)\n",
    "\n",
    "print('Linear layer forward check: ', np.isclose(lin_out.detach().numpy(), lin_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "minor-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7812, grad_fn=<NllLossBackward>)\n",
      "[TORCH] dE/dy:\n",
      "tensor([[ 0.0154, -0.1254,  0.0595,  0.0505],\n",
      "        [ 0.0260, -0.0758,  0.0206,  0.0292],\n",
      "        [ 0.0358,  0.0895,  0.0610, -0.1863],\n",
      "        [ 0.0995,  0.0885, -0.1909,  0.0029],\n",
      "        [ 0.0300,  0.0869,  0.0454, -0.1623]])\n",
      "\n",
      "[TORCH] dE/dW:\n",
      "tensor([[ 0.0685,  0.1054, -0.2851],\n",
      "        [ 0.4073,  0.0524, -0.2640],\n",
      "        [-0.2708, -0.4270,  0.2497],\n",
      "        [-0.2051,  0.2692,  0.2994]])\n",
      "\n",
      "[TORCH] dE/dB:\n",
      "tensor([ 0.2067,  0.0636, -0.0044, -0.2660])\n",
      "\n",
      "[TORCH] dE/dX:\n",
      "tensor([[ 0.0082, -0.0504,  0.0747],\n",
      "        [ 0.0138, -0.0136,  0.0439],\n",
      "        [ 0.0379,  0.0321, -0.1905],\n",
      "        [ 0.0463,  0.1636, -0.0177],\n",
      "        [ 0.0319,  0.0328, -0.1681]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = cel(lin_out, y)\n",
    "lin_out.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(f'[TORCH] dE/dy:\\n{lin_out.grad}\\n')\n",
    "print(f'[TORCH] dE/dW:\\n{linear.weight.grad}\\n')\n",
    "print(f'[TORCH] dE/dB:\\n{linear.bias.grad}\\n')\n",
    "print(f'[TORCH] dE/dX:\\n{x.grad}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "gothic-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = cel_.forward(y_, lin_out_)\n",
    "\n",
    "de_dy = cel_.backward(y_)\n",
    "de_dx, de_dw, de_db = linear_.backward(de_dy, x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "liked-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dy:\n",
      "tensor([[[-0.4858, -0.9538, -0.4078,  0.4785, -0.1805],\n",
      "         [-1.4176,  1.4444, -1.2428, -0.0258, -0.1189],\n",
      "         [-1.3722, -1.4655, -1.6678,  1.1944, -1.8137]],\n",
      "\n",
      "        [[-0.6549,  0.6896, -1.5850,  0.1409,  0.7199],\n",
      "         [ 1.3465, -1.1447,  0.3793,  0.3712,  1.4250],\n",
      "         [ 0.0330,  1.5172,  0.5199,  1.6614, -1.3760]],\n",
      "\n",
      "        [[ 0.3420, -0.6921,  1.2166, -1.3336,  0.8845],\n",
      "         [-0.6354,  0.0516, -1.1423, -0.2269,  0.1890],\n",
      "         [ 0.5430,  0.5265, -1.1912,  2.3043, -0.3202]],\n",
      "\n",
      "        [[-0.2344, -0.2877,  1.8131, -0.6401, -1.1516],\n",
      "         [ 0.4000,  1.3476, -0.4403, -0.9127, -0.5773],\n",
      "         [-0.7366, -0.8067, -0.1965,  0.1194,  0.4144]]])\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'de_dw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-af77db6fc6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[CUSTOM] dE/dy:\\n{de_dy}\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[CUSTOM] dE/dW:\\n{de_dw}\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[CUSTOM] dE/dB:\\n{de_db}\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[CUSTOM] dE/dX:\\n{de_dx}\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'de_dw' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'[CUSTOM] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[CUSTOM] dE/dW:\\n{de_dw}\\n')\n",
    "print(f'[CUSTOM] dE/dB:\\n{de_db}\\n')\n",
    "print(f'[CUSTOM] dE/dX:\\n{de_dx}\\n')\n",
    "\n",
    "print('Check dE/dy:', np.isclose(lin_out.grad, de_dy).all())\n",
    "print('Check dE/dX:', np.isclose(x.grad, de_dx).all())\n",
    "print('Check dE/dW:', np.isclose(linear.weight.grad, de_dw).all())\n",
    "print('Check dE/dB:', np.isclose(linear.bias.grad, de_db).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-briefs",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "coordinated-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN layer forward check:  True\n",
      "RNN layer forward check last hidden:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "rnn = nn.RNN(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "rnn_ = RnnLayer(emb_dim, hidden_dim, seq_len, N, use_bias=False)\n",
    "rnn_.input_weights = rnn.weight_ih_l0.detach().numpy()\n",
    "rnn_.hidden_weights = rnn.weight_hh_l0.detach().numpy()\n",
    "\n",
    "x.retain_grad()\n",
    "rnn_out, h_n = rnn(x)\n",
    "rnn_out_, h_n_ = rnn_.forward(x_)\n",
    "rnn_out__ = rnn_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('RNN layer forward check: ', np.isclose(rnn_out.detach().numpy(), rnn_out__, atol=1e-3).all())\n",
    "print('RNN layer forward check last hidden: ', np.isclose(h_n.detach().numpy(), h_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "surprised-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TORCH] dE/dWih:\n",
      "tensor([[ 0.2449, -1.7056,  2.5466,  1.6024, -2.7113,  1.4530],\n",
      "        [-0.9463, -0.3401,  0.0679, -7.3683,  1.4519,  4.1646],\n",
      "        [ 3.3150,  1.0614,  5.3475,  1.8725, -3.1538,  2.6769],\n",
      "        [ 7.0559, -7.0339,  3.7580,  6.3774,  3.6920, 10.4161],\n",
      "        [-5.8275,  0.6323, -1.1140, -0.4600,  1.7497, -3.0474]])\n",
      "\n",
      "[TORCH] dE/dWhh:\n",
      "tensor([[ 2.1474, -0.7241,  0.8992, -1.1755,  0.3038],\n",
      "        [ 1.9606,  0.0551, -0.5860, -0.6625, -1.1284],\n",
      "        [ 1.1062,  1.4638, -0.6872,  0.3111, -0.5890],\n",
      "        [ 2.0179,  1.7246,  2.4917, -1.2085, -0.8041],\n",
      "        [ 1.0057,  1.5027, -1.4213, -0.5773, -2.0118]])\n",
      "\n",
      "[TORCH] dE/dy:\n",
      "tensor([[[ 6.2996e-01, -1.2433e+00,  1.2866e+00, -5.8629e-02, -3.5287e-01],\n",
      "         [ 1.0334e+00,  1.4286e-01, -1.5877e+00, -1.5272e+00,  6.0149e-01],\n",
      "         [-1.1036e+00, -1.1458e+00, -4.2392e-01,  1.5527e+00,  7.6814e-01]],\n",
      "\n",
      "        [[-1.3452e-04, -6.8384e-01, -1.0651e+00, -5.0410e-01,  2.1486e+00],\n",
      "         [ 5.2262e-01,  2.7422e-01,  1.1414e-01,  4.3588e-01,  1.3690e+00],\n",
      "         [ 1.2424e+00, -2.2931e+00, -5.7983e-02,  5.7007e-01, -1.0312e+00]],\n",
      "\n",
      "        [[-1.3154e-01, -4.7957e-01, -1.8189e+00,  9.2028e-01,  2.7710e-01],\n",
      "         [ 4.8000e-01,  8.1137e-02,  2.4101e-01, -1.4772e+00, -5.4957e-01],\n",
      "         [-3.2614e-01, -6.1632e-01, -3.5226e-01, -4.9969e-01, -1.5957e+00]],\n",
      "\n",
      "        [[-7.9348e-02,  9.4617e-01,  1.4713e-01, -1.3796e-01, -7.0440e-01],\n",
      "         [ 7.5995e-01, -8.6068e-02, -5.4839e-01, -3.4787e+00,  1.7772e+00],\n",
      "         [ 1.6808e+00, -7.3135e-01, -6.4731e-01,  3.6970e-01,  6.9971e-02]],\n",
      "\n",
      "        [[-1.4954e+00,  1.6106e+00, -1.3490e+00,  6.7242e-01,  3.5195e-01],\n",
      "         [-1.6328e+00, -1.1841e+00,  4.2379e-01,  6.4959e-01, -1.0291e+00],\n",
      "         [ 7.7518e-01,  5.9714e-02, -1.6003e-01,  1.4488e+00, -7.8541e-01]]])\n",
      "\n",
      "[TORCH] dE/dH:\n",
      "tensor([[[ 6.2996e-01, -1.2433e+00,  1.2866e+00, -5.8629e-02, -3.5287e-01],\n",
      "         [ 1.0334e+00,  1.4286e-01, -1.5877e+00, -1.5272e+00,  6.0149e-01],\n",
      "         [-1.1036e+00, -1.1458e+00, -4.2392e-01,  1.5527e+00,  7.6814e-01]],\n",
      "\n",
      "        [[-1.3452e-04, -6.8384e-01, -1.0651e+00, -5.0410e-01,  2.1486e+00],\n",
      "         [ 5.2262e-01,  2.7422e-01,  1.1414e-01,  4.3588e-01,  1.3690e+00],\n",
      "         [ 1.2424e+00, -2.2931e+00, -5.7983e-02,  5.7007e-01, -1.0312e+00]],\n",
      "\n",
      "        [[-1.3154e-01, -4.7957e-01, -1.8189e+00,  9.2028e-01,  2.7710e-01],\n",
      "         [ 4.8000e-01,  8.1137e-02,  2.4101e-01, -1.4772e+00, -5.4957e-01],\n",
      "         [-3.2614e-01, -6.1632e-01, -3.5226e-01, -4.9969e-01, -1.5957e+00]],\n",
      "\n",
      "        [[-7.9348e-02,  9.4617e-01,  1.4713e-01, -1.3796e-01, -7.0440e-01],\n",
      "         [ 7.5995e-01, -8.6068e-02, -5.4839e-01, -3.4787e+00,  1.7772e+00],\n",
      "         [ 1.6808e+00, -7.3135e-01, -6.4731e-01,  3.6970e-01,  6.9971e-02]],\n",
      "\n",
      "        [[-1.4954e+00,  1.6106e+00, -1.3490e+00,  6.7242e-01,  3.5195e-01],\n",
      "         [-1.6328e+00, -1.1841e+00,  4.2379e-01,  6.4959e-01, -1.0291e+00],\n",
      "         [ 7.7518e-01,  5.9714e-02, -1.6003e-01,  1.4488e+00, -7.8541e-01]]])\n",
      "\n",
      "X_grad=tensor([[[-0.1022, -0.1400,  0.2461,  0.7432, -0.6906, -0.1609],\n",
      "         [ 1.1560,  0.6716, -0.7482, -0.2238,  0.2669,  0.0498],\n",
      "         [ 0.2565, -0.7631,  0.8179, -0.3056, -0.0451, -0.1199]],\n",
      "\n",
      "        [[ 0.9383, -0.3103,  0.0120, -0.4912, -0.4502, -0.4685],\n",
      "         [ 0.5133, -0.1718,  0.0161, -0.3355,  0.0341, -0.1621],\n",
      "         [ 0.0085, -0.3796,  0.4896,  0.6692, -0.0091,  0.0129]],\n",
      "\n",
      "        [[ 0.9109,  0.0026,  0.4971, -0.5118,  0.5224,  0.4300],\n",
      "         [ 0.4005,  0.1597, -0.5231,  0.3390, -0.1141, -0.2847],\n",
      "         [-0.3803,  0.0769,  0.0518,  0.6145,  0.5046,  0.2923]],\n",
      "\n",
      "        [[-0.6307,  0.5820,  0.1561,  0.2124,  0.2431,  0.7153],\n",
      "         [ 1.7609,  0.0573, -1.1866, -0.0404, -0.6010, -1.0781],\n",
      "         [ 0.4859,  0.3996, -0.1113,  0.2956, -0.1376,  0.1764]],\n",
      "\n",
      "        [[-0.2483,  0.7266, -0.0951, -0.9642,  0.9162,  0.8475],\n",
      "         [-0.3513, -0.9545,  0.6496, -0.1104,  0.3770, -0.2084],\n",
      "         [-0.2966,  0.4414,  0.2223,  0.2768,  0.3052,  0.6373]]])\n"
     ]
    }
   ],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "rnn_out.retain_grad()\n",
    "rnn_out.backward(de_dy)\n",
    "\n",
    "print(f'[TORCH] dE/dWih:\\n{rnn.weight_ih_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dWhh:\\n{rnn.weight_hh_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[TORCH] dE/dH:\\n{rnn_out.grad}\\n')\n",
    "print(f'X_grad={x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fourth-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dWih:\n",
      "[[-0.84065264 -7.068542    5.7418056   0.9002589  -5.5570254  -3.4874358 ]\n",
      " [-1.5509931   0.21796954 -1.5923568  -1.5642024   0.2130806  -0.2485686 ]\n",
      " [ 0.6651537  -0.68229634 -2.7304165   3.6788378  -0.33462602 -5.857197  ]\n",
      " [-1.3273039   1.052206   -0.39686674 -6.426206    1.8830379   4.0530033 ]\n",
      " [ 3.228954   -3.563257    2.8654382   1.1615889   0.6939566  -4.1528783 ]]\n",
      "\n",
      "[CUSTOM] dE/dWhh:\n",
      "[[-0.05816048  0.36913016  0.46005616  0.3695405   0.81958103]\n",
      " [-0.3268031   2.1463962   0.11377817  1.9113404  -0.9561599 ]\n",
      " [ 0.89730275 -1.4937084  -0.2324918   0.04177425  0.8274532 ]\n",
      " [ 2.3509252   0.17922425 -0.41860518 -0.6692463  -2.4644444 ]\n",
      " [ 1.0182368   0.737316    1.3249016   0.15629236  0.80138135]]\n",
      "\n",
      "RNN layer gradient check dEdW_in:  True\n",
      "RNN layer gradient check dEdW_hh:  True\n"
     ]
    }
   ],
   "source": [
    "dEdW_in, dEdW_hh, _ = rnn_.backward(x_, rnn_out_, de_dy_)\n",
    "\n",
    "print(f'[CUSTOM] dE/dWih:\\n{dEdW_in}\\n')\n",
    "print(f'[CUSTOM] dE/dWhh:\\n{dEdW_hh}\\n')\n",
    "\n",
    "print('RNN layer gradient check dEdW_in: ', np.isclose(rnn.weight_ih_l0.grad.numpy(), dEdW_in).all())\n",
    "print('RNN layer gradient check dEdW_hh: ', np.isclose(rnn.weight_hh_l0.grad.numpy(), dEdW_hh).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-rebate",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "private-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer forward check:  True\n",
      "LSTM layer forward check last hidden:  True\n",
      "LSTM layer forward check last c_n:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 32\n",
    "emb_dim = 300\n",
    "seq_len = 32\n",
    "hidden_dim = 200\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "lstm = nn.LSTM(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "lstm_ = LSTMLayer(emb_dim, hidden_dim, use_bias=False)\n",
    "wih = lstm.weight_ih_l0.detach().numpy()\n",
    "whh = lstm.weight_hh_l0.detach().numpy()\n",
    "\n",
    "lstm_.input_weights[0,:,:] = wih[0:hidden_dim, :]\n",
    "lstm_.input_weights[1,:,:] = wih[hidden_dim: 2*hidden_dim, :]\n",
    "lstm_.input_weights[2,:,:] = wih[2*hidden_dim: 3*hidden_dim, :]\n",
    "lstm_.input_weights[3,:,:] = wih[3*hidden_dim: 4*hidden_dim, :]\n",
    "\n",
    "lstm_.hidden_weights[0,:,:] = whh[0:hidden_dim, :]\n",
    "lstm_.hidden_weights[1,:,:] = whh[hidden_dim: 2*hidden_dim, :]\n",
    "lstm_.hidden_weights[2,:,:] = whh[2*hidden_dim: 3*hidden_dim, :]\n",
    "lstm_.hidden_weights[3,:,:] = whh[3*hidden_dim: 4*hidden_dim, :]\n",
    "\n",
    "\n",
    "lstm_out, h_n = lstm(x)\n",
    "lstm_out_, h_n_, c_n_ = lstm_.forward(x_)\n",
    "lstm_out__ = lstm_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('LSTM layer forward check: ', np.isclose(lstm_out.detach().numpy(), lstm_out__, atol=1e-3).all())\n",
    "print('LSTM layer forward check last hidden: ', np.isclose(h_n[0].detach().numpy(), h_n_, atol=1e-3).all())\n",
    "print('LSTM layer forward check last c_n: ', np.isclose(h_n[1].detach().numpy(), c_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "painful-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "x.retain_grad()\n",
    "lstm_out.backward(de_dy)\n",
    "\n",
    "dEdW_in, dEdW_hh, a, X_grad = lstm_.backward(x_, de_dy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "honest-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer gradient check dEdW_in:  True\n",
      "LSTM layer gradient check dEdW_hh:  True\n",
      "LSTM layer gradient check dEdX:  True\n"
     ]
    }
   ],
   "source": [
    "print('LSTM layer gradient check dEdW_in: ', np.isclose(lstm.weight_ih_l0.grad.numpy(), dEdW_in.reshape(4*hidden_dim,emb_dim), atol=1e-3).all())\n",
    "print('LSTM layer gradient check dEdW_hh: ', np.isclose(lstm.weight_hh_l0.grad.numpy(), dEdW_hh.reshape(4*hidden_dim,hidden_dim), atol=1e-3).all())\n",
    "print('LSTM layer gradient check dEdX: ', np.isclose(x.grad.numpy(), X_grad, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-collaboration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "classical-tissue",
   "metadata": {},
   "source": [
    "## Pokusaj pravljenja torch modela rnn, fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "turkish-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "important-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 3\n",
    "hidden_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "favorite-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.RNN(emb_dim, hidden_dim, bias=False, batch_first=True),\n",
    "    torch.nn.Linear(hidden_dim, 5, bias=False)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-persian",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "inclusive-accommodation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU layer forward check:  True\n",
      "GRU layer forward check last hidden:  True\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 1\n",
    "hidden_dim = 8\n",
    "\n",
    "#N = 20\n",
    "#emb_dim = 40\n",
    "#seq_len = 32\n",
    "#hidden_dim = 200\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "gru = nn.GRU(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "gru_ = GRULayer(emb_dim, hidden_dim, use_bias=False)\n",
    "wih = gru.weight_ih_l0.detach().numpy()\n",
    "whh = gru.weight_hh_l0.detach().numpy()\n",
    "\n",
    "gru_.input_weights[0,:,:] = wih[0:hidden_dim, :]\n",
    "gru_.input_weights[1,:,:] = wih[hidden_dim: 2*hidden_dim, :]\n",
    "gru_.input_weights[2,:,:] = wih[2*hidden_dim: 3*hidden_dim, :]\n",
    "\n",
    "gru_.hidden_weights[0,:,:] = whh[0:hidden_dim, :]\n",
    "gru_.hidden_weights[1,:,:] = whh[hidden_dim: 2*hidden_dim, :]\n",
    "gru_.hidden_weights[2,:,:] = whh[2*hidden_dim: 3*hidden_dim, :]\n",
    "\n",
    "gru_out, h_n = gru(x)\n",
    "gru_out_, h_n_ = gru_.forward(x_)\n",
    "gru_out__ = gru_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('GRU layer forward check: ', np.isclose(gru_out.detach().numpy(), gru_out__, atol=1e-3).all())\n",
    "print('GRU layer forward check last hidden: ', np.isclose(h_n.detach().numpy(), h_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "loving-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "x.retain_grad()\n",
    "gru_out.backward(de_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "utility-franchise",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,8) and (6,8) not aligned: 8 (dim 1) != 6 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-611c15d37c99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdEdW_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdEdW_hh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgru_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_dy_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-bed065e87e04>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, X_in, dEdY)\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_calculated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m             \u001b[0mX_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[0mh_t_T\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,8) and (6,8) not aligned: 8 (dim 1) != 6 (dim 0)"
     ]
    }
   ],
   "source": [
    "dEdW_in, dEdW_hh, a, X_grad = gru_.backward(x_, de_dy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "selective-range",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1,2],[2,2]],[[3,2],[1,2]],[[1,2],[2,2]]])\n",
    "np.prod(np.array(a.shape[0:len(a.shape)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "living-deposit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[1]]]), array([[2]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[[1]]])\n",
    "b = np.array([[2]])\n",
    "\n",
    "c = [a, b]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recognized-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1, 2],[2, 3],[3, 4],[4, 5]], [[1, 2],[2, 3],[3, 4],[4, 5]]])\n",
    "#a = 2,4,2\n",
    "b = np.array([[1, 2, 4],[2, 3, 5],[3, 4, 4],[4, 5, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-reproduction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-render",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
