{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "objective-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "marine-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the length of the longest string in our data\n",
    "maxlen = len(max(text, key=len))\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of\n",
    "# the sentence matches the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "      text[i] += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiac-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "  input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove first character for target sequence\n",
    "  target_seq.append(text[i][1:])\n",
    "  print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "documented-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "independent-heating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 3\n"
     ]
    }
   ],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "print(seq_len, batch_size)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "solid-israel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1.]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inner-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_seq = torch.from_numpy(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cosmetic-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "catholic-liberal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]]),\n",
       " tensor([[11.,  0.,  6.,  5., 14.,  1.,  6., 16.,  2., 11.,  6.,  0., 14., 12.],\n",
       "         [14., 14.,  8.,  6.,  4.,  6., 16., 15.,  6.,  9.,  4., 13., 11.,  6.],\n",
       "         [16., 10., 11.,  6., 16.,  6., 13.,  4.,  3., 11.,  6.,  8., 16.,  0.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "humanitarian-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "warming-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "major-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 1000\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accompanied-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000............. Loss: 2.8535\n",
      "Epoch: 2/1000............. Loss: 2.7942\n",
      "Epoch: 3/1000............. Loss: 2.7427\n",
      "Epoch: 4/1000............. Loss: 2.6966\n",
      "Epoch: 5/1000............. Loss: 2.6542\n",
      "Epoch: 6/1000............. Loss: 2.6142\n",
      "Epoch: 7/1000............. Loss: 2.5761\n",
      "Epoch: 8/1000............. Loss: 2.5396\n",
      "Epoch: 9/1000............. Loss: 2.5048\n",
      "Epoch: 10/1000............. Loss: 2.4718\n",
      "Epoch: 11/1000............. Loss: 2.4407\n",
      "Epoch: 12/1000............. Loss: 2.4114\n",
      "Epoch: 13/1000............. Loss: 2.3840\n",
      "Epoch: 14/1000............. Loss: 2.3581\n",
      "Epoch: 15/1000............. Loss: 2.3336\n",
      "Epoch: 16/1000............. Loss: 2.3099\n",
      "Epoch: 17/1000............. Loss: 2.2862\n",
      "Epoch: 18/1000............. Loss: 2.2616\n",
      "Epoch: 19/1000............. Loss: 2.2352\n",
      "Epoch: 20/1000............. Loss: 2.2064\n",
      "Epoch: 21/1000............. Loss: 2.1749\n",
      "Epoch: 22/1000............. Loss: 2.1409\n",
      "Epoch: 23/1000............. Loss: 2.1047\n",
      "Epoch: 24/1000............. Loss: 2.0668\n",
      "Epoch: 25/1000............. Loss: 2.0274\n",
      "Epoch: 26/1000............. Loss: 1.9868\n",
      "Epoch: 27/1000............. Loss: 1.9456\n",
      "Epoch: 28/1000............. Loss: 1.9038\n",
      "Epoch: 29/1000............. Loss: 1.8617\n",
      "Epoch: 30/1000............. Loss: 1.8192\n",
      "Epoch: 31/1000............. Loss: 1.7760\n",
      "Epoch: 32/1000............. Loss: 1.7320\n",
      "Epoch: 33/1000............. Loss: 1.6870\n",
      "Epoch: 34/1000............. Loss: 1.6411\n",
      "Epoch: 35/1000............. Loss: 1.5944\n",
      "Epoch: 36/1000............. Loss: 1.5476\n",
      "Epoch: 37/1000............. Loss: 1.5011\n",
      "Epoch: 38/1000............. Loss: 1.4556\n",
      "Epoch: 39/1000............. Loss: 1.4115\n",
      "Epoch: 40/1000............. Loss: 1.3688\n",
      "Epoch: 41/1000............. Loss: 1.3272\n",
      "Epoch: 42/1000............. Loss: 1.2861\n",
      "Epoch: 43/1000............. Loss: 1.2450\n",
      "Epoch: 44/1000............. Loss: 1.2037\n",
      "Epoch: 45/1000............. Loss: 1.1623\n",
      "Epoch: 46/1000............. Loss: 1.1213\n",
      "Epoch: 47/1000............. Loss: 1.0810\n",
      "Epoch: 48/1000............. Loss: 1.0419\n",
      "Epoch: 49/1000............. Loss: 1.0043\n",
      "Epoch: 50/1000............. Loss: 0.9680\n",
      "Epoch: 51/1000............. Loss: 0.9329\n",
      "Epoch: 52/1000............. Loss: 0.8986\n",
      "Epoch: 53/1000............. Loss: 0.8654\n",
      "Epoch: 54/1000............. Loss: 0.8335\n",
      "Epoch: 55/1000............. Loss: 0.8032\n",
      "Epoch: 56/1000............. Loss: 0.7739\n",
      "Epoch: 57/1000............. Loss: 0.7451\n",
      "Epoch: 58/1000............. Loss: 0.7165\n",
      "Epoch: 59/1000............. Loss: 0.6887\n",
      "Epoch: 60/1000............. Loss: 0.6622\n",
      "Epoch: 61/1000............. Loss: 0.6366\n",
      "Epoch: 62/1000............. Loss: 0.6114\n",
      "Epoch: 63/1000............. Loss: 0.5871\n",
      "Epoch: 64/1000............. Loss: 0.5639\n",
      "Epoch: 65/1000............. Loss: 0.5417\n",
      "Epoch: 66/1000............. Loss: 0.5202\n",
      "Epoch: 67/1000............. Loss: 0.4995\n",
      "Epoch: 68/1000............. Loss: 0.4798\n",
      "Epoch: 69/1000............. Loss: 0.4609\n",
      "Epoch: 70/1000............. Loss: 0.4426\n",
      "Epoch: 71/1000............. Loss: 0.4251\n",
      "Epoch: 72/1000............. Loss: 0.4082\n",
      "Epoch: 73/1000............. Loss: 0.3919\n",
      "Epoch: 74/1000............. Loss: 0.3766\n",
      "Epoch: 75/1000............. Loss: 0.3620\n",
      "Epoch: 76/1000............. Loss: 0.3480\n",
      "Epoch: 77/1000............. Loss: 0.3345\n",
      "Epoch: 78/1000............. Loss: 0.3218\n",
      "Epoch: 79/1000............. Loss: 0.3097\n",
      "Epoch: 80/1000............. Loss: 0.2981\n",
      "Epoch: 81/1000............. Loss: 0.2872\n",
      "Epoch: 82/1000............. Loss: 0.2769\n",
      "Epoch: 83/1000............. Loss: 0.2671\n",
      "Epoch: 84/1000............. Loss: 0.2578\n",
      "Epoch: 85/1000............. Loss: 0.2489\n",
      "Epoch: 86/1000............. Loss: 0.2405\n",
      "Epoch: 87/1000............. Loss: 0.2326\n",
      "Epoch: 88/1000............. Loss: 0.2250\n",
      "Epoch: 89/1000............. Loss: 0.2179\n",
      "Epoch: 90/1000............. Loss: 0.2110\n",
      "Epoch: 91/1000............. Loss: 0.2046\n",
      "Epoch: 92/1000............. Loss: 0.1984\n",
      "Epoch: 93/1000............. Loss: 0.1926\n",
      "Epoch: 94/1000............. Loss: 0.1870\n",
      "Epoch: 95/1000............. Loss: 0.1818\n",
      "Epoch: 96/1000............. Loss: 0.1768\n",
      "Epoch: 97/1000............. Loss: 0.1721\n",
      "Epoch: 98/1000............. Loss: 0.1675\n",
      "Epoch: 99/1000............. Loss: 0.1633\n",
      "Epoch: 100/1000............. Loss: 0.1592\n",
      "Epoch: 101/1000............. Loss: 0.1553\n",
      "Epoch: 102/1000............. Loss: 0.1516\n",
      "Epoch: 103/1000............. Loss: 0.1481\n",
      "Epoch: 104/1000............. Loss: 0.1448\n",
      "Epoch: 105/1000............. Loss: 0.1416\n",
      "Epoch: 106/1000............. Loss: 0.1385\n",
      "Epoch: 107/1000............. Loss: 0.1356\n",
      "Epoch: 108/1000............. Loss: 0.1328\n",
      "Epoch: 109/1000............. Loss: 0.1302\n",
      "Epoch: 110/1000............. Loss: 0.1277\n",
      "Epoch: 111/1000............. Loss: 0.1253\n",
      "Epoch: 112/1000............. Loss: 0.1229\n",
      "Epoch: 113/1000............. Loss: 0.1207\n",
      "Epoch: 114/1000............. Loss: 0.1186\n",
      "Epoch: 115/1000............. Loss: 0.1166\n",
      "Epoch: 116/1000............. Loss: 0.1146\n",
      "Epoch: 117/1000............. Loss: 0.1128\n",
      "Epoch: 118/1000............. Loss: 0.1110\n",
      "Epoch: 119/1000............. Loss: 0.1092\n",
      "Epoch: 120/1000............. Loss: 0.1076\n",
      "Epoch: 121/1000............. Loss: 0.1060\n",
      "Epoch: 122/1000............. Loss: 0.1045\n",
      "Epoch: 123/1000............. Loss: 0.1030\n",
      "Epoch: 124/1000............. Loss: 0.1016\n",
      "Epoch: 125/1000............. Loss: 0.1002\n",
      "Epoch: 126/1000............. Loss: 0.0989\n",
      "Epoch: 127/1000............. Loss: 0.0976\n",
      "Epoch: 128/1000............. Loss: 0.0964\n",
      "Epoch: 129/1000............. Loss: 0.0952\n",
      "Epoch: 130/1000............. Loss: 0.0940\n",
      "Epoch: 131/1000............. Loss: 0.0929\n",
      "Epoch: 132/1000............. Loss: 0.0918\n",
      "Epoch: 133/1000............. Loss: 0.0908\n",
      "Epoch: 134/1000............. Loss: 0.0898\n",
      "Epoch: 135/1000............. Loss: 0.0888\n",
      "Epoch: 136/1000............. Loss: 0.0878\n",
      "Epoch: 137/1000............. Loss: 0.0869\n",
      "Epoch: 138/1000............. Loss: 0.0860\n",
      "Epoch: 139/1000............. Loss: 0.0852\n",
      "Epoch: 140/1000............. Loss: 0.0843\n",
      "Epoch: 141/1000............. Loss: 0.0835\n",
      "Epoch: 142/1000............. Loss: 0.0827\n",
      "Epoch: 143/1000............. Loss: 0.0819\n",
      "Epoch: 144/1000............. Loss: 0.0812\n",
      "Epoch: 145/1000............. Loss: 0.0805\n",
      "Epoch: 146/1000............. Loss: 0.0798\n",
      "Epoch: 147/1000............. Loss: 0.0791\n",
      "Epoch: 148/1000............. Loss: 0.0784\n",
      "Epoch: 149/1000............. Loss: 0.0777\n",
      "Epoch: 150/1000............. Loss: 0.0771\n",
      "Epoch: 151/1000............. Loss: 0.0765\n",
      "Epoch: 152/1000............. Loss: 0.0759\n",
      "Epoch: 153/1000............. Loss: 0.0753\n",
      "Epoch: 154/1000............. Loss: 0.0747\n",
      "Epoch: 155/1000............. Loss: 0.0741\n",
      "Epoch: 156/1000............. Loss: 0.0736\n",
      "Epoch: 157/1000............. Loss: 0.0730\n",
      "Epoch: 158/1000............. Loss: 0.0725\n",
      "Epoch: 159/1000............. Loss: 0.0720\n",
      "Epoch: 160/1000............. Loss: 0.0715\n",
      "Epoch: 161/1000............. Loss: 0.0710\n",
      "Epoch: 162/1000............. Loss: 0.0705\n",
      "Epoch: 163/1000............. Loss: 0.0700\n",
      "Epoch: 164/1000............. Loss: 0.0696\n",
      "Epoch: 165/1000............. Loss: 0.0691\n",
      "Epoch: 166/1000............. Loss: 0.0687\n",
      "Epoch: 167/1000............. Loss: 0.0683\n",
      "Epoch: 168/1000............. Loss: 0.0678\n",
      "Epoch: 169/1000............. Loss: 0.0674\n",
      "Epoch: 170/1000............. Loss: 0.0670\n",
      "Epoch: 171/1000............. Loss: 0.0666\n",
      "Epoch: 172/1000............. Loss: 0.0662\n",
      "Epoch: 173/1000............. Loss: 0.0658\n",
      "Epoch: 174/1000............. Loss: 0.0655\n",
      "Epoch: 175/1000............. Loss: 0.0651\n",
      "Epoch: 176/1000............. Loss: 0.0647\n",
      "Epoch: 177/1000............. Loss: 0.0644\n",
      "Epoch: 178/1000............. Loss: 0.0640\n",
      "Epoch: 179/1000............. Loss: 0.0637\n",
      "Epoch: 180/1000............. Loss: 0.0634\n",
      "Epoch: 181/1000............. Loss: 0.0630\n",
      "Epoch: 182/1000............. Loss: 0.0627\n",
      "Epoch: 183/1000............. Loss: 0.0624\n",
      "Epoch: 184/1000............. Loss: 0.0621\n",
      "Epoch: 185/1000............. Loss: 0.0618\n",
      "Epoch: 186/1000............. Loss: 0.0615\n",
      "Epoch: 187/1000............. Loss: 0.0612\n",
      "Epoch: 188/1000............. Loss: 0.0609\n",
      "Epoch: 189/1000............. Loss: 0.0606\n",
      "Epoch: 190/1000............. Loss: 0.0603\n",
      "Epoch: 191/1000............. Loss: 0.0601\n",
      "Epoch: 192/1000............. Loss: 0.0598\n",
      "Epoch: 193/1000............. Loss: 0.0595\n",
      "Epoch: 194/1000............. Loss: 0.0593\n",
      "Epoch: 195/1000............. Loss: 0.0590\n",
      "Epoch: 196/1000............. Loss: 0.0588\n",
      "Epoch: 197/1000............. Loss: 0.0585\n",
      "Epoch: 198/1000............. Loss: 0.0583\n",
      "Epoch: 199/1000............. Loss: 0.0580\n",
      "Epoch: 200/1000............. Loss: 0.0578\n",
      "Epoch: 201/1000............. Loss: 0.0575\n",
      "Epoch: 202/1000............. Loss: 0.0573\n",
      "Epoch: 203/1000............. Loss: 0.0571\n",
      "Epoch: 204/1000............. Loss: 0.0569\n",
      "Epoch: 205/1000............. Loss: 0.0567\n",
      "Epoch: 206/1000............. Loss: 0.0564\n",
      "Epoch: 207/1000............. Loss: 0.0562\n",
      "Epoch: 208/1000............. Loss: 0.0560\n",
      "Epoch: 209/1000............. Loss: 0.0558\n",
      "Epoch: 210/1000............. Loss: 0.0556\n",
      "Epoch: 211/1000............. Loss: 0.0554\n",
      "Epoch: 212/1000............. Loss: 0.0552\n",
      "Epoch: 213/1000............. Loss: 0.0550\n",
      "Epoch: 214/1000............. Loss: 0.0548\n",
      "Epoch: 215/1000............. Loss: 0.0546\n",
      "Epoch: 216/1000............. Loss: 0.0545\n",
      "Epoch: 217/1000............. Loss: 0.0543\n",
      "Epoch: 218/1000............. Loss: 0.0541\n",
      "Epoch: 219/1000............. Loss: 0.0539\n",
      "Epoch: 220/1000............. Loss: 0.0537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221/1000............. Loss: 0.0536\n",
      "Epoch: 222/1000............. Loss: 0.0534\n",
      "Epoch: 223/1000............. Loss: 0.0532\n",
      "Epoch: 224/1000............. Loss: 0.0531\n",
      "Epoch: 225/1000............. Loss: 0.0529\n",
      "Epoch: 226/1000............. Loss: 0.0527\n",
      "Epoch: 227/1000............. Loss: 0.0526\n",
      "Epoch: 228/1000............. Loss: 0.0524\n",
      "Epoch: 229/1000............. Loss: 0.0523\n",
      "Epoch: 230/1000............. Loss: 0.0521\n",
      "Epoch: 231/1000............. Loss: 0.0520\n",
      "Epoch: 232/1000............. Loss: 0.0518\n",
      "Epoch: 233/1000............. Loss: 0.0517\n",
      "Epoch: 234/1000............. Loss: 0.0515\n",
      "Epoch: 235/1000............. Loss: 0.0514\n",
      "Epoch: 236/1000............. Loss: 0.0512\n",
      "Epoch: 237/1000............. Loss: 0.0511\n",
      "Epoch: 238/1000............. Loss: 0.0510\n",
      "Epoch: 239/1000............. Loss: 0.0508\n",
      "Epoch: 240/1000............. Loss: 0.0507\n",
      "Epoch: 241/1000............. Loss: 0.0505\n",
      "Epoch: 242/1000............. Loss: 0.0504\n",
      "Epoch: 243/1000............. Loss: 0.0503\n",
      "Epoch: 244/1000............. Loss: 0.0502\n",
      "Epoch: 245/1000............. Loss: 0.0500\n",
      "Epoch: 246/1000............. Loss: 0.0499\n",
      "Epoch: 247/1000............. Loss: 0.0498\n",
      "Epoch: 248/1000............. Loss: 0.0497\n",
      "Epoch: 249/1000............. Loss: 0.0495\n",
      "Epoch: 250/1000............. Loss: 0.0494\n",
      "Epoch: 251/1000............. Loss: 0.0493\n",
      "Epoch: 252/1000............. Loss: 0.0492\n",
      "Epoch: 253/1000............. Loss: 0.0491\n",
      "Epoch: 254/1000............. Loss: 0.0490\n",
      "Epoch: 255/1000............. Loss: 0.0489\n",
      "Epoch: 256/1000............. Loss: 0.0487\n",
      "Epoch: 257/1000............. Loss: 0.0486\n",
      "Epoch: 258/1000............. Loss: 0.0485\n",
      "Epoch: 259/1000............. Loss: 0.0484\n",
      "Epoch: 260/1000............. Loss: 0.0483\n",
      "Epoch: 261/1000............. Loss: 0.0482\n",
      "Epoch: 262/1000............. Loss: 0.0481\n",
      "Epoch: 263/1000............. Loss: 0.0480\n",
      "Epoch: 264/1000............. Loss: 0.0479\n",
      "Epoch: 265/1000............. Loss: 0.0478\n",
      "Epoch: 266/1000............. Loss: 0.0477\n",
      "Epoch: 267/1000............. Loss: 0.0476\n",
      "Epoch: 268/1000............. Loss: 0.0475\n",
      "Epoch: 269/1000............. Loss: 0.0474\n",
      "Epoch: 270/1000............. Loss: 0.0473\n",
      "Epoch: 271/1000............. Loss: 0.0472\n",
      "Epoch: 272/1000............. Loss: 0.0471\n",
      "Epoch: 273/1000............. Loss: 0.0470\n",
      "Epoch: 274/1000............. Loss: 0.0470\n",
      "Epoch: 275/1000............. Loss: 0.0469\n",
      "Epoch: 276/1000............. Loss: 0.0468\n",
      "Epoch: 277/1000............. Loss: 0.0467\n",
      "Epoch: 278/1000............. Loss: 0.0466\n",
      "Epoch: 279/1000............. Loss: 0.0465\n",
      "Epoch: 280/1000............. Loss: 0.0464\n",
      "Epoch: 281/1000............. Loss: 0.0463\n",
      "Epoch: 282/1000............. Loss: 0.0463\n",
      "Epoch: 283/1000............. Loss: 0.0462\n",
      "Epoch: 284/1000............. Loss: 0.0461\n",
      "Epoch: 285/1000............. Loss: 0.0460\n",
      "Epoch: 286/1000............. Loss: 0.0459\n",
      "Epoch: 287/1000............. Loss: 0.0459\n",
      "Epoch: 288/1000............. Loss: 0.0458\n",
      "Epoch: 289/1000............. Loss: 0.0457\n",
      "Epoch: 290/1000............. Loss: 0.0456\n",
      "Epoch: 291/1000............. Loss: 0.0456\n",
      "Epoch: 292/1000............. Loss: 0.0455\n",
      "Epoch: 293/1000............. Loss: 0.0454\n",
      "Epoch: 294/1000............. Loss: 0.0453\n",
      "Epoch: 295/1000............. Loss: 0.0453\n",
      "Epoch: 296/1000............. Loss: 0.0452\n",
      "Epoch: 297/1000............. Loss: 0.0451\n",
      "Epoch: 298/1000............. Loss: 0.0450\n",
      "Epoch: 299/1000............. Loss: 0.0450\n",
      "Epoch: 300/1000............. Loss: 0.0449\n",
      "Epoch: 301/1000............. Loss: 0.0448\n",
      "Epoch: 302/1000............. Loss: 0.0448\n",
      "Epoch: 303/1000............. Loss: 0.0447\n",
      "Epoch: 304/1000............. Loss: 0.0446\n",
      "Epoch: 305/1000............. Loss: 0.0446\n",
      "Epoch: 306/1000............. Loss: 0.0445\n",
      "Epoch: 307/1000............. Loss: 0.0444\n",
      "Epoch: 308/1000............. Loss: 0.0444\n",
      "Epoch: 309/1000............. Loss: 0.0443\n",
      "Epoch: 310/1000............. Loss: 0.0442\n",
      "Epoch: 311/1000............. Loss: 0.0442\n",
      "Epoch: 312/1000............. Loss: 0.0441\n",
      "Epoch: 313/1000............. Loss: 0.0441\n",
      "Epoch: 314/1000............. Loss: 0.0440\n",
      "Epoch: 315/1000............. Loss: 0.0439\n",
      "Epoch: 316/1000............. Loss: 0.0439\n",
      "Epoch: 317/1000............. Loss: 0.0438\n",
      "Epoch: 318/1000............. Loss: 0.0438\n",
      "Epoch: 319/1000............. Loss: 0.0437\n",
      "Epoch: 320/1000............. Loss: 0.0436\n",
      "Epoch: 321/1000............. Loss: 0.0436\n",
      "Epoch: 322/1000............. Loss: 0.0435\n",
      "Epoch: 323/1000............. Loss: 0.0435\n",
      "Epoch: 324/1000............. Loss: 0.0434\n",
      "Epoch: 325/1000............. Loss: 0.0434\n",
      "Epoch: 326/1000............. Loss: 0.0433\n",
      "Epoch: 327/1000............. Loss: 0.0433\n",
      "Epoch: 328/1000............. Loss: 0.0432\n",
      "Epoch: 329/1000............. Loss: 0.0431\n",
      "Epoch: 330/1000............. Loss: 0.0431\n",
      "Epoch: 331/1000............. Loss: 0.0430\n",
      "Epoch: 332/1000............. Loss: 0.0430\n",
      "Epoch: 333/1000............. Loss: 0.0429\n",
      "Epoch: 334/1000............. Loss: 0.0429\n",
      "Epoch: 335/1000............. Loss: 0.0428\n",
      "Epoch: 336/1000............. Loss: 0.0428\n",
      "Epoch: 337/1000............. Loss: 0.0427\n",
      "Epoch: 338/1000............. Loss: 0.0427\n",
      "Epoch: 339/1000............. Loss: 0.0426\n",
      "Epoch: 340/1000............. Loss: 0.0426\n",
      "Epoch: 341/1000............. Loss: 0.0425\n",
      "Epoch: 342/1000............. Loss: 0.0425\n",
      "Epoch: 343/1000............. Loss: 0.0424\n",
      "Epoch: 344/1000............. Loss: 0.0424\n",
      "Epoch: 345/1000............. Loss: 0.0423\n",
      "Epoch: 346/1000............. Loss: 0.0423\n",
      "Epoch: 347/1000............. Loss: 0.0423\n",
      "Epoch: 348/1000............. Loss: 0.0422\n",
      "Epoch: 349/1000............. Loss: 0.0422\n",
      "Epoch: 350/1000............. Loss: 0.0421\n",
      "Epoch: 351/1000............. Loss: 0.0421\n",
      "Epoch: 352/1000............. Loss: 0.0420\n",
      "Epoch: 353/1000............. Loss: 0.0420\n",
      "Epoch: 354/1000............. Loss: 0.0419\n",
      "Epoch: 355/1000............. Loss: 0.0419\n",
      "Epoch: 356/1000............. Loss: 0.0419\n",
      "Epoch: 357/1000............. Loss: 0.0418\n",
      "Epoch: 358/1000............. Loss: 0.0418\n",
      "Epoch: 359/1000............. Loss: 0.0417\n",
      "Epoch: 360/1000............. Loss: 0.0417\n",
      "Epoch: 361/1000............. Loss: 0.0416\n",
      "Epoch: 362/1000............. Loss: 0.0416\n",
      "Epoch: 363/1000............. Loss: 0.0416\n",
      "Epoch: 364/1000............. Loss: 0.0415\n",
      "Epoch: 365/1000............. Loss: 0.0415\n",
      "Epoch: 366/1000............. Loss: 0.0414\n",
      "Epoch: 367/1000............. Loss: 0.0414\n",
      "Epoch: 368/1000............. Loss: 0.0414\n",
      "Epoch: 369/1000............. Loss: 0.0413\n",
      "Epoch: 370/1000............. Loss: 0.0413\n",
      "Epoch: 371/1000............. Loss: 0.0413\n",
      "Epoch: 372/1000............. Loss: 0.0412\n",
      "Epoch: 373/1000............. Loss: 0.0412\n",
      "Epoch: 374/1000............. Loss: 0.0411\n",
      "Epoch: 375/1000............. Loss: 0.0411\n",
      "Epoch: 376/1000............. Loss: 0.0411\n",
      "Epoch: 377/1000............. Loss: 0.0410\n",
      "Epoch: 378/1000............. Loss: 0.0410\n",
      "Epoch: 379/1000............. Loss: 0.0410\n",
      "Epoch: 380/1000............. Loss: 0.0409\n",
      "Epoch: 381/1000............. Loss: 0.0409\n",
      "Epoch: 382/1000............. Loss: 0.0409\n",
      "Epoch: 383/1000............. Loss: 0.0408\n",
      "Epoch: 384/1000............. Loss: 0.0408\n",
      "Epoch: 385/1000............. Loss: 0.0407\n",
      "Epoch: 386/1000............. Loss: 0.0407\n",
      "Epoch: 387/1000............. Loss: 0.0407\n",
      "Epoch: 388/1000............. Loss: 0.0406\n",
      "Epoch: 389/1000............. Loss: 0.0406\n",
      "Epoch: 390/1000............. Loss: 0.0406\n",
      "Epoch: 391/1000............. Loss: 0.0405\n",
      "Epoch: 392/1000............. Loss: 0.0405\n",
      "Epoch: 393/1000............. Loss: 0.0405\n",
      "Epoch: 394/1000............. Loss: 0.0405\n",
      "Epoch: 395/1000............. Loss: 0.0404\n",
      "Epoch: 396/1000............. Loss: 0.0404\n",
      "Epoch: 397/1000............. Loss: 0.0404\n",
      "Epoch: 398/1000............. Loss: 0.0403\n",
      "Epoch: 399/1000............. Loss: 0.0403\n",
      "Epoch: 400/1000............. Loss: 0.0403\n",
      "Epoch: 401/1000............. Loss: 0.0402\n",
      "Epoch: 402/1000............. Loss: 0.0402\n",
      "Epoch: 403/1000............. Loss: 0.0402\n",
      "Epoch: 404/1000............. Loss: 0.0401\n",
      "Epoch: 405/1000............. Loss: 0.0401\n",
      "Epoch: 406/1000............. Loss: 0.0401\n",
      "Epoch: 407/1000............. Loss: 0.0400\n",
      "Epoch: 408/1000............. Loss: 0.0400\n",
      "Epoch: 409/1000............. Loss: 0.0400\n",
      "Epoch: 410/1000............. Loss: 0.0400\n",
      "Epoch: 411/1000............. Loss: 0.0399\n",
      "Epoch: 412/1000............. Loss: 0.0399\n",
      "Epoch: 413/1000............. Loss: 0.0399\n",
      "Epoch: 414/1000............. Loss: 0.0398\n",
      "Epoch: 415/1000............. Loss: 0.0398\n",
      "Epoch: 416/1000............. Loss: 0.0398\n",
      "Epoch: 417/1000............. Loss: 0.0398\n",
      "Epoch: 418/1000............. Loss: 0.0397\n",
      "Epoch: 419/1000............. Loss: 0.0397\n",
      "Epoch: 420/1000............. Loss: 0.0397\n",
      "Epoch: 421/1000............. Loss: 0.0397\n",
      "Epoch: 422/1000............. Loss: 0.0396\n",
      "Epoch: 423/1000............. Loss: 0.0396\n",
      "Epoch: 424/1000............. Loss: 0.0396\n",
      "Epoch: 425/1000............. Loss: 0.0395\n",
      "Epoch: 426/1000............. Loss: 0.0395\n",
      "Epoch: 427/1000............. Loss: 0.0395\n",
      "Epoch: 428/1000............. Loss: 0.0395\n",
      "Epoch: 429/1000............. Loss: 0.0394\n",
      "Epoch: 430/1000............. Loss: 0.0394\n",
      "Epoch: 431/1000............. Loss: 0.0394\n",
      "Epoch: 432/1000............. Loss: 0.0394\n",
      "Epoch: 433/1000............. Loss: 0.0393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 434/1000............. Loss: 0.0393\n",
      "Epoch: 435/1000............. Loss: 0.0393\n",
      "Epoch: 436/1000............. Loss: 0.0393\n",
      "Epoch: 437/1000............. Loss: 0.0392\n",
      "Epoch: 438/1000............. Loss: 0.0392\n",
      "Epoch: 439/1000............. Loss: 0.0392\n",
      "Epoch: 440/1000............. Loss: 0.0392\n",
      "Epoch: 441/1000............. Loss: 0.0392\n",
      "Epoch: 442/1000............. Loss: 0.0391\n",
      "Epoch: 443/1000............. Loss: 0.0391\n",
      "Epoch: 444/1000............. Loss: 0.0391\n",
      "Epoch: 445/1000............. Loss: 0.0391\n",
      "Epoch: 446/1000............. Loss: 0.0390\n",
      "Epoch: 447/1000............. Loss: 0.0390\n",
      "Epoch: 448/1000............. Loss: 0.0390\n",
      "Epoch: 449/1000............. Loss: 0.0390\n",
      "Epoch: 450/1000............. Loss: 0.0389\n",
      "Epoch: 451/1000............. Loss: 0.0389\n",
      "Epoch: 452/1000............. Loss: 0.0389\n",
      "Epoch: 453/1000............. Loss: 0.0389\n",
      "Epoch: 454/1000............. Loss: 0.0389\n",
      "Epoch: 455/1000............. Loss: 0.0388\n",
      "Epoch: 456/1000............. Loss: 0.0388\n",
      "Epoch: 457/1000............. Loss: 0.0388\n",
      "Epoch: 458/1000............. Loss: 0.0388\n",
      "Epoch: 459/1000............. Loss: 0.0387\n",
      "Epoch: 460/1000............. Loss: 0.0387\n",
      "Epoch: 461/1000............. Loss: 0.0387\n",
      "Epoch: 462/1000............. Loss: 0.0387\n",
      "Epoch: 463/1000............. Loss: 0.0387\n",
      "Epoch: 464/1000............. Loss: 0.0386\n",
      "Epoch: 465/1000............. Loss: 0.0386\n",
      "Epoch: 466/1000............. Loss: 0.0386\n",
      "Epoch: 467/1000............. Loss: 0.0386\n",
      "Epoch: 468/1000............. Loss: 0.0386\n",
      "Epoch: 469/1000............. Loss: 0.0385\n",
      "Epoch: 470/1000............. Loss: 0.0385\n",
      "Epoch: 471/1000............. Loss: 0.0385\n",
      "Epoch: 472/1000............. Loss: 0.0385\n",
      "Epoch: 473/1000............. Loss: 0.0385\n",
      "Epoch: 474/1000............. Loss: 0.0384\n",
      "Epoch: 475/1000............. Loss: 0.0384\n",
      "Epoch: 476/1000............. Loss: 0.0384\n",
      "Epoch: 477/1000............. Loss: 0.0384\n",
      "Epoch: 478/1000............. Loss: 0.0384\n",
      "Epoch: 479/1000............. Loss: 0.0383\n",
      "Epoch: 480/1000............. Loss: 0.0383\n",
      "Epoch: 481/1000............. Loss: 0.0383\n",
      "Epoch: 482/1000............. Loss: 0.0383\n",
      "Epoch: 483/1000............. Loss: 0.0383\n",
      "Epoch: 484/1000............. Loss: 0.0383\n",
      "Epoch: 485/1000............. Loss: 0.0382\n",
      "Epoch: 486/1000............. Loss: 0.0382\n",
      "Epoch: 487/1000............. Loss: 0.0382\n",
      "Epoch: 488/1000............. Loss: 0.0382\n",
      "Epoch: 489/1000............. Loss: 0.0382\n",
      "Epoch: 490/1000............. Loss: 0.0381\n",
      "Epoch: 491/1000............. Loss: 0.0381\n",
      "Epoch: 492/1000............. Loss: 0.0381\n",
      "Epoch: 493/1000............. Loss: 0.0381\n",
      "Epoch: 494/1000............. Loss: 0.0381\n",
      "Epoch: 495/1000............. Loss: 0.0381\n",
      "Epoch: 496/1000............. Loss: 0.0380\n",
      "Epoch: 497/1000............. Loss: 0.0380\n",
      "Epoch: 498/1000............. Loss: 0.0380\n",
      "Epoch: 499/1000............. Loss: 0.0380\n",
      "Epoch: 500/1000............. Loss: 0.0380\n",
      "Epoch: 501/1000............. Loss: 0.0380\n",
      "Epoch: 502/1000............. Loss: 0.0379\n",
      "Epoch: 503/1000............. Loss: 0.0379\n",
      "Epoch: 504/1000............. Loss: 0.0379\n",
      "Epoch: 505/1000............. Loss: 0.0379\n",
      "Epoch: 506/1000............. Loss: 0.0379\n",
      "Epoch: 507/1000............. Loss: 0.0379\n",
      "Epoch: 508/1000............. Loss: 0.0378\n",
      "Epoch: 509/1000............. Loss: 0.0378\n",
      "Epoch: 510/1000............. Loss: 0.0378\n",
      "Epoch: 511/1000............. Loss: 0.0378\n",
      "Epoch: 512/1000............. Loss: 0.0378\n",
      "Epoch: 513/1000............. Loss: 0.0378\n",
      "Epoch: 514/1000............. Loss: 0.0377\n",
      "Epoch: 515/1000............. Loss: 0.0377\n",
      "Epoch: 516/1000............. Loss: 0.0377\n",
      "Epoch: 517/1000............. Loss: 0.0377\n",
      "Epoch: 518/1000............. Loss: 0.0377\n",
      "Epoch: 519/1000............. Loss: 0.0377\n",
      "Epoch: 520/1000............. Loss: 0.0377\n",
      "Epoch: 521/1000............. Loss: 0.0376\n",
      "Epoch: 522/1000............. Loss: 0.0376\n",
      "Epoch: 523/1000............. Loss: 0.0376\n",
      "Epoch: 524/1000............. Loss: 0.0376\n",
      "Epoch: 525/1000............. Loss: 0.0376\n",
      "Epoch: 526/1000............. Loss: 0.0376\n",
      "Epoch: 527/1000............. Loss: 0.0375\n",
      "Epoch: 528/1000............. Loss: 0.0375\n",
      "Epoch: 529/1000............. Loss: 0.0375\n",
      "Epoch: 530/1000............. Loss: 0.0375\n",
      "Epoch: 531/1000............. Loss: 0.0375\n",
      "Epoch: 532/1000............. Loss: 0.0375\n",
      "Epoch: 533/1000............. Loss: 0.0375\n",
      "Epoch: 534/1000............. Loss: 0.0374\n",
      "Epoch: 535/1000............. Loss: 0.0374\n",
      "Epoch: 536/1000............. Loss: 0.0374\n",
      "Epoch: 537/1000............. Loss: 0.0374\n",
      "Epoch: 538/1000............. Loss: 0.0374\n",
      "Epoch: 539/1000............. Loss: 0.0374\n",
      "Epoch: 540/1000............. Loss: 0.0374\n",
      "Epoch: 541/1000............. Loss: 0.0374\n",
      "Epoch: 542/1000............. Loss: 0.0373\n",
      "Epoch: 543/1000............. Loss: 0.0373\n",
      "Epoch: 544/1000............. Loss: 0.0373\n",
      "Epoch: 545/1000............. Loss: 0.0373\n",
      "Epoch: 546/1000............. Loss: 0.0373\n",
      "Epoch: 547/1000............. Loss: 0.0373\n",
      "Epoch: 548/1000............. Loss: 0.0373\n",
      "Epoch: 549/1000............. Loss: 0.0372\n",
      "Epoch: 550/1000............. Loss: 0.0372\n",
      "Epoch: 551/1000............. Loss: 0.0372\n",
      "Epoch: 552/1000............. Loss: 0.0372\n",
      "Epoch: 553/1000............. Loss: 0.0372\n",
      "Epoch: 554/1000............. Loss: 0.0372\n",
      "Epoch: 555/1000............. Loss: 0.0372\n",
      "Epoch: 556/1000............. Loss: 0.0372\n",
      "Epoch: 557/1000............. Loss: 0.0371\n",
      "Epoch: 558/1000............. Loss: 0.0371\n",
      "Epoch: 559/1000............. Loss: 0.0371\n",
      "Epoch: 560/1000............. Loss: 0.0371\n",
      "Epoch: 561/1000............. Loss: 0.0371\n",
      "Epoch: 562/1000............. Loss: 0.0371\n",
      "Epoch: 563/1000............. Loss: 0.0371\n",
      "Epoch: 564/1000............. Loss: 0.0371\n",
      "Epoch: 565/1000............. Loss: 0.0370\n",
      "Epoch: 566/1000............. Loss: 0.0370\n",
      "Epoch: 567/1000............. Loss: 0.0370\n",
      "Epoch: 568/1000............. Loss: 0.0370\n",
      "Epoch: 569/1000............. Loss: 0.0370\n",
      "Epoch: 570/1000............. Loss: 0.0370\n",
      "Epoch: 571/1000............. Loss: 0.0370\n",
      "Epoch: 572/1000............. Loss: 0.0370\n",
      "Epoch: 573/1000............. Loss: 0.0369\n",
      "Epoch: 574/1000............. Loss: 0.0369\n",
      "Epoch: 575/1000............. Loss: 0.0369\n",
      "Epoch: 576/1000............. Loss: 0.0369\n",
      "Epoch: 577/1000............. Loss: 0.0369\n",
      "Epoch: 578/1000............. Loss: 0.0369\n",
      "Epoch: 579/1000............. Loss: 0.0369\n",
      "Epoch: 580/1000............. Loss: 0.0369\n",
      "Epoch: 581/1000............. Loss: 0.0369\n",
      "Epoch: 582/1000............. Loss: 0.0368\n",
      "Epoch: 583/1000............. Loss: 0.0368\n",
      "Epoch: 584/1000............. Loss: 0.0368\n",
      "Epoch: 585/1000............. Loss: 0.0368\n",
      "Epoch: 586/1000............. Loss: 0.0368\n",
      "Epoch: 587/1000............. Loss: 0.0368\n",
      "Epoch: 588/1000............. Loss: 0.0368\n",
      "Epoch: 589/1000............. Loss: 0.0368\n",
      "Epoch: 590/1000............. Loss: 0.0368\n",
      "Epoch: 591/1000............. Loss: 0.0367\n",
      "Epoch: 592/1000............. Loss: 0.0367\n",
      "Epoch: 593/1000............. Loss: 0.0367\n",
      "Epoch: 594/1000............. Loss: 0.0367\n",
      "Epoch: 595/1000............. Loss: 0.0367\n",
      "Epoch: 596/1000............. Loss: 0.0367\n",
      "Epoch: 597/1000............. Loss: 0.0367\n",
      "Epoch: 598/1000............. Loss: 0.0367\n",
      "Epoch: 599/1000............. Loss: 0.0367\n",
      "Epoch: 600/1000............. Loss: 0.0367\n",
      "Epoch: 601/1000............. Loss: 0.0366\n",
      "Epoch: 602/1000............. Loss: 0.0366\n",
      "Epoch: 603/1000............. Loss: 0.0366\n",
      "Epoch: 604/1000............. Loss: 0.0366\n",
      "Epoch: 605/1000............. Loss: 0.0366\n",
      "Epoch: 606/1000............. Loss: 0.0366\n",
      "Epoch: 607/1000............. Loss: 0.0366\n",
      "Epoch: 608/1000............. Loss: 0.0366\n",
      "Epoch: 609/1000............. Loss: 0.0366\n",
      "Epoch: 610/1000............. Loss: 0.0366\n",
      "Epoch: 611/1000............. Loss: 0.0365\n",
      "Epoch: 612/1000............. Loss: 0.0365\n",
      "Epoch: 613/1000............. Loss: 0.0365\n",
      "Epoch: 614/1000............. Loss: 0.0365\n",
      "Epoch: 615/1000............. Loss: 0.0365\n",
      "Epoch: 616/1000............. Loss: 0.0365\n",
      "Epoch: 617/1000............. Loss: 0.0365\n",
      "Epoch: 618/1000............. Loss: 0.0365\n",
      "Epoch: 619/1000............. Loss: 0.0365\n",
      "Epoch: 620/1000............. Loss: 0.0365\n",
      "Epoch: 621/1000............. Loss: 0.0364\n",
      "Epoch: 622/1000............. Loss: 0.0364\n",
      "Epoch: 623/1000............. Loss: 0.0364\n",
      "Epoch: 624/1000............. Loss: 0.0364\n",
      "Epoch: 625/1000............. Loss: 0.0364\n",
      "Epoch: 626/1000............. Loss: 0.0364\n",
      "Epoch: 627/1000............. Loss: 0.0364\n",
      "Epoch: 628/1000............. Loss: 0.0364\n",
      "Epoch: 629/1000............. Loss: 0.0364\n",
      "Epoch: 630/1000............. Loss: 0.0364\n",
      "Epoch: 631/1000............. Loss: 0.0364\n",
      "Epoch: 632/1000............. Loss: 0.0363\n",
      "Epoch: 633/1000............. Loss: 0.0363\n",
      "Epoch: 634/1000............. Loss: 0.0363\n",
      "Epoch: 635/1000............. Loss: 0.0363\n",
      "Epoch: 636/1000............. Loss: 0.0363\n",
      "Epoch: 637/1000............. Loss: 0.0363\n",
      "Epoch: 638/1000............. Loss: 0.0363\n",
      "Epoch: 639/1000............. Loss: 0.0363\n",
      "Epoch: 640/1000............. Loss: 0.0363\n",
      "Epoch: 641/1000............. Loss: 0.0363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 642/1000............. Loss: 0.0363\n",
      "Epoch: 643/1000............. Loss: 0.0362\n",
      "Epoch: 644/1000............. Loss: 0.0362\n",
      "Epoch: 645/1000............. Loss: 0.0362\n",
      "Epoch: 646/1000............. Loss: 0.0362\n",
      "Epoch: 647/1000............. Loss: 0.0362\n",
      "Epoch: 648/1000............. Loss: 0.0362\n",
      "Epoch: 649/1000............. Loss: 0.0362\n",
      "Epoch: 650/1000............. Loss: 0.0362\n",
      "Epoch: 651/1000............. Loss: 0.0362\n",
      "Epoch: 652/1000............. Loss: 0.0362\n",
      "Epoch: 653/1000............. Loss: 0.0362\n",
      "Epoch: 654/1000............. Loss: 0.0362\n",
      "Epoch: 655/1000............. Loss: 0.0361\n",
      "Epoch: 656/1000............. Loss: 0.0361\n",
      "Epoch: 657/1000............. Loss: 0.0361\n",
      "Epoch: 658/1000............. Loss: 0.0361\n",
      "Epoch: 659/1000............. Loss: 0.0361\n",
      "Epoch: 660/1000............. Loss: 0.0361\n",
      "Epoch: 661/1000............. Loss: 0.0361\n",
      "Epoch: 662/1000............. Loss: 0.0361\n",
      "Epoch: 663/1000............. Loss: 0.0361\n",
      "Epoch: 664/1000............. Loss: 0.0361\n",
      "Epoch: 665/1000............. Loss: 0.0361\n",
      "Epoch: 666/1000............. Loss: 0.0361\n",
      "Epoch: 667/1000............. Loss: 0.0361\n",
      "Epoch: 668/1000............. Loss: 0.0360\n",
      "Epoch: 669/1000............. Loss: 0.0360\n",
      "Epoch: 670/1000............. Loss: 0.0360\n",
      "Epoch: 671/1000............. Loss: 0.0360\n",
      "Epoch: 672/1000............. Loss: 0.0360\n",
      "Epoch: 673/1000............. Loss: 0.0360\n",
      "Epoch: 674/1000............. Loss: 0.0360\n",
      "Epoch: 675/1000............. Loss: 0.0360\n",
      "Epoch: 676/1000............. Loss: 0.0360\n",
      "Epoch: 677/1000............. Loss: 0.0360\n",
      "Epoch: 678/1000............. Loss: 0.0360\n",
      "Epoch: 679/1000............. Loss: 0.0360\n",
      "Epoch: 680/1000............. Loss: 0.0360\n",
      "Epoch: 681/1000............. Loss: 0.0359\n",
      "Epoch: 682/1000............. Loss: 0.0359\n",
      "Epoch: 683/1000............. Loss: 0.0359\n",
      "Epoch: 684/1000............. Loss: 0.0359\n",
      "Epoch: 685/1000............. Loss: 0.0359\n",
      "Epoch: 686/1000............. Loss: 0.0359\n",
      "Epoch: 687/1000............. Loss: 0.0359\n",
      "Epoch: 688/1000............. Loss: 0.0359\n",
      "Epoch: 689/1000............. Loss: 0.0359\n",
      "Epoch: 690/1000............. Loss: 0.0359\n",
      "Epoch: 691/1000............. Loss: 0.0359\n",
      "Epoch: 692/1000............. Loss: 0.0359\n",
      "Epoch: 693/1000............. Loss: 0.0359\n",
      "Epoch: 694/1000............. Loss: 0.0359\n",
      "Epoch: 695/1000............. Loss: 0.0358\n",
      "Epoch: 696/1000............. Loss: 0.0358\n",
      "Epoch: 697/1000............. Loss: 0.0358\n",
      "Epoch: 698/1000............. Loss: 0.0358\n",
      "Epoch: 699/1000............. Loss: 0.0358\n",
      "Epoch: 700/1000............. Loss: 0.0358\n",
      "Epoch: 701/1000............. Loss: 0.0358\n",
      "Epoch: 702/1000............. Loss: 0.0358\n",
      "Epoch: 703/1000............. Loss: 0.0358\n",
      "Epoch: 704/1000............. Loss: 0.0358\n",
      "Epoch: 705/1000............. Loss: 0.0358\n",
      "Epoch: 706/1000............. Loss: 0.0358\n",
      "Epoch: 707/1000............. Loss: 0.0358\n",
      "Epoch: 708/1000............. Loss: 0.0358\n",
      "Epoch: 709/1000............. Loss: 0.0358\n",
      "Epoch: 710/1000............. Loss: 0.0357\n",
      "Epoch: 711/1000............. Loss: 0.0357\n",
      "Epoch: 712/1000............. Loss: 0.0357\n",
      "Epoch: 713/1000............. Loss: 0.0357\n",
      "Epoch: 714/1000............. Loss: 0.0357\n",
      "Epoch: 715/1000............. Loss: 0.0357\n",
      "Epoch: 716/1000............. Loss: 0.0357\n",
      "Epoch: 717/1000............. Loss: 0.0357\n",
      "Epoch: 718/1000............. Loss: 0.0357\n",
      "Epoch: 719/1000............. Loss: 0.0357\n",
      "Epoch: 720/1000............. Loss: 0.0357\n",
      "Epoch: 721/1000............. Loss: 0.0357\n",
      "Epoch: 722/1000............. Loss: 0.0357\n",
      "Epoch: 723/1000............. Loss: 0.0357\n",
      "Epoch: 724/1000............. Loss: 0.0357\n",
      "Epoch: 725/1000............. Loss: 0.0356\n",
      "Epoch: 726/1000............. Loss: 0.0356\n",
      "Epoch: 727/1000............. Loss: 0.0356\n",
      "Epoch: 728/1000............. Loss: 0.0356\n",
      "Epoch: 729/1000............. Loss: 0.0356\n",
      "Epoch: 730/1000............. Loss: 0.0356\n",
      "Epoch: 731/1000............. Loss: 0.0356\n",
      "Epoch: 732/1000............. Loss: 0.0356\n",
      "Epoch: 733/1000............. Loss: 0.0356\n",
      "Epoch: 734/1000............. Loss: 0.0356\n",
      "Epoch: 735/1000............. Loss: 0.0356\n",
      "Epoch: 736/1000............. Loss: 0.0356\n",
      "Epoch: 737/1000............. Loss: 0.0356\n",
      "Epoch: 738/1000............. Loss: 0.0356\n",
      "Epoch: 739/1000............. Loss: 0.0356\n",
      "Epoch: 740/1000............. Loss: 0.0356\n",
      "Epoch: 741/1000............. Loss: 0.0356\n",
      "Epoch: 742/1000............. Loss: 0.0355\n",
      "Epoch: 743/1000............. Loss: 0.0355\n",
      "Epoch: 744/1000............. Loss: 0.0355\n",
      "Epoch: 745/1000............. Loss: 0.0355\n",
      "Epoch: 746/1000............. Loss: 0.0355\n",
      "Epoch: 747/1000............. Loss: 0.0355\n",
      "Epoch: 748/1000............. Loss: 0.0355\n",
      "Epoch: 749/1000............. Loss: 0.0355\n",
      "Epoch: 750/1000............. Loss: 0.0355\n",
      "Epoch: 751/1000............. Loss: 0.0355\n",
      "Epoch: 752/1000............. Loss: 0.0355\n",
      "Epoch: 753/1000............. Loss: 0.0355\n",
      "Epoch: 754/1000............. Loss: 0.0355\n",
      "Epoch: 755/1000............. Loss: 0.0355\n",
      "Epoch: 756/1000............. Loss: 0.0355\n",
      "Epoch: 757/1000............. Loss: 0.0355\n",
      "Epoch: 758/1000............. Loss: 0.0355\n",
      "Epoch: 759/1000............. Loss: 0.0354\n",
      "Epoch: 760/1000............. Loss: 0.0354\n",
      "Epoch: 761/1000............. Loss: 0.0354\n",
      "Epoch: 762/1000............. Loss: 0.0354\n",
      "Epoch: 763/1000............. Loss: 0.0354\n",
      "Epoch: 764/1000............. Loss: 0.0354\n",
      "Epoch: 765/1000............. Loss: 0.0354\n",
      "Epoch: 766/1000............. Loss: 0.0354\n",
      "Epoch: 767/1000............. Loss: 0.0354\n",
      "Epoch: 768/1000............. Loss: 0.0354\n",
      "Epoch: 769/1000............. Loss: 0.0354\n",
      "Epoch: 770/1000............. Loss: 0.0354\n",
      "Epoch: 771/1000............. Loss: 0.0354\n",
      "Epoch: 772/1000............. Loss: 0.0354\n",
      "Epoch: 773/1000............. Loss: 0.0354\n",
      "Epoch: 774/1000............. Loss: 0.0354\n",
      "Epoch: 775/1000............. Loss: 0.0354\n",
      "Epoch: 776/1000............. Loss: 0.0354\n",
      "Epoch: 777/1000............. Loss: 0.0354\n",
      "Epoch: 778/1000............. Loss: 0.0353\n",
      "Epoch: 779/1000............. Loss: 0.0353\n",
      "Epoch: 780/1000............. Loss: 0.0353\n",
      "Epoch: 781/1000............. Loss: 0.0353\n",
      "Epoch: 782/1000............. Loss: 0.0353\n",
      "Epoch: 783/1000............. Loss: 0.0353\n",
      "Epoch: 784/1000............. Loss: 0.0353\n",
      "Epoch: 785/1000............. Loss: 0.0353\n",
      "Epoch: 786/1000............. Loss: 0.0353\n",
      "Epoch: 787/1000............. Loss: 0.0353\n",
      "Epoch: 788/1000............. Loss: 0.0353\n",
      "Epoch: 789/1000............. Loss: 0.0353\n",
      "Epoch: 790/1000............. Loss: 0.0353\n",
      "Epoch: 791/1000............. Loss: 0.0353\n",
      "Epoch: 792/1000............. Loss: 0.0353\n",
      "Epoch: 793/1000............. Loss: 0.0353\n",
      "Epoch: 794/1000............. Loss: 0.0353\n",
      "Epoch: 795/1000............. Loss: 0.0353\n",
      "Epoch: 796/1000............. Loss: 0.0353\n",
      "Epoch: 797/1000............. Loss: 0.0353\n",
      "Epoch: 798/1000............. Loss: 0.0352\n",
      "Epoch: 799/1000............. Loss: 0.0352\n",
      "Epoch: 800/1000............. Loss: 0.0352\n",
      "Epoch: 801/1000............. Loss: 0.0352\n",
      "Epoch: 802/1000............. Loss: 0.0352\n",
      "Epoch: 803/1000............. Loss: 0.0352\n",
      "Epoch: 804/1000............. Loss: 0.0352\n",
      "Epoch: 805/1000............. Loss: 0.0352\n",
      "Epoch: 806/1000............. Loss: 0.0352\n",
      "Epoch: 807/1000............. Loss: 0.0352\n",
      "Epoch: 808/1000............. Loss: 0.0352\n",
      "Epoch: 809/1000............. Loss: 0.0352\n",
      "Epoch: 810/1000............. Loss: 0.0352\n",
      "Epoch: 811/1000............. Loss: 0.0352\n",
      "Epoch: 812/1000............. Loss: 0.0352\n",
      "Epoch: 813/1000............. Loss: 0.0352\n",
      "Epoch: 814/1000............. Loss: 0.0352\n",
      "Epoch: 815/1000............. Loss: 0.0352\n",
      "Epoch: 816/1000............. Loss: 0.0352\n",
      "Epoch: 817/1000............. Loss: 0.0352\n",
      "Epoch: 818/1000............. Loss: 0.0352\n",
      "Epoch: 819/1000............. Loss: 0.0352\n",
      "Epoch: 820/1000............. Loss: 0.0351\n",
      "Epoch: 821/1000............. Loss: 0.0351\n",
      "Epoch: 822/1000............. Loss: 0.0351\n",
      "Epoch: 823/1000............. Loss: 0.0351\n",
      "Epoch: 824/1000............. Loss: 0.0351\n",
      "Epoch: 825/1000............. Loss: 0.0351\n",
      "Epoch: 826/1000............. Loss: 0.0351\n",
      "Epoch: 827/1000............. Loss: 0.0351\n",
      "Epoch: 828/1000............. Loss: 0.0351\n",
      "Epoch: 829/1000............. Loss: 0.0351\n",
      "Epoch: 830/1000............. Loss: 0.0351\n",
      "Epoch: 831/1000............. Loss: 0.0351\n",
      "Epoch: 832/1000............. Loss: 0.0351\n",
      "Epoch: 833/1000............. Loss: 0.0351\n",
      "Epoch: 834/1000............. Loss: 0.0351\n",
      "Epoch: 835/1000............. Loss: 0.0351\n",
      "Epoch: 836/1000............. Loss: 0.0351\n",
      "Epoch: 837/1000............. Loss: 0.0351\n",
      "Epoch: 838/1000............. Loss: 0.0351\n",
      "Epoch: 839/1000............. Loss: 0.0351\n",
      "Epoch: 840/1000............. Loss: 0.0351\n",
      "Epoch: 841/1000............. Loss: 0.0351\n",
      "Epoch: 842/1000............. Loss: 0.0351\n",
      "Epoch: 843/1000............. Loss: 0.0350\n",
      "Epoch: 844/1000............. Loss: 0.0350\n",
      "Epoch: 845/1000............. Loss: 0.0350\n",
      "Epoch: 846/1000............. Loss: 0.0350\n",
      "Epoch: 847/1000............. Loss: 0.0350\n",
      "Epoch: 848/1000............. Loss: 0.0350\n",
      "Epoch: 849/1000............. Loss: 0.0350\n",
      "Epoch: 850/1000............. Loss: 0.0350\n",
      "Epoch: 851/1000............. Loss: 0.0350\n",
      "Epoch: 852/1000............. Loss: 0.0350\n",
      "Epoch: 853/1000............. Loss: 0.0350\n",
      "Epoch: 854/1000............. Loss: 0.0350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 855/1000............. Loss: 0.0350\n",
      "Epoch: 856/1000............. Loss: 0.0350\n",
      "Epoch: 857/1000............. Loss: 0.0350\n",
      "Epoch: 858/1000............. Loss: 0.0350\n",
      "Epoch: 859/1000............. Loss: 0.0350\n",
      "Epoch: 860/1000............. Loss: 0.0350\n",
      "Epoch: 861/1000............. Loss: 0.0350\n",
      "Epoch: 862/1000............. Loss: 0.0350\n",
      "Epoch: 863/1000............. Loss: 0.0350\n",
      "Epoch: 864/1000............. Loss: 0.0350\n",
      "Epoch: 865/1000............. Loss: 0.0350\n",
      "Epoch: 866/1000............. Loss: 0.0350\n",
      "Epoch: 867/1000............. Loss: 0.0349\n",
      "Epoch: 868/1000............. Loss: 0.0349\n",
      "Epoch: 869/1000............. Loss: 0.0349\n",
      "Epoch: 870/1000............. Loss: 0.0349\n",
      "Epoch: 871/1000............. Loss: 0.0349\n",
      "Epoch: 872/1000............. Loss: 0.0349\n",
      "Epoch: 873/1000............. Loss: 0.0349\n",
      "Epoch: 874/1000............. Loss: 0.0349\n",
      "Epoch: 875/1000............. Loss: 0.0349\n",
      "Epoch: 876/1000............. Loss: 0.0349\n",
      "Epoch: 877/1000............. Loss: 0.0349\n",
      "Epoch: 878/1000............. Loss: 0.0349\n",
      "Epoch: 879/1000............. Loss: 0.0349\n",
      "Epoch: 880/1000............. Loss: 0.0349\n",
      "Epoch: 881/1000............. Loss: 0.0349\n",
      "Epoch: 882/1000............. Loss: 0.0349\n",
      "Epoch: 883/1000............. Loss: 0.0349\n",
      "Epoch: 884/1000............. Loss: 0.0349\n",
      "Epoch: 885/1000............. Loss: 0.0349\n",
      "Epoch: 886/1000............. Loss: 0.0349\n",
      "Epoch: 887/1000............. Loss: 0.0349\n",
      "Epoch: 888/1000............. Loss: 0.0349\n",
      "Epoch: 889/1000............. Loss: 0.0349\n",
      "Epoch: 890/1000............. Loss: 0.0349\n",
      "Epoch: 891/1000............. Loss: 0.0349\n",
      "Epoch: 892/1000............. Loss: 0.0349\n",
      "Epoch: 893/1000............. Loss: 0.0349\n",
      "Epoch: 894/1000............. Loss: 0.0348\n",
      "Epoch: 895/1000............. Loss: 0.0348\n",
      "Epoch: 896/1000............. Loss: 0.0348\n",
      "Epoch: 897/1000............. Loss: 0.0348\n",
      "Epoch: 898/1000............. Loss: 0.0348\n",
      "Epoch: 899/1000............. Loss: 0.0348\n",
      "Epoch: 900/1000............. Loss: 0.0348\n",
      "Epoch: 901/1000............. Loss: 0.0348\n",
      "Epoch: 902/1000............. Loss: 0.0348\n",
      "Epoch: 903/1000............. Loss: 0.0348\n",
      "Epoch: 904/1000............. Loss: 0.0348\n",
      "Epoch: 905/1000............. Loss: 0.0348\n",
      "Epoch: 906/1000............. Loss: 0.0348\n",
      "Epoch: 907/1000............. Loss: 0.0348\n",
      "Epoch: 908/1000............. Loss: 0.0348\n",
      "Epoch: 909/1000............. Loss: 0.0348\n",
      "Epoch: 910/1000............. Loss: 0.0348\n",
      "Epoch: 911/1000............. Loss: 0.0348\n",
      "Epoch: 912/1000............. Loss: 0.0348\n",
      "Epoch: 913/1000............. Loss: 0.0348\n",
      "Epoch: 914/1000............. Loss: 0.0348\n",
      "Epoch: 915/1000............. Loss: 0.0348\n",
      "Epoch: 916/1000............. Loss: 0.0348\n",
      "Epoch: 917/1000............. Loss: 0.0348\n",
      "Epoch: 918/1000............. Loss: 0.0348\n",
      "Epoch: 919/1000............. Loss: 0.0348\n",
      "Epoch: 920/1000............. Loss: 0.0348\n",
      "Epoch: 921/1000............. Loss: 0.0348\n",
      "Epoch: 922/1000............. Loss: 0.0348\n",
      "Epoch: 923/1000............. Loss: 0.0348\n",
      "Epoch: 924/1000............. Loss: 0.0347\n",
      "Epoch: 925/1000............. Loss: 0.0347\n",
      "Epoch: 926/1000............. Loss: 0.0347\n",
      "Epoch: 927/1000............. Loss: 0.0347\n",
      "Epoch: 928/1000............. Loss: 0.0347\n",
      "Epoch: 929/1000............. Loss: 0.0347\n",
      "Epoch: 930/1000............. Loss: 0.0347\n",
      "Epoch: 931/1000............. Loss: 0.0347\n",
      "Epoch: 932/1000............. Loss: 0.0347\n",
      "Epoch: 933/1000............. Loss: 0.0347\n",
      "Epoch: 934/1000............. Loss: 0.0347\n",
      "Epoch: 935/1000............. Loss: 0.0347\n",
      "Epoch: 936/1000............. Loss: 0.0347\n",
      "Epoch: 937/1000............. Loss: 0.0347\n",
      "Epoch: 938/1000............. Loss: 0.0347\n",
      "Epoch: 939/1000............. Loss: 0.0347\n",
      "Epoch: 940/1000............. Loss: 0.0347\n",
      "Epoch: 941/1000............. Loss: 0.0347\n",
      "Epoch: 942/1000............. Loss: 0.0347\n",
      "Epoch: 943/1000............. Loss: 0.0347\n",
      "Epoch: 944/1000............. Loss: 0.0347\n",
      "Epoch: 945/1000............. Loss: 0.0347\n",
      "Epoch: 946/1000............. Loss: 0.0347\n",
      "Epoch: 947/1000............. Loss: 0.0347\n",
      "Epoch: 948/1000............. Loss: 0.0347\n",
      "Epoch: 949/1000............. Loss: 0.0347\n",
      "Epoch: 950/1000............. Loss: 0.0347\n",
      "Epoch: 951/1000............. Loss: 0.0347\n",
      "Epoch: 952/1000............. Loss: 0.0347\n",
      "Epoch: 953/1000............. Loss: 0.0347\n",
      "Epoch: 954/1000............. Loss: 0.0347\n",
      "Epoch: 955/1000............. Loss: 0.0347\n",
      "Epoch: 956/1000............. Loss: 0.0346\n",
      "Epoch: 957/1000............. Loss: 0.0346\n",
      "Epoch: 958/1000............. Loss: 0.0346\n",
      "Epoch: 959/1000............. Loss: 0.0346\n",
      "Epoch: 960/1000............. Loss: 0.0346\n",
      "Epoch: 961/1000............. Loss: 0.0346\n",
      "Epoch: 962/1000............. Loss: 0.0346\n",
      "Epoch: 963/1000............. Loss: 0.0346\n",
      "Epoch: 964/1000............. Loss: 0.0346\n",
      "Epoch: 965/1000............. Loss: 0.0346\n",
      "Epoch: 966/1000............. Loss: 0.0346\n",
      "Epoch: 967/1000............. Loss: 0.0346\n",
      "Epoch: 968/1000............. Loss: 0.0346\n",
      "Epoch: 969/1000............. Loss: 0.0346\n",
      "Epoch: 970/1000............. Loss: 0.0346\n",
      "Epoch: 971/1000............. Loss: 0.0346\n",
      "Epoch: 972/1000............. Loss: 0.0346\n",
      "Epoch: 973/1000............. Loss: 0.0346\n",
      "Epoch: 974/1000............. Loss: 0.0346\n",
      "Epoch: 975/1000............. Loss: 0.0346\n",
      "Epoch: 976/1000............. Loss: 0.0346\n",
      "Epoch: 977/1000............. Loss: 0.0346\n",
      "Epoch: 978/1000............. Loss: 0.0346\n",
      "Epoch: 979/1000............. Loss: 0.0346\n",
      "Epoch: 980/1000............. Loss: 0.0346\n",
      "Epoch: 981/1000............. Loss: 0.0346\n",
      "Epoch: 982/1000............. Loss: 0.0346\n",
      "Epoch: 983/1000............. Loss: 0.0346\n",
      "Epoch: 984/1000............. Loss: 0.0346\n",
      "Epoch: 985/1000............. Loss: 0.0346\n",
      "Epoch: 986/1000............. Loss: 0.0346\n",
      "Epoch: 987/1000............. Loss: 0.0346\n",
      "Epoch: 988/1000............. Loss: 0.0346\n",
      "Epoch: 989/1000............. Loss: 0.0346\n",
      "Epoch: 990/1000............. Loss: 0.0346\n",
      "Epoch: 991/1000............. Loss: 0.0345\n",
      "Epoch: 992/1000............. Loss: 0.0345\n",
      "Epoch: 993/1000............. Loss: 0.0345\n",
      "Epoch: 994/1000............. Loss: 0.0345\n",
      "Epoch: 995/1000............. Loss: 0.0345\n",
      "Epoch: 996/1000............. Loss: 0.0345\n",
      "Epoch: 997/1000............. Loss: 0.0345\n",
      "Epoch: 998/1000............. Loss: 0.0345\n",
      "Epoch: 999/1000............. Loss: 0.0345\n",
      "Epoch: 1000/1000............. Loss: 0.0345\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "protected-makeup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[[1,2,3],[2,3,1]], [[-1,2,0],[2,-3,3]], [[-1,-2,3],[-2,3,2]]])\n",
    "np.mean(np.sum(X, axis=(1,2), keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "liquid-french",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan,  0.],\n",
       "       [nan,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = np.zeros((2,2))\n",
    "dl = np.array([[np.nan, 0],[np.nan, 0]])\n",
    "\n",
    "dd + dl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
