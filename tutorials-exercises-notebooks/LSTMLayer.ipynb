{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "completed-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "spread-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(X_in):\n",
    "        return np.tanh(X_in)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(X_in):\n",
    "        #dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(X_in))**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward_calculated(tanh_x_in):\n",
    "        return 1 - tanh_x_in**2\n",
    "    \n",
    "class Sigmoid(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return 1./(1 + np.exp(-x_in))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        fw = Sigmoid().forward(x_in)\n",
    "        return fw * (1 - fw)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward_calculated(sigmoid_x):\n",
    "        return sigmoid_x * (1 - sigmoid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "aging-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, use_bias=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        # input weights (W_in_hi|W_fgt_hi|W_g_hi|W_out_hi)\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (4, hidden_dim, input_dim))\n",
    "        # hidden weights (W_in_hh|W_fgt_hh|W_g_hh|W_out_hh)\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (4, hidden_dim, hidden_dim))\n",
    "\n",
    "        self.tanh = Tanh\n",
    "        self.sigmoid = Sigmoid\n",
    "\n",
    "        self.gates = None\n",
    "        self.H = None\n",
    "        self.C = None\n",
    "\n",
    "        if self.use_bias:\n",
    "            # bias = (in_bias|fgt_bias|g_bias|out_bias)\n",
    "            self.bias = np.random.uniform(-sq, sq, (4, hidden_dim))\n",
    "        else:\n",
    "            self.bias = np.zeros((4, hidden_dim))\n",
    "\n",
    "    def forward(self, X_in, h_0=None, c_0=None):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        self.H = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if h_0 is not None:\n",
    "            self.H[:, 0, :] = h_0\n",
    "\n",
    "        self.C = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if c_0 is not None:\n",
    "            self.C[:, 0, :] = c_0\n",
    "\n",
    "        self.gates = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # input_gate\n",
    "            self.gates[0, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[0, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[0, :, :].T) + self.bias[0, :])\n",
    "            # forget gate\n",
    "            self.gates[1, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[1, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[1, :, :].T) + self.bias[1, :])\n",
    "            # c~ gate\n",
    "            self.gates[2, :, i, :] = self.tanh.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[2, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[2, :, :].T) + self.bias[2, :])\n",
    "            # output gate\n",
    "            self.gates[3, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[3, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[3, :, :].T) + self.bias[3, :])\n",
    "\n",
    "            self.C[:, i + 1, :] = self.gates[1, :, i, :] * self.C[:, i, :] + self.gates[0, :, i, :] * self.gates[2, :, i, :]\n",
    "            self.H[:, i + 1, :] = self.gates[3, :, i, :] * self.tanh.forward(self.C[:, i + 1, :])\n",
    "\n",
    "        return self.H, self.H[:, seq_len, :], self.C[:, seq_len, :]\n",
    "\n",
    "    def backward(self, X_in, dEdY):\n",
    "\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        C_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        gates_grad = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                H_grad[:, i, :] = np.matmul(gates_grad[:, :, i + 1, :], self.hidden_weights).sum(axis=0) + dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad[:, i, :] * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :]) + C_grad[:, i + 1, :] * self.gates[1, :, i + 1, :]\n",
    "            else:\n",
    "                H_grad[:, i, :] = dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad[:, i, :] * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :])\n",
    "\n",
    "            gates_grad[0, :, i, :] = C_grad[:, i, :] * self.gates[2, :, i, :] * self.sigmoid.backward_calculated(self.gates[0, :, i, :])\n",
    "            gates_grad[1, :, i, :] = C_grad[:, i, :] * self.C[:, i, :] * self.sigmoid.backward_calculated(self.gates[1, :, i, :])\n",
    "            gates_grad[2, :, i, :] = C_grad[:, i, :] * self.gates[0, :, i, :] * self.tanh.backward_calculated(self.gates[2, :, i, :])\n",
    "            gates_grad[3, :, i, :] = H_grad[:, i, :] * self.tanh.forward(self.C[:, i + 1, :]) * self.sigmoid.backward_calculated(self.gates[3, :, i, :])\n",
    "\n",
    "            dEdW_in[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                dEdW_hh[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in[0, :] += np.sum(gates_grad[0, :, i, :], axis=0)\n",
    "                dEdB_in[1, :] += np.sum(gates_grad[1, :, i, :], axis=0)\n",
    "                dEdB_in[2, :] += np.sum(gates_grad[2, :, i, :], axis=0)\n",
    "                dEdB_in[3, :] += np.sum(gates_grad[3, :, i, :], axis=0)\n",
    "        \n",
    "        return dEdW_in, dEdW_hh, dEdB_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "coordinate-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMLayer(2,1)\n",
    "\n",
    "lstm.input_weights = np.array([[[0.95, 0.8]],\n",
    "\n",
    "       [[0.7, 0.45]],\n",
    "\n",
    "       [[0.45, 0.25]],\n",
    "\n",
    "       [[0.6, 0.4]]])\n",
    "\n",
    "lstm.hidden_weights = np.array([[[0.8]],\n",
    "\n",
    "       [[ 0.1]],\n",
    "\n",
    "       [[0.15]],\n",
    "\n",
    "       [[0.25]]])\n",
    "\n",
    "lstm.bias = np.array([[0.65], [0.15],  [0.2],  [0.1]])\n",
    "x = np.array([[[1,2],[0.5,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "productive-scholar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.        ],\n",
       "         [0.5363134 ],\n",
       "         [0.77198111]]]),\n",
       " array([[0.77198111]]),\n",
       " array([[1.5176331]]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "official-romance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2,  3],\n",
       "        [ 2,  4,  6]],\n",
       "\n",
       "       [[ 0, -1, -2],\n",
       "        [ 0,  3,  6]]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([[[1,2,3],[3,4,1]],[[1,2,3],[1,2,1]],[[2,3,1],[2,3,1]],[[2,3,2],[3,2,1]]])\n",
    "x2 = np.array([[[1,5,3],[3,4,1], [2,3,1]],[[1,2,3],[1,2,1],[2,3,1]],[[2,7,1],[2,3,1],[2,3,1]],[[2,9,2],[3,11,1],[2,3,1]]])\n",
    "\n",
    "bi = np.array([[1,2,3],[0,1,2]])\n",
    "bh = np.array([[1,2],[-1,3]])\n",
    "\n",
    "np.einsum('bi,bo->bio',bh,bi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
