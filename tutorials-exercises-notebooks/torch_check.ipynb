{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "treated-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "standard-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        exps = np.exp(x_in-np.max(x_in, axis=-1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "class Sigmoid(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return 1./(1 + np.exp(-x_in))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        fw = Sigmoid().forward(x_in)\n",
    "        return fw * (1 - fw)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward_calculated(sigmoid_x):\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "    \n",
    "class Tanh(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(X_in):\n",
    "        return np.tanh(X_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(X_in):\n",
    "        # dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(X_in)) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward_calculated(tanh_x_in):\n",
    "        return 1 - tanh_x_in ** 2\n",
    "\n",
    "\n",
    "class ReLu(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.maximum(x_in, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        return x_in > 0\n",
    "    \n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, y, o):\n",
    "        self.y_pred = Softmax.forward(o)\n",
    "        return np.sum(-y * np.log(self.y_pred + 1e-15))/(y.shape[0])\n",
    "\n",
    "    def backward(self, y):\n",
    "        return (self.y_pred - y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "engaging-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        # treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "\n",
    "        # u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            input_part = np.einsum('ij,jk->ik', x_in[:, i, :], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', H[:, i, :], self.hidden_weights.T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def book_forward(self, x_in):\n",
    "\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            # ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki', self.input_weights, x_in[:, i, :].T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', self.hidden_weights, H[:, i, :].T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def backward(self, x, h, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "        \n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:,self.seq_len - 1, :]\n",
    "        \n",
    "\n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            \n",
    "            activation_backward = self.activation.backward(h[:, i, :])\n",
    "            back_reshaped = activation_backward.reshape(self.batch_size, self.hidden_dim, 1)\n",
    "            \n",
    "            dEdW_in += np.sum(back_reshaped * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i - 1, :])), axis=0)\n",
    "            dEdW_hh += np.sum(back_reshaped * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i - 1, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(h[:, i, :]) * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * activation_backward + dEdY[:, i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * activation_backward\n",
    "\n",
    "    \n",
    "        return dEdW_in, dEdW_hh, dEdB_in, H_grad\n",
    "\n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:, self.seq_len - 1, :]\n",
    "        \n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "\n",
    "            for k in range(self.batch_size):\n",
    "                act_grad = np.diag(self.activation.backward(H[k, i, :]))\n",
    "                h_grad = H_grad[k, i, :].reshape(self.hidden_dim, 1)\n",
    "\n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k, i - 1, :].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k, i - 1, :].reshape(1, self.hidden_dim)))\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:, i, :]) * H_grad[:, i, :], axis=(0))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.dot(H_grad[:, i, :],self.hidden_weights) * self.activation.backward(H[:, i, :]) + dEdY[:,i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.dot(H_grad[:, i, :],self.hidden_weights) * self.activation.backward(H[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "    \n",
    "    \n",
    "class DenseLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        sq = np.sqrt(1. / input_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = np.random.uniform(-sq, sq, (output_dim, input_dim))\n",
    "        if use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, output_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(output_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        return np.tensordot(x_in, self.weights.T, axes=((-1), 0)) + self.bias\n",
    "\n",
    "    def backward(self, de_dy, x_in):\n",
    "        # de_dw = de_dy * dYdW = de_dy * X\n",
    "        # dEdb = de_dy * dYdb = de_dy\n",
    "        # dEdX = de_dy * dYdX = de_dy * W\n",
    "        axis = tuple(range(len(x_in.shape) - 1))\n",
    "        de_dw = np.tensordot(de_dy, x_in, axes=(axis, axis))\n",
    "        de_db = np.sum(de_dy, axis=axis)\n",
    "        de_dx = np.tensordot(de_dy, self.weights, axes=(-1, 0))\n",
    "\n",
    "        return de_dx, de_dw, de_db\n",
    "\n",
    "    def refresh(self, de_dw, de_db, learning_rate):\n",
    "        self.weights = self.weights - learning_rate * de_dw\n",
    "        if self.use_bias:\n",
    "            self.bias = self.bias - learning_rate * de_db\n",
    "            \n",
    "            \n",
    "class LSTMLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, use_bias=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        # input weights (W_in_hi|W_fgt_hi|W_g_hi|W_out_hi)\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (4, hidden_dim, input_dim))\n",
    "        # hidden weights (W_in_hh|W_fgt_hh|W_g_hh|W_out_hh)\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (4, hidden_dim, hidden_dim))\n",
    "\n",
    "        self.tanh = Tanh\n",
    "        self.sigmoid = Sigmoid\n",
    "\n",
    "        self.gates = None\n",
    "        self.H = None\n",
    "        self.C = None\n",
    "\n",
    "        if self.use_bias:\n",
    "            # bias = (in_bias|fgt_bias|g_bias|out_bias)\n",
    "            self.bias = np.random.uniform(-sq, sq, (4, hidden_dim))\n",
    "        else:\n",
    "            self.bias = np.zeros((4, hidden_dim))\n",
    "\n",
    "    def forward(self, X_in, h_0=None, c_0=None):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        self.H = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if h_0 is not None:\n",
    "            self.H[:, 0, :] = h_0\n",
    "\n",
    "        self.C = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if c_0 is not None:\n",
    "            self.C[:, 0, :] = c_0\n",
    "\n",
    "        self.gates = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # input_gate\n",
    "            self.gates[0, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[0, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[0, :, :].T) + self.bias[0, :])\n",
    "            # forget gate\n",
    "            self.gates[1, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[1, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[1, :, :].T) + self.bias[1, :])\n",
    "            # c~ gate\n",
    "            self.gates[2, :, i, :] = self.tanh.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[2, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[2, :, :].T) + self.bias[2, :])\n",
    "            # output gate\n",
    "            self.gates[3, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[3, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[3, :, :].T) + self.bias[3, :])\n",
    "\n",
    "            self.C[:, i + 1, :] = self.gates[1, :, i, :] * self.C[:, i, :] + self.gates[0, :, i, :] * self.gates[2, :, i, :]\n",
    "            self.H[:, i + 1, :] = self.gates[3, :, i, :] * self.tanh.forward(self.C[:, i + 1, :])\n",
    "\n",
    "        return self.H, self.H[:, seq_len, :], self.C[:, seq_len, :]\n",
    "\n",
    "    def backward(self, X_in, dEdY):\n",
    "\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        C_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        X_grad = np.zeros((batch_size, seq_len, self.input_dim))\n",
    "\n",
    "        gates_grad = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                H_grad[:, i, :] = np.matmul(gates_grad[:, :, i + 1, :], self.hidden_weights).sum(axis=0) + dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad[:, i, :] * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :]) + C_grad[:, i + 1, :] * self.gates[1, :, i + 1, :]\n",
    "            else:\n",
    "                H_grad[:, i, :] = dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad[:, i, :] * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :])\n",
    "\n",
    "            gates_grad[0, :, i, :] = C_grad[:, i, :] * self.gates[2, :, i, :] * self.sigmoid.backward_calculated(self.gates[0, :, i, :])\n",
    "            gates_grad[1, :, i, :] = C_grad[:, i, :] * self.C[:, i, :] * self.sigmoid.backward_calculated(self.gates[1, :, i, :])\n",
    "            gates_grad[2, :, i, :] = C_grad[:, i, :] * self.gates[0, :, i, :] * self.tanh.backward_calculated(self.gates[2, :, i, :])\n",
    "            gates_grad[3, :, i, :] = H_grad[:, i, :] * self.tanh.forward(self.C[:, i + 1, :]) * self.sigmoid.backward_calculated(self.gates[3, :, i, :])\n",
    "\n",
    "            X_grad[:, i, :] = np.matmul(gates_grad[:, :, i, :], self.input_weights).sum(axis=0)\n",
    "                \n",
    "            dEdW_in[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                dEdW_hh[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in[0, :] += np.sum(gates_grad[0, :, i, :], axis=0)\n",
    "                dEdB_in[1, :] += np.sum(gates_grad[1, :, i, :], axis=0)\n",
    "                dEdB_in[2, :] += np.sum(gates_grad[2, :, i, :], axis=0)\n",
    "                dEdB_in[3, :] += np.sum(gates_grad[3, :, i, :], axis=0)\n",
    "        \n",
    "        return dEdW_in, dEdW_hh, dEdB_in, X_grad\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-sarah",
   "metadata": {},
   "source": [
    "## Softmax, Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aging-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "\n",
    "x = torch.randn(N, num_classes) # logits\n",
    "y = torch.randint(num_classes, (N,)) # labels\n",
    "\n",
    "x_ = x.numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()] # one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regional-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax check: True\n"
     ]
    }
   ],
   "source": [
    "softmax_out = F.softmax(x, dim=-1)\n",
    "softmax_out\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax_out_ = softmax.forward(x_)\n",
    "\n",
    "print('Softmax check:', np.isclose(softmax_out, softmax_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "focused-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss check: True\n"
     ]
    }
   ],
   "source": [
    "cel = nn.CrossEntropyLoss()\n",
    "loss = cel(x, y).item()\n",
    "\n",
    "cel_ = CrossEntropyLoss()\n",
    "loss_ = cel_.forward(y_, x_)\n",
    "loss_\n",
    "\n",
    "print('Cross entropy loss check:', np.isclose(loss, loss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-display",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numerous-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer forward check:  True\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "seq_len = 3\n",
    "\n",
    "x = torch.randn(N, seq_len, requires_grad=True)\n",
    "y = torch.randint(num_classes, (N,))\n",
    "\n",
    "x_ = x.detach().numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()]\n",
    "\n",
    "linear = nn.Linear(seq_len, num_classes)\n",
    "linear_ = DenseLayer(seq_len, num_classes)\n",
    "\n",
    "linear_.weights = linear.weight.detach().numpy()\n",
    "linear_.bias = linear.bias.detach().numpy()\n",
    "\n",
    "lin_out = linear(x)\n",
    "lin_out_ = linear_.forward(x_)\n",
    "\n",
    "print('Linear layer forward check: ', np.isclose(lin_out.detach().numpy(), lin_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "minor-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7812, grad_fn=<NllLossBackward>)\n",
      "[TORCH] dE/dy:\n",
      "tensor([[ 0.0154, -0.1254,  0.0595,  0.0505],\n",
      "        [ 0.0260, -0.0758,  0.0206,  0.0292],\n",
      "        [ 0.0358,  0.0895,  0.0610, -0.1863],\n",
      "        [ 0.0995,  0.0885, -0.1909,  0.0029],\n",
      "        [ 0.0300,  0.0869,  0.0454, -0.1623]])\n",
      "\n",
      "[TORCH] dE/dW:\n",
      "tensor([[ 0.0685,  0.1054, -0.2851],\n",
      "        [ 0.4073,  0.0524, -0.2640],\n",
      "        [-0.2708, -0.4270,  0.2497],\n",
      "        [-0.2051,  0.2692,  0.2994]])\n",
      "\n",
      "[TORCH] dE/dB:\n",
      "tensor([ 0.2067,  0.0636, -0.0044, -0.2660])\n",
      "\n",
      "[TORCH] dE/dX:\n",
      "tensor([[ 0.0082, -0.0504,  0.0747],\n",
      "        [ 0.0138, -0.0136,  0.0439],\n",
      "        [ 0.0379,  0.0321, -0.1905],\n",
      "        [ 0.0463,  0.1636, -0.0177],\n",
      "        [ 0.0319,  0.0328, -0.1681]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = cel(lin_out, y)\n",
    "lin_out.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(f'[TORCH] dE/dy:\\n{lin_out.grad}\\n')\n",
    "print(f'[TORCH] dE/dW:\\n{linear.weight.grad}\\n')\n",
    "print(f'[TORCH] dE/dB:\\n{linear.bias.grad}\\n')\n",
    "print(f'[TORCH] dE/dX:\\n{x.grad}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "gothic-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = cel_.forward(y_, lin_out_)\n",
    "\n",
    "de_dy = cel_.backward(y_)\n",
    "de_dx, de_dw, de_db = linear_.backward(de_dy, x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liked-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dy:\n",
      "[[ 0.01289863 -0.12475173  0.07498912  0.03686398]\n",
      " [ 0.01184563 -0.15936113  0.11452467  0.03299084]\n",
      " [ 0.09872308  0.02475784  0.04569956 -0.16918049]\n",
      " [-0.1750032   0.03356197  0.10637606  0.03506516]\n",
      " [-0.18160434  0.03501907  0.11235602  0.03422925]]\n",
      "\n",
      "[CUSTOM] dE/dW:\n",
      "[[-0.02335379  0.34712666  0.28857261]\n",
      " [ 0.18554716  0.20977268  0.25030635]\n",
      " [ 0.01165509 -0.29133464 -0.25821118]\n",
      " [-0.17384847 -0.26556471 -0.28066779]]\n",
      "\n",
      "[CUSTOM] dE/dB:\n",
      "[-0.2331402  -0.19077397  0.45394544 -0.03003125]\n",
      "\n",
      "[CUSTOM] dE/dX:\n",
      "[[ 0.09699437 -0.03969966 -0.00161258]\n",
      " [ 0.12726112 -0.05942757 -0.00761959]\n",
      " [ 0.0481368   0.0253786   0.03034998]\n",
      " [-0.07159334 -0.10329892 -0.09504108]\n",
      " [-0.07380924 -0.1078932  -0.09902084]]\n",
      "\n",
      "Check dE/dy: True\n",
      "Check dE/dX: True\n",
      "Check dE/dW: True\n",
      "Check dE/dB: True\n"
     ]
    }
   ],
   "source": [
    "print(f'[CUSTOM] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[CUSTOM] dE/dW:\\n{de_dw}\\n')\n",
    "print(f'[CUSTOM] dE/dB:\\n{de_db}\\n')\n",
    "print(f'[CUSTOM] dE/dX:\\n{de_dx}\\n')\n",
    "\n",
    "print('Check dE/dy:', np.isclose(lin_out.grad, de_dy).all())\n",
    "print('Check dE/dX:', np.isclose(x.grad, de_dx).all())\n",
    "print('Check dE/dW:', np.isclose(linear.weight.grad, de_dw).all())\n",
    "print('Check dE/dB:', np.isclose(linear.bias.grad, de_db).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-briefs",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coordinated-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN layer forward check:  True\n",
      "RNN layer forward check last hidden:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "rnn = nn.RNN(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "rnn_ = RnnLayer(emb_dim, hidden_dim, seq_len, N, use_bias=False)\n",
    "rnn_.input_weights = rnn.weight_ih_l0.detach().numpy()\n",
    "rnn_.hidden_weights = rnn.weight_hh_l0.detach().numpy()\n",
    "\n",
    "rnn_out, h_n = rnn(x)\n",
    "rnn_out_, h_n_ = rnn_.forward(x_)\n",
    "rnn_out__ = rnn_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('RNN layer forward check: ', np.isclose(rnn_out.detach().numpy(), rnn_out__).all())\n",
    "print('RNN layer forward check last hidden: ', np.isclose(h_n.detach().numpy(), h_n_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "surprised-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TORCH] dE/dWih:\n",
      "tensor([[-3.3167,  0.5416, -7.4245,  0.0461, -1.4704, -1.7172],\n",
      "        [-0.7243,  1.1735,  2.7081,  2.2718, -2.2479, -1.1513],\n",
      "        [ 0.6725,  3.8284, -5.6269,  1.1262, -1.7440,  1.2039],\n",
      "        [-0.2735, -0.0848,  0.6839,  4.6405,  2.1151,  1.0014],\n",
      "        [-1.8822, -2.2299,  2.0466, -2.6408,  2.7569,  7.1491]])\n",
      "\n",
      "[TORCH] dE/dWhh:\n",
      "tensor([[ 1.2223,  0.4866, -1.0587, -0.5920, -0.6407],\n",
      "        [-1.0210,  0.4920,  0.9444,  0.0907,  1.6406],\n",
      "        [ 0.9988, -0.2676, -0.1303, -1.7490, -0.9632],\n",
      "        [ 0.5061, -0.0735,  0.3186, -1.2058, -0.2807],\n",
      "        [-0.0880, -0.1201, -0.5381,  1.0406, -1.2745]])\n",
      "\n",
      "[TORCH] dE/dy:\n",
      "tensor([[[ 1.3564e+00,  4.0721e-02,  1.6143e+00,  4.7909e-01,  7.8997e-01],\n",
      "         [-6.2923e-01,  1.7595e-01, -7.4402e-01, -7.8599e-01,  1.1243e-01],\n",
      "         [ 7.2088e-01,  1.3641e+00, -1.3155e+00,  5.0142e-02, -1.3644e+00]],\n",
      "\n",
      "        [[ 1.1343e+00,  8.4810e-01,  1.2058e+00, -6.4061e-01, -2.5855e+00],\n",
      "         [-6.5947e-02,  1.6423e-01, -2.4149e-04, -1.3041e-02, -2.0471e-01],\n",
      "         [-1.2383e+00,  1.6254e-01, -7.7324e-01, -5.3715e-01,  7.8861e-02]],\n",
      "\n",
      "        [[ 7.5662e-01, -1.2905e+00,  9.7097e-01, -4.5687e-02,  1.7481e-01],\n",
      "         [-9.4000e-01,  1.9173e+00, -1.1829e+00,  4.1989e-01,  1.1297e+00],\n",
      "         [-6.5529e-01, -2.1240e-01, -1.9119e+00, -6.2758e-01,  1.4387e+00]],\n",
      "\n",
      "        [[-1.0839e+00,  5.3420e-01, -5.0448e-02,  7.2085e-01, -9.9162e-01],\n",
      "         [-1.3084e+00,  2.9591e-01,  4.7226e-01, -4.1868e-01,  7.6635e-01],\n",
      "         [ 1.2526e+00,  9.7521e-01, -3.0203e-01,  1.1785e-01, -2.2540e+00]],\n",
      "\n",
      "        [[-7.3599e-01,  5.4435e-01,  9.6250e-01, -2.4514e+00, -3.0609e-01],\n",
      "         [ 4.0075e-01,  4.1055e-02,  5.1825e-01,  6.4937e-01,  4.2090e-01],\n",
      "         [-1.6045e+00,  7.0722e-01, -2.5785e-01,  3.2105e-01, -9.6245e-01]]])\n",
      "\n",
      "[TORCH] dE/dH:\n",
      "tensor([[[ 1.3564e+00,  4.0721e-02,  1.6143e+00,  4.7909e-01,  7.8997e-01],\n",
      "         [-6.2923e-01,  1.7595e-01, -7.4402e-01, -7.8599e-01,  1.1243e-01],\n",
      "         [ 7.2088e-01,  1.3641e+00, -1.3155e+00,  5.0142e-02, -1.3644e+00]],\n",
      "\n",
      "        [[ 1.1343e+00,  8.4810e-01,  1.2058e+00, -6.4061e-01, -2.5855e+00],\n",
      "         [-6.5947e-02,  1.6423e-01, -2.4149e-04, -1.3041e-02, -2.0471e-01],\n",
      "         [-1.2383e+00,  1.6254e-01, -7.7324e-01, -5.3715e-01,  7.8861e-02]],\n",
      "\n",
      "        [[ 7.5662e-01, -1.2905e+00,  9.7097e-01, -4.5687e-02,  1.7481e-01],\n",
      "         [-9.4000e-01,  1.9173e+00, -1.1829e+00,  4.1989e-01,  1.1297e+00],\n",
      "         [-6.5529e-01, -2.1240e-01, -1.9119e+00, -6.2758e-01,  1.4387e+00]],\n",
      "\n",
      "        [[-1.0839e+00,  5.3420e-01, -5.0448e-02,  7.2085e-01, -9.9162e-01],\n",
      "         [-1.3084e+00,  2.9591e-01,  4.7226e-01, -4.1868e-01,  7.6635e-01],\n",
      "         [ 1.2526e+00,  9.7521e-01, -3.0203e-01,  1.1785e-01, -2.2540e+00]],\n",
      "\n",
      "        [[-7.3599e-01,  5.4435e-01,  9.6250e-01, -2.4514e+00, -3.0609e-01],\n",
      "         [ 4.0075e-01,  4.1055e-02,  5.1825e-01,  6.4937e-01,  4.2090e-01],\n",
      "         [-1.6045e+00,  7.0722e-01, -2.5785e-01,  3.2105e-01, -9.6245e-01]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "rnn_out.retain_grad()\n",
    "rnn_out.backward(de_dy)\n",
    "\n",
    "print(f'[TORCH] dE/dWih:\\n{rnn.weight_ih_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dWhh:\\n{rnn.weight_hh_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[TORCH] dE/dH:\\n{rnn_out.grad}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fourth-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dWih:\n",
      "[[-2.9129841   0.3366964  -7.298915    0.2762408  -1.308749   -2.1736734 ]\n",
      " [-1.0284758   0.95830005  2.6368146   1.8294667  -2.5619981  -0.7641725 ]\n",
      " [-0.73598444  4.11507    -5.719607    0.6079343  -1.8728046   1.4738001 ]\n",
      " [-0.34538245 -0.6158072   0.46779677  4.2226386   2.1171274   0.37274322]\n",
      " [-1.2222011  -2.0906603   3.08966    -2.145599    2.9344556   7.363285  ]]\n",
      "\n",
      "[CUSTOM] dE/dWhh:\n",
      "[[ 1.1868922   0.453833   -1.0819117  -0.5528156  -0.64420515]\n",
      " [-0.89827275  0.5567818   0.87454146  0.05949303  1.6204437 ]\n",
      " [ 0.74795824 -0.27476835  0.00224801 -1.4019312  -0.8334215 ]\n",
      " [ 0.4918106  -0.08144882  0.18285216 -1.0069394  -0.3097951 ]\n",
      " [-0.04485604 -0.03566557 -0.38454136  0.77045953 -1.1370219 ]]\n",
      "\n",
      "[CUSTOM] dE/dH:\n",
      "[[[ 0.92513262  0.08011615  1.68275788  0.54211286  0.73723214]\n",
      "  [-1.26205693  1.07338584 -1.26625229 -0.36155829  0.11710336]\n",
      "  [ 0.72087544  1.36405849 -1.31551814  0.05014242 -1.36439168]]\n",
      "\n",
      " [[ 1.25257488  0.80552966  1.14910393 -0.55060726 -2.47052174]\n",
      "  [-0.05354902 -0.03765595  0.05716798  0.07499788 -0.37070373]\n",
      "  [-1.23826265  0.16254093 -0.77324039 -0.53715128  0.07886122]]\n",
      "\n",
      " [[-0.26922198 -0.88626468  1.27180546 -0.6251323   0.90033098]\n",
      "  [-1.47553392  2.18175762 -0.61379473  0.74212722  0.91519791]\n",
      "  [-0.65528667 -0.21239708 -1.91194642 -0.62757969  1.43870163]]\n",
      "\n",
      " [[-1.41976299  0.20943386  0.16986264  0.48112842 -0.73388564]\n",
      "  [-1.27273552  0.95634206  0.12709034 -0.04106816  1.08069138]\n",
      "  [ 1.25258255  0.97520566 -0.3020252   0.11785372 -2.25401425]]\n",
      "\n",
      " [[-1.21975668  1.10757162  1.38505904 -3.04732364 -0.19856568]\n",
      "  [ 0.79226959 -0.24802659  0.49251603  0.95370483  1.09888269]\n",
      "  [-1.60453892  0.70721656 -0.25784838  0.3210468  -0.96244532]]]\n",
      "\n",
      "RNN layer gradient check dEdW_in:  False\n",
      "RNN layer gradient check dEdW_hh:  False\n",
      "RNN layer gradient check dEdH:  False\n"
     ]
    }
   ],
   "source": [
    "dEdW_in, dEdW_hh, _, H_grad = rnn_.backward(x_, rnn_out_, de_dy_)\n",
    "\n",
    "print(f'[CUSTOM] dE/dWih:\\n{dEdW_in}\\n')\n",
    "print(f'[CUSTOM] dE/dWhh:\\n{dEdW_hh}\\n')\n",
    "print(f'[CUSTOM] dE/dH:\\n{H_grad[:,1:,:]}\\n')\n",
    "\n",
    "print('RNN layer gradient check dEdW_in: ', np.isclose(rnn.weight_ih_l0.grad.numpy(), dEdW_in).all())\n",
    "print('RNN layer gradient check dEdW_hh: ', np.isclose(rnn.weight_hh_l0.grad.numpy(), dEdW_hh).all())\n",
    "print('RNN layer gradient check dEdH: ', np.isclose(rnn_out.grad.numpy(), H_grad[:,1:,:]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-rebate",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "private-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer forward check:  True\n",
      "LSTM layer forward check last hidden:  True\n",
      "LSTM layer forward check last c_n:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 4\n",
    "emb_dim = 3\n",
    "seq_len = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "lstm = nn.LSTM(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "lstm_ = LSTMLayer(emb_dim, hidden_dim, use_bias=False)\n",
    "wih = lstm.weight_ih_l0.detach().numpy()\n",
    "whh = lstm.weight_hh_l0.detach().numpy()\n",
    "\n",
    "lstm_.input_weights[0,:,:] = wih[0:hidden_dim, :]\n",
    "lstm_.input_weights[1,:,:] = wih[hidden_dim: 2*hidden_dim, :]\n",
    "lstm_.input_weights[2,:,:] = wih[2*hidden_dim: 3*hidden_dim, :]\n",
    "lstm_.input_weights[3,:,:] = wih[3*hidden_dim: 4*hidden_dim, :]\n",
    "\n",
    "lstm_.hidden_weights[0,:,:] = whh[0:hidden_dim, :]\n",
    "lstm_.hidden_weights[1,:,:] = whh[hidden_dim: 2*hidden_dim, :]\n",
    "lstm_.hidden_weights[2,:,:] = whh[2*hidden_dim: 3*hidden_dim, :]\n",
    "lstm_.hidden_weights[3,:,:] = whh[3*hidden_dim: 4*hidden_dim, :]\n",
    "\n",
    "\n",
    "lstm_out, h_n = lstm(x)\n",
    "lstm_out_, h_n_, c_n_ = lstm_.forward(x_)\n",
    "lstm_out__ = lstm_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('LSTM layer forward check: ', np.isclose(lstm_out.detach().numpy(), lstm_out__, atol=1e-3).all())\n",
    "print('LSTM layer forward check last hidden: ', np.isclose(h_n[0].detach().numpy(), h_n_, atol=1e-3).all())\n",
    "print('LSTM layer forward check last c_n: ', np.isclose(h_n[1].detach().numpy(), c_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "painful-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "x.retain_grad()\n",
    "lstm_out.backward(de_dy)\n",
    "\n",
    "dEdW_in, dEdW_hh, a, X_grad = lstm_.backward(x_, de_dy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "honest-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer gradient check dEdW_in:  True\n",
      "LSTM layer gradient check dEdW_hh:  True\n",
      "LSTM layer gradient check dEdX:  True\n"
     ]
    }
   ],
   "source": [
    "print('LSTM layer gradient check dEdW_in: ', np.isclose(lstm.weight_ih_l0.grad.numpy(), dEdW_in.reshape(4*hidden_dim,emb_dim), atol=1e-3).all())\n",
    "print('LSTM layer gradient check dEdW_hh: ', np.isclose(lstm.weight_hh_l0.grad.numpy(), dEdW_hh.reshape(4*hidden_dim,hidden_dim), atol=1e-3).all())\n",
    "print('LSTM layer gradient check dEdX: ', np.isclose(x.grad.numpy(), X_grad, atol=1e-3).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
