{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "treated-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "standard-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        exps = np.exp(x_in-np.max(x_in, axis=-1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "class Sigmoid(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return 1./(1 + np.exp(-x_in))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        fw = Sigmoid().forward(x_in)\n",
    "        return fw * (1 - fw)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward_calculated(sigmoid_x):\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n",
    "    \n",
    "    \n",
    "class Tanh(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(X_in):\n",
    "        return np.tanh(X_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(X_in):\n",
    "        # dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(X_in)) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward_calculated(tanh_x_in):\n",
    "        return 1 - tanh_x_in ** 2\n",
    "\n",
    "\n",
    "class ReLu(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.maximum(x_in, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        return x_in > 0\n",
    "    \n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, y, o):\n",
    "        self.y_pred = Softmax.forward(o)\n",
    "        return np.sum(-y * np.log(self.y_pred + 1e-15))/(y.shape[0])\n",
    "\n",
    "    def backward(self, y):\n",
    "        return (self.y_pred - y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "engaging-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        # treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "\n",
    "        # u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            input_part = np.einsum('ij,jk->ik', x_in[:, i, :], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', H[:, i, :], self.hidden_weights.T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def book_forward(self, x_in):\n",
    "\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            # ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki', self.input_weights, x_in[:, i, :].T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', self.hidden_weights, H[:, i, :].T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def backward(self, x, h, dEdY):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        act = self.activation.backward_calculated(h)\n",
    "\n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "            if i < seq_len - 1:\n",
    "                H_grad[:, i, :] = np.dot(H_grad[:, i + 1, :] * act[:, i+2, :], self.hidden_weights) + dEdY[:, i, :]\n",
    "            else:\n",
    "                H_grad[:, i, :] = dEdY[:, i, :]\n",
    "\n",
    "            dEdW_in += np.sum(act[:, i+1, :].reshape(batch_size, self.hidden_dim, 1) * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i, :])), axis=0)\n",
    "            dEdW_hh += np.sum(act[:, i+1, :].reshape(batch_size, self.hidden_dim, 1) * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(act[:, i+1, :] * H_grad[:, i, :], axis=0)\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_2nd(self, x, h, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, seq_len, :] = dEdY[:, seq_len - 1, :]\n",
    "\n",
    "        for i in range(seq_len, 0, -1):\n",
    "\n",
    "            activation_backward = self.activation.backward_calculated(h[:, i, :])\n",
    "            back_reshaped = activation_backward.reshape(batch_size, self.hidden_dim, 1)\n",
    "\n",
    "            dEdW_in += np.sum(back_reshaped * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i - 1, :])), axis=0)\n",
    "            dEdW_hh += np.sum(back_reshaped * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i - 1, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(activation_backward * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "            b = np.dot(H_grad[:, i, :], self.hidden_weights)\n",
    "            a = b * activation_backward\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = a + dEdY[:, i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.dot(H_grad[:, i, :], self.hidden_weights) * activation_backward\n",
    "\n",
    "            # if i > 1:\n",
    "            #    H_grad[:, i - 1, :] = ((np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * activation_backward) + dEdY[:, i - 2, :])\n",
    "            # else:\n",
    "            #    H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * activation_backward\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        seq_len = X.shape[1]\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, seq_len, :] = dEdY[:, seq_len - 1, :]\n",
    "\n",
    "        for i in range(seq_len, 0, -1):\n",
    "\n",
    "            for k in range(batch_size):\n",
    "                act_grad = np.diag(self.activation.backward_calculated(H[k, i, :]))\n",
    "                h_grad = H_grad[k, i, :].reshape(self.hidden_dim, 1)\n",
    "\n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k, i - 1, :].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k, i - 1, :].reshape(1, self.hidden_dim)))\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward_calculated(H[:, i, :]) * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward_calculated(H[:, i, :]) + dEdY[:,\n",
    "                                                                                                                         i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward_calculated(H[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "    \n",
    "    \n",
    "class DenseLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        sq = np.sqrt(1. / input_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = np.random.uniform(-sq, sq, (output_dim, input_dim))\n",
    "        if use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, output_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(output_dim)\n",
    "\n",
    "        self.x_in = None\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        self.x_in = x_in\n",
    "        return np.einsum('b...i,ih->b...h', self.x_in, self.weights.T) + self.bias\n",
    "\n",
    "    def backward(self, de_dy):\n",
    "        # de_dw = de_dy * dYdW = de_dy * X\n",
    "        # dEdb = de_dy * dYdb = de_dy\n",
    "        # dEdX = de_dy * dYdX = de_dy * W\n",
    "\n",
    "        #einsum nema mogucnost sumiranja po opcionalnim\n",
    "        #dimenzijama, ako barem jedan od argumenata nije fiksan\n",
    "        #zato se koristi tensordot\n",
    "        axis = tuple(range(len(self.x_in.shape) - 1))\n",
    "        de_dw = np.tensordot(de_dy, self.x_in, axes=(axis, axis))\n",
    "        de_db = de_dy.sum(axis=axis)\n",
    "        de_dx = np.einsum('b...h,hi->b...i', de_dy, self.weights)\n",
    "\n",
    "        return de_dx, de_dw, de_db\n",
    "            \n",
    "            \n",
    "class LSTMLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, use_bias=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        # input weights (W_in_hi|W_fgt_hi|W_g_hi|W_out_hi)\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (4, hidden_dim, input_dim))\n",
    "        # hidden weights (W_in_hh|W_fgt_hh|W_g_hh|W_out_hh)\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (4, hidden_dim, hidden_dim))\n",
    "\n",
    "        self.tanh = Tanh\n",
    "        self.sigmoid = Sigmoid\n",
    "\n",
    "        self.gates = None\n",
    "        self.H = None\n",
    "        self.C = None\n",
    "\n",
    "        if self.use_bias:\n",
    "            # bias = (in_bias|fgt_bias|g_bias|out_bias)\n",
    "            self.bias = np.random.uniform(-sq, sq, (4, hidden_dim))\n",
    "        else:\n",
    "            self.bias = np.zeros((4, hidden_dim))\n",
    "\n",
    "    def forward(self, X_in, h_0=None, c_0=None):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        self.H = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if h_0 is not None:\n",
    "            self.H[:, 0, :] = h_0\n",
    "\n",
    "        self.C = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if c_0 is not None:\n",
    "            self.C[:, 0, :] = c_0\n",
    "\n",
    "        self.gates = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # input_gate\n",
    "            self.gates[0, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[0, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[0, :, :].T) + self.bias[0, :])\n",
    "            # forget gate\n",
    "            self.gates[1, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[1, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[1, :, :].T) + self.bias[1, :])\n",
    "            # c~ gate\n",
    "            self.gates[2, :, i, :] = self.tanh.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[2, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[2, :, :].T) + self.bias[2, :])\n",
    "            # output gate\n",
    "            self.gates[3, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[3, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[3, :, :].T) + self.bias[3, :])\n",
    "\n",
    "            self.C[:, i + 1, :] = self.gates[1, :, i, :] * self.C[:, i, :] + self.gates[0, :, i, :] * self.gates[2, :, i, :]\n",
    "            self.H[:, i + 1, :] = self.gates[3, :, i, :] * self.tanh.forward(self.C[:, i + 1, :])\n",
    "\n",
    "        return self.H, self.H[:, seq_len, :], self.C[:, seq_len, :]\n",
    "\n",
    "    def backward(self, X_in, dEdY):\n",
    "\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        C_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        X_grad = np.zeros((batch_size, seq_len, self.input_dim))\n",
    "\n",
    "        gates_grad = np.zeros((4, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                H_grad = np.matmul(gates_grad[:, :, i + 1, :], self.hidden_weights).sum(axis=0) + dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :]) + C_grad[:, i + 1, :] * self.gates[1, :, i + 1, :]\n",
    "            else:\n",
    "                H_grad = dEdY[:, i, :]\n",
    "                C_grad[:, i, :] = H_grad * self.gates[3, :, i, :] * self.tanh.backward(self.C[:, i + 1, :])\n",
    "\n",
    "            gates_grad[0, :, i, :] = C_grad[:, i, :] * self.gates[2, :, i, :] * self.sigmoid.backward_calculated(self.gates[0, :, i, :])\n",
    "            gates_grad[1, :, i, :] = C_grad[:, i, :] * self.C[:, i, :] * self.sigmoid.backward_calculated(self.gates[1, :, i, :])\n",
    "            gates_grad[2, :, i, :] = C_grad[:, i, :] * self.gates[0, :, i, :] * self.tanh.backward_calculated(self.gates[2, :, i, :])\n",
    "            gates_grad[3, :, i, :] = H_grad * self.tanh.forward(self.C[:, i + 1, :]) * self.sigmoid.backward_calculated(self.gates[3, :, i, :])\n",
    "\n",
    "            X_grad[:, i, :] = np.matmul(gates_grad[:, :, i, :], self.input_weights).sum(axis=0)\n",
    "\n",
    "            dEdW_in[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "            dEdW_in[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i, :], X_in[:, i, :]).sum(axis=0)\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                dEdW_hh[0, :, :] += np.einsum('bi,bo->bio', gates_grad[0, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[1, :, :] += np.einsum('bi,bo->bio', gates_grad[1, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[2, :, :] += np.einsum('bi,bo->bio', gates_grad[2, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "                dEdW_hh[3, :, :] += np.einsum('bi,bo->bio', gates_grad[3, :, i + 1, :], self.H[:, i + 1, :]).sum(axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in[0, :] += np.sum(gates_grad[0, :, i, :], axis=0)\n",
    "                dEdB_in[1, :] += np.sum(gates_grad[1, :, i, :], axis=0)\n",
    "                dEdB_in[2, :] += np.sum(gates_grad[2, :, i, :], axis=0)\n",
    "                dEdB_in[3, :] += np.sum(gates_grad[3, :, i, :], axis=0)\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in, X_grad\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class GRULayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, use_bias=True):\n",
    "        \n",
    "        #r_t = sigmoid(W_r_hi.x_t + W_r_hh.h_(t-1) + b_r)\n",
    "        #z_t = sigmoid(W_z_hi.x_t + W_z_hh.h_(t-1) + b_z)\n",
    "        #c_t = tanh(W_n_hi.x_t + W_n_hh.h_(t-1) * r_t + b_c)\n",
    "        #h_t = (1-z_t) * n_t + z_t * h_(t-1)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        # input weights [W_r_hi,W_z_hi,W_c_hi]\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (3, hidden_dim, input_dim))\n",
    "        # hidden weights [W_r_hi,W_z_hi,W_c_hi]\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (3, hidden_dim, hidden_dim))\n",
    "\n",
    "        self.tanh = Tanh\n",
    "        self.sigmoid = Sigmoid\n",
    "\n",
    "        self.gates = None\n",
    "        self.H = None\n",
    "        self.C = None\n",
    "\n",
    "        if self.use_bias:\n",
    "            # bias = [r_bias,z_bias,c_bias]\n",
    "            self.bias = np.random.uniform(-sq, sq, (3, hidden_dim))\n",
    "        else:\n",
    "            self.bias = np.zeros((3, hidden_dim))\n",
    "\n",
    "    def forward(self, X_in, h_0=None, c_0=None):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        self.H = np.zeros((batch_size, seq_len + 1, self.hidden_dim))\n",
    "        if h_0 is not None:\n",
    "            self.H[:, 0, :] = h_0\n",
    "\n",
    "        self.gates = np.zeros((3, batch_size, seq_len, self.hidden_dim))\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # reset_gate\n",
    "            self.gates[0, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[0, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[0, :, :].T) + self.bias[0, :])\n",
    "            # z_gate\n",
    "            self.gates[1, :, i, :] = self.sigmoid.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[1, :, :].T) + np.dot(self.H[:, i, :], self.hidden_weights[1, :, :].T) + self.bias[1, :])\n",
    "            # update gate\n",
    "            self.gates[2, :, i, :] = self.tanh.forward(\n",
    "                np.dot(X_in[:, i, :], self.input_weights[2, :, :].T) + self.gates[0,:,i,:] * np.dot(self.H[:, i, :], self.hidden_weights[2, :, :].T) + self.bias[2, :])\n",
    "            \n",
    "            self.H[:, i + 1, :] = self.gates[1, :, i, :] * self.H[:, i, :] +  (1 - self.gates[1, :, i, :]) * self.gates[2,:,i,:]\n",
    "\n",
    "        return self.H, self.H[:, seq_len, :]\n",
    "    \n",
    "    def backward(self, X_in, dEdY):\n",
    "        batch_size = X_in.shape[0]\n",
    "        seq_len = X_in.shape[1]\n",
    "\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((batch_size, seq_len, self.hidden_dim))\n",
    "        X_grad = np.zeros((batch_size, seq_len, self.input_dim))\n",
    "        \n",
    "        gates_grad = np.zeros((3, batch_size, seq_len, self.hidden_dim))\n",
    "        \n",
    "        for i in range(seq_len - 1, -1, -1):\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                H_grad[:, i, :] = np.matmul(gates_grad[:, :, i + 1, :], self.hidden_weights).sum(axis=0) + dEdY[:, i, :]\n",
    "            else:\n",
    "                H_grad[:, i, :] = dEdY[:, i, :]\n",
    "\n",
    "            gates_grad[2, :, i, :] = (1 - self.gates[1, :, i, :]) * dEdY[:, i, :] * self.tanh.backward_calculated(self.gates[2, :, i, :])\n",
    "            gates_grad[1, :, i, :] = ((self.H[:, i, :] - self.gates[2, :, i, :]) * dEdY[:, i, :]) * self.sigmoid.backward_calculated(self.gates[1, :, i, :])\n",
    "            gates_grad[0, :, i, :] = (np.dot(gates_grad[2,:,i,:], self.hidden_weights[2,:,:].T) * self.H[:,i,:]) * self.sigmoid.backward_calculated(self.gates[0, :, i, :])\n",
    "            \n",
    "            X_grad[:, i, :] = np.dot(gates_grad[2,:,i,:], self.input_weights[2,:,:].T) + np.dot(gates_grad[1,:,i,:], self.input_weights[1,:,:].T) + np.dot(gates_grad[0,:,i,:], self.input_weights[0,:,:].T)\n",
    "                \n",
    "            h_t_T = self.H[:, i, :].T    \n",
    "                \n",
    "            dEdW_in[0, :, :] += np.dot(gates_grad[0, :, i, :].T, X_in[:, i, :])\n",
    "            dEdW_in[1, :, :] += np.dot(gates_grad[1, :, i, :].T, X_in[:, i, :])\n",
    "            dEdW_in[2, :, :] += np.dot(gates_grad[2, :, i, :].T, X_in[:, i, :])\n",
    "            \n",
    "            if i < seq_len - 1:\n",
    "                dEdW_hh[0, :, :] += np.dot(h_t_T, gates_grad[0, :, i, :])\n",
    "                dEdW_hh[1, :, :] += np.dot(h_t_T, gates_grad[1, :, i, :])\n",
    "                dEdW_hh[2, :, :] += np.dot((self.H[:, i, :] * self.gates[0, :, i, :]).T, gates_grad[2, :, i, :])\n",
    "                \n",
    "            if self.use_bias:\n",
    "                dEdB_in[0, :] += np.sum(gates_grad[0, :, i, :], axis=0)\n",
    "                dEdB_in[1, :] += np.sum(gates_grad[1, :, i, :], axis=0)\n",
    "                dEdB_in[2, :] += np.sum(gates_grad[2, :, i, :], axis=0)\n",
    "                \n",
    "        return dEdW_in, dEdW_hh, dEdB_in, X_grad\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-sarah",
   "metadata": {},
   "source": [
    "## Softmax, Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aging-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "\n",
    "x = torch.randn(N, num_classes) # logits\n",
    "y = torch.randint(num_classes, (N,)) # labels\n",
    "\n",
    "x_ = x.numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()] # one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regional-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax check: True\n"
     ]
    }
   ],
   "source": [
    "softmax_out = F.softmax(x, dim=-1)\n",
    "softmax_out\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax_out_ = softmax.forward(x_)\n",
    "\n",
    "print('Softmax check:', np.isclose(softmax_out, softmax_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "focused-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss check: True\n"
     ]
    }
   ],
   "source": [
    "cel = nn.CrossEntropyLoss()\n",
    "loss = cel(x, y).item()\n",
    "\n",
    "cel_ = CrossEntropyLoss()\n",
    "loss_ = cel_.forward(y_, x_)\n",
    "loss_\n",
    "\n",
    "print('Cross entropy loss check:', np.isclose(loss, loss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-display",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numerous-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer forward check:  True\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "seq_len = 3\n",
    "\n",
    "x = torch.randn(N, seq_len, requires_grad=True)\n",
    "y = torch.randint(num_classes, (N,))\n",
    "\n",
    "x_ = x.detach().numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()]\n",
    "\n",
    "linear = nn.Linear(seq_len, num_classes)\n",
    "linear_ = DenseLayer(seq_len, num_classes)\n",
    "\n",
    "linear_.weights = linear.weight.detach().numpy()\n",
    "linear_.bias = linear.bias.detach().numpy()\n",
    "\n",
    "lin_out = linear(x)\n",
    "lin_out_ = linear_.forward(x_)\n",
    "\n",
    "print('Linear layer forward check: ', np.isclose(lin_out.detach().numpy(), lin_out_,atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "minor-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8085, grad_fn=<NllLossBackward>)\n",
      "[TORCH] dE/dy:\n",
      "tensor([[ 0.0754,  0.0343,  0.0554, -0.1651],\n",
      "        [ 0.0504,  0.0551,  0.0752, -0.1808],\n",
      "        [ 0.0337,  0.1067,  0.0430, -0.1834],\n",
      "        [ 0.0211,  0.0992, -0.1416,  0.0213],\n",
      "        [ 0.0498, -0.1418,  0.0624,  0.0296]])\n",
      "\n",
      "[TORCH] dE/dW:\n",
      "tensor([[ 0.0145,  0.0878, -0.1285],\n",
      "        [-0.1073, -0.1311, -0.0720],\n",
      "        [-0.0961,  0.3064, -0.1093],\n",
      "        [ 0.1888, -0.2631,  0.3098]])\n",
      "\n",
      "[TORCH] dE/dB:\n",
      "tensor([ 0.2304,  0.1536,  0.0944, -0.4784])\n",
      "\n",
      "[TORCH] dE/dX:\n",
      "tensor([[-0.0571, -0.0092, -0.0331],\n",
      "        [-0.0670, -0.0345, -0.0492],\n",
      "        [-0.0890, -0.0650, -0.0415],\n",
      "        [-0.0427, -0.0243,  0.0617],\n",
      "        [ 0.0646,  0.0907, -0.0064]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = cel(lin_out, y)\n",
    "lin_out.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(f'[TORCH] dE/dy:\\n{lin_out.grad}\\n')\n",
    "print(f'[TORCH] dE/dW:\\n{linear.weight.grad}\\n')\n",
    "print(f'[TORCH] dE/dB:\\n{linear.bias.grad}\\n')\n",
    "print(f'[TORCH] dE/dX:\\n{x.grad}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gothic-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = cel_.forward(y_, lin_out_)\n",
    "\n",
    "de_dy = cel_.backward(y_)\n",
    "de_dx, de_dw, de_db = linear_.backward(de_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liked-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dy:\n",
      "[[ 0.07540435  0.03428566  0.05541155 -0.16510156]\n",
      " [ 0.05040607  0.05514765  0.07521916 -0.18077289]\n",
      " [ 0.03371936  0.10667331  0.04300876 -0.18340143]\n",
      " [ 0.02109811  0.09923471 -0.14163023  0.02129741]\n",
      " [ 0.04978328 -0.14177455  0.06243125  0.02956001]]\n",
      "\n",
      "[CUSTOM] dE/dW:\n",
      "[[ 0.01450441  0.08779823 -0.1285421 ]\n",
      " [-0.10727576 -0.13110164 -0.0719611 ]\n",
      " [-0.09606381  0.30635763 -0.10931161]\n",
      " [ 0.18883515 -0.26305424  0.30981482]]\n",
      "\n",
      "[CUSTOM] dE/dB:\n",
      "[ 0.23041117  0.15356679  0.09444049 -0.47841845]\n",
      "\n",
      "[CUSTOM] dE/dX:\n",
      "[[-0.05705203 -0.00918977 -0.03310385]\n",
      " [-0.06695847 -0.03450404 -0.04918016]\n",
      " [-0.08895305 -0.06496174 -0.04149992]\n",
      " [-0.04273666 -0.02430381  0.06174648]\n",
      " [ 0.06462991  0.09073251 -0.0063831 ]]\n",
      "\n",
      "Check dE/dy: True\n",
      "Check dE/dX: True\n",
      "Check dE/dW: True\n",
      "Check dE/dB: True\n"
     ]
    }
   ],
   "source": [
    "print(f'[CUSTOM] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[CUSTOM] dE/dW:\\n{de_dw}\\n')\n",
    "print(f'[CUSTOM] dE/dB:\\n{de_db}\\n')\n",
    "print(f'[CUSTOM] dE/dX:\\n{de_dx}\\n')\n",
    "\n",
    "print('Check dE/dy:', np.isclose(lin_out.grad, de_dy).all())\n",
    "print('Check dE/dX:', np.isclose(x.grad, de_dx).all())\n",
    "print('Check dE/dW:', np.isclose(linear.weight.grad, de_dw).all())\n",
    "print('Check dE/dB:', np.isclose(linear.bias.grad, de_db).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-briefs",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coordinated-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN layer forward check:  True\n",
      "RNN layer forward check last hidden:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "rnn = nn.RNN(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "rnn_ = RnnLayer(emb_dim, hidden_dim, seq_len, N, use_bias=False)\n",
    "rnn_.input_weights = rnn.weight_ih_l0.detach().numpy()\n",
    "rnn_.hidden_weights = rnn.weight_hh_l0.detach().numpy()\n",
    "\n",
    "x.retain_grad()\n",
    "rnn_out, h_n = rnn(x)\n",
    "rnn_out_, h_n_ = rnn_.forward(x_)\n",
    "rnn_out__ = rnn_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('RNN layer forward check: ', np.isclose(rnn_out.detach().numpy(), rnn_out__, atol=1e-3).all())\n",
    "print('RNN layer forward check last hidden: ', np.isclose(h_n.detach().numpy(), h_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "surprised-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TORCH] dE/dWih:\n",
      "tensor([[ 1.7141,  1.9824,  1.0661,  2.1549,  6.6352, -0.7808],\n",
      "        [-0.8973, -2.4814, -0.6834, -0.6731, -2.3435, -0.4271],\n",
      "        [-1.0498,  1.8254, -0.4997, -8.1593,  6.2764, -5.4964],\n",
      "        [-8.0790, -2.2785, -0.0352, -3.4041,  1.2095,  5.9440],\n",
      "        [-0.5816, -1.4935, -2.9621, -0.8830,  4.2657, -2.3443]])\n",
      "\n",
      "[TORCH] dE/dWhh:\n",
      "tensor([[-0.2543, -0.6755, -0.5482, -1.4885,  1.5176],\n",
      "        [ 1.0794, -0.6978, -0.0932,  2.3761,  1.4980],\n",
      "        [-0.0539, -0.7005,  1.0785, -2.4175, -4.7500],\n",
      "        [ 0.4645,  1.6020,  0.5751,  1.5038, -1.7592],\n",
      "        [-0.5096, -0.6170, -0.0402, -0.9757,  0.5781]])\n",
      "\n",
      "[TORCH] dE/dy:\n",
      "tensor([[[-0.3532,  0.2270,  0.1020,  1.6220,  0.2858],\n",
      "         [-0.2570,  0.9653, -0.4660, -0.0568,  0.4982],\n",
      "         [ 1.8082, -1.1110,  0.5169, -1.6098,  0.7948]],\n",
      "\n",
      "        [[ 0.3515,  1.1685, -0.0134,  1.5991,  1.3022],\n",
      "         [ 1.5997, -1.4602,  1.0215, -0.6152, -0.1429],\n",
      "         [ 1.6330,  0.7056, -3.0349, -2.5106,  1.6060]],\n",
      "\n",
      "        [[-1.1461,  1.1932,  1.4172, -1.6833, -0.9189],\n",
      "         [-0.4445, -0.7816, -0.7320,  0.6942, -1.0607],\n",
      "         [ 0.0152,  0.5836, -1.4120, -0.0102, -1.0351]],\n",
      "\n",
      "        [[-2.3748, -0.1757,  0.2257, -0.0353, -0.7651],\n",
      "         [-0.8013, -0.1986, -0.3613, -0.6489,  1.8461],\n",
      "         [ 0.5380, -0.1875,  1.5581,  0.5674, -0.4032]],\n",
      "\n",
      "        [[ 0.5873,  0.1640,  0.2686,  0.5162,  1.0610],\n",
      "         [-1.2765, -1.5210, -1.2313,  1.3992,  0.0446],\n",
      "         [-0.0848, -2.4348,  2.9683, -1.1811,  0.8426]]])\n",
      "\n",
      "[TORCH] dE/dH:\n",
      "tensor([[[-0.3532,  0.2270,  0.1020,  1.6220,  0.2858],\n",
      "         [-0.2570,  0.9653, -0.4660, -0.0568,  0.4982],\n",
      "         [ 1.8082, -1.1110,  0.5169, -1.6098,  0.7948]],\n",
      "\n",
      "        [[ 0.3515,  1.1685, -0.0134,  1.5991,  1.3022],\n",
      "         [ 1.5997, -1.4602,  1.0215, -0.6152, -0.1429],\n",
      "         [ 1.6330,  0.7056, -3.0349, -2.5106,  1.6060]],\n",
      "\n",
      "        [[-1.1461,  1.1932,  1.4172, -1.6833, -0.9189],\n",
      "         [-0.4445, -0.7816, -0.7320,  0.6942, -1.0607],\n",
      "         [ 0.0152,  0.5836, -1.4120, -0.0102, -1.0351]],\n",
      "\n",
      "        [[-2.3748, -0.1757,  0.2257, -0.0353, -0.7651],\n",
      "         [-0.8013, -0.1986, -0.3613, -0.6489,  1.8461],\n",
      "         [ 0.5380, -0.1875,  1.5581,  0.5674, -0.4032]],\n",
      "\n",
      "        [[ 0.5873,  0.1640,  0.2686,  0.5162,  1.0610],\n",
      "         [-1.2765, -1.5210, -1.2313,  1.3992,  0.0446],\n",
      "         [-0.0848, -2.4348,  2.9683, -1.1811,  0.8426]]])\n",
      "\n",
      "X_grad=tensor([[[-0.0587, -0.0625, -0.1416,  0.3536,  0.6034,  0.2621],\n",
      "         [ 0.1891, -0.5170,  0.3298, -0.2405,  0.2868, -0.0191],\n",
      "         [-0.6050,  0.7554, -0.1204, -0.7507, -0.8383, -0.5628]],\n",
      "\n",
      "        [[-0.3081,  0.1907, -0.1563, -0.3924,  0.6229,  0.0269],\n",
      "         [-0.4405,  1.0946, -0.3359, -0.0228, -1.2445, -0.2509],\n",
      "         [-0.4884, -0.8881,  0.8566, -0.2015,  0.7514, -0.0873]],\n",
      "\n",
      "        [[ 0.5259,  0.0617, -0.2149, -0.4705, -0.3501, -0.1669],\n",
      "         [ 0.1064, -0.0082, -0.0163,  1.0592,  0.0535,  0.5087],\n",
      "         [ 0.2529, -0.4362, -0.0118,  0.4334,  0.1967,  0.0393]],\n",
      "\n",
      "        [[ 0.6445, -0.5457,  0.1768,  0.3015, -0.1729,  0.0907],\n",
      "         [-0.1865, -0.3915,  0.6253, -0.3902,  0.2351,  0.0089],\n",
      "         [ 0.0497,  0.6699, -0.6218, -0.1079, -0.4066, -0.1370]],\n",
      "\n",
      "        [[-0.4906,  0.3743, -0.0985, -0.0765,  0.2131,  0.0301],\n",
      "         [ 0.0369, -0.8612,  0.6843,  0.3830,  0.4538,  0.2078],\n",
      "         [-0.1261,  0.8142, -0.0164, -0.7988, -1.0925, -0.2748]]])\n"
     ]
    }
   ],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "rnn_out.retain_grad()\n",
    "rnn_out.backward(de_dy)\n",
    "\n",
    "print(f'[TORCH] dE/dWih:\\n{rnn.weight_ih_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dWhh:\\n{rnn.weight_hh_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[TORCH] dE/dH:\\n{rnn_out.grad}\\n')\n",
    "print(f'X_grad={x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fourth-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dWih:\n",
      "[[ 1.7140859   1.9824136   1.0660958   2.1548996   6.6352262  -0.78076553]\n",
      " [-0.8973262  -2.4813638  -0.68342984 -0.6730628  -2.3434696  -0.42708877]\n",
      " [-1.0497817   1.8254004  -0.4997068  -8.159294    6.276383   -5.4964433 ]\n",
      " [-8.079005   -2.2785258  -0.03523949 -3.4041152   1.2095047   5.944037  ]\n",
      " [-0.5815874  -1.4934934  -2.9621274  -0.883028    4.2656655  -2.3443449 ]]\n",
      "\n",
      "[CUSTOM] dE/dWhh:\n",
      "[[-0.25431004 -0.67553383 -0.5481536  -1.4884566   1.5175735 ]\n",
      " [ 1.0793998  -0.6977873  -0.09316816  2.3760862   1.4980339 ]\n",
      " [-0.05392415 -0.70052     1.0784665  -2.417494   -4.7499957 ]\n",
      " [ 0.46451026  1.6019883   0.5750852   1.5038161  -1.7591625 ]\n",
      " [-0.50958115 -0.6169616  -0.04024398 -0.9756547   0.5781071 ]]\n",
      "\n",
      "RNN layer gradient check dEdW_in:  True\n",
      "RNN layer gradient check dEdW_hh:  True\n"
     ]
    }
   ],
   "source": [
    "dEdW_in, dEdW_hh, _ = rnn_.backward(x_, rnn_out_, de_dy_)\n",
    "\n",
    "print(f'[CUSTOM] dE/dWih:\\n{dEdW_in}\\n')\n",
    "print(f'[CUSTOM] dE/dWhh:\\n{dEdW_hh}\\n')\n",
    "\n",
    "print('RNN layer gradient check dEdW_in: ', np.isclose(rnn.weight_ih_l0.grad.numpy(), dEdW_in).all())\n",
    "print('RNN layer gradient check dEdW_hh: ', np.isclose(rnn.weight_hh_l0.grad.numpy(), dEdW_hh).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-rebate",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "private-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer forward check:  True\n",
      "LSTM layer forward check last hidden:  True\n",
      "LSTM layer forward check last c_n:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 32\n",
    "emb_dim = 300\n",
    "seq_len = 32\n",
    "hidden_dim = 200\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "lstm = nn.LSTM(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "lstm_ = LSTMLayer(emb_dim, hidden_dim, use_bias=False)\n",
    "wih = lstm.weight_ih_l0.detach().numpy()\n",
    "whh = lstm.weight_hh_l0.detach().numpy()\n",
    "\n",
    "lstm_.input_weights[0,:,:] = wih[0:hidden_dim, :]\n",
    "lstm_.input_weights[1,:,:] = wih[hidden_dim: 2*hidden_dim, :]\n",
    "lstm_.input_weights[2,:,:] = wih[2*hidden_dim: 3*hidden_dim, :]\n",
    "lstm_.input_weights[3,:,:] = wih[3*hidden_dim: 4*hidden_dim, :]\n",
    "\n",
    "lstm_.hidden_weights[0,:,:] = whh[0:hidden_dim, :]\n",
    "lstm_.hidden_weights[1,:,:] = whh[hidden_dim: 2*hidden_dim, :]\n",
    "lstm_.hidden_weights[2,:,:] = whh[2*hidden_dim: 3*hidden_dim, :]\n",
    "lstm_.hidden_weights[3,:,:] = whh[3*hidden_dim: 4*hidden_dim, :]\n",
    "\n",
    "\n",
    "lstm_out, h_n = lstm(x)\n",
    "lstm_out_, h_n_, c_n_ = lstm_.forward(x_)\n",
    "lstm_out__ = lstm_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('LSTM layer forward check: ', np.isclose(lstm_out.detach().numpy(), lstm_out__, atol=1e-3).all())\n",
    "print('LSTM layer forward check last hidden: ', np.isclose(h_n[0].detach().numpy(), h_n_, atol=1e-3).all())\n",
    "print('LSTM layer forward check last c_n: ', np.isclose(h_n[1].detach().numpy(), c_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "painful-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "x.retain_grad()\n",
    "lstm_out.backward(de_dy)\n",
    "\n",
    "dEdW_in, dEdW_hh, a, X_grad = lstm_.backward(x_, de_dy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "honest-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer gradient check dEdW_in:  True\n",
      "LSTM layer gradient check dEdW_hh:  True\n",
      "LSTM layer gradient check dEdX:  True\n"
     ]
    }
   ],
   "source": [
    "print('LSTM layer gradient check dEdW_in: ', np.isclose(lstm.weight_ih_l0.grad.numpy(), dEdW_in.reshape(4*hidden_dim,emb_dim), atol=1e-3).all())\n",
    "print('LSTM layer gradient check dEdW_hh: ', np.isclose(lstm.weight_hh_l0.grad.numpy(), dEdW_hh.reshape(4*hidden_dim,hidden_dim), atol=1e-3).all())\n",
    "print('LSTM layer gradient check dEdX: ', np.isclose(x.grad.numpy(), X_grad, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-collaboration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "classical-tissue",
   "metadata": {},
   "source": [
    "## Pokusaj pravljenja torch modela rnn, fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "turkish-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "important-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 3\n",
    "hidden_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "favorite-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.RNN(emb_dim, hidden_dim, bias=False, batch_first=True),\n",
    "    torch.nn.Linear(hidden_dim, 5, bias=False)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-persian",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "inclusive-accommodation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU layer forward check:  True\n",
      "GRU layer forward check last hidden:  True\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 1\n",
    "hidden_dim = 8\n",
    "\n",
    "#N = 20\n",
    "#emb_dim = 40\n",
    "#seq_len = 32\n",
    "#hidden_dim = 200\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "gru = nn.GRU(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "gru_ = GRULayer(emb_dim, hidden_dim, use_bias=False)\n",
    "wih = gru.weight_ih_l0.detach().numpy()\n",
    "whh = gru.weight_hh_l0.detach().numpy()\n",
    "\n",
    "gru_.input_weights[0,:,:] = wih[0:hidden_dim, :]\n",
    "gru_.input_weights[1,:,:] = wih[hidden_dim: 2*hidden_dim, :]\n",
    "gru_.input_weights[2,:,:] = wih[2*hidden_dim: 3*hidden_dim, :]\n",
    "\n",
    "gru_.hidden_weights[0,:,:] = whh[0:hidden_dim, :]\n",
    "gru_.hidden_weights[1,:,:] = whh[hidden_dim: 2*hidden_dim, :]\n",
    "gru_.hidden_weights[2,:,:] = whh[2*hidden_dim: 3*hidden_dim, :]\n",
    "\n",
    "gru_out, h_n = gru(x)\n",
    "gru_out_, h_n_ = gru_.forward(x_)\n",
    "gru_out__ = gru_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('GRU layer forward check: ', np.isclose(gru_out.detach().numpy(), gru_out__, atol=1e-3).all())\n",
    "print('GRU layer forward check last hidden: ', np.isclose(h_n.detach().numpy(), h_n_, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "loving-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "x.retain_grad()\n",
    "gru_out.backward(de_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "utility-franchise",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,8) and (6,8) not aligned: 8 (dim 1) != 6 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-611c15d37c99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdEdW_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdEdW_hh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgru_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mde_dy_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-11b47570a367>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, X_in, dEdY)\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_calculated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m             \u001b[0mX_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgates_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[0mh_t_T\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,8) and (6,8) not aligned: 8 (dim 1) != 6 (dim 0)"
     ]
    }
   ],
   "source": [
    "dEdW_in, dEdW_hh, a, X_grad = gru_.backward(x_, de_dy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1,2],[2,2]],[[3,2],[1,2]],[[1,2],[2,2]]])\n",
    "np.prod(np.array(a.shape[0:len(a.shape)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[[1]]])\n",
    "b = np.array([[2]])\n",
    "\n",
    "c = [a, b]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1, 2],[2, 3],[3, 4],[4, 5]], [[1, 2],[2, 3],[3, 4],[4, 5]]])\n",
    "#a = 2,4,2\n",
    "b = np.array([[1, 2, 4],[2, 3, 5],[3, 4, 4],[4, 5, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10,20,5,3,30)\n",
    "b = np.random.rand(40, 30)\n",
    "\n",
    "e = np.einsum('b...i,ih->b...h', a, b.T)\n",
    "t = np.tensordot(a, b.T, axes=(-1, 0))\n",
    "print('GRU layer forward check: ', np.isclose(e,t, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10,20,30)\n",
    "b = np.random.rand(10,20,40)\n",
    "\n",
    "e = np.einsum('b...o,b...i->oi', a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
