{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "valuable-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "obvious-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(object):\n",
    "    \n",
    "    def forward(self, X_in):\n",
    "        return np.tanh(X_in)\n",
    "    \n",
    "    def backward(self, X_in):\n",
    "        #dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(X_in))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "accompanied-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu(object):\n",
    "    \n",
    "    def forward(self, X_in):\n",
    "        return np.maximum(X_in, 0)\n",
    "    \n",
    "    def backward(self, X_in):\n",
    "        dYdX = (X_in > 0)  \n",
    "        return dYdX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "retained-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros((hidden_dim))\n",
    "        \n",
    "    def forward(self, X_in):        \n",
    "        #treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        #treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "        \n",
    "        #u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim)) \n",
    "        \n",
    "        for i in range(self.seq_len):\n",
    "            \n",
    "            input_part = np.einsum('ij,jk->ik', X_in[:,i,:], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jj->ij', H[:,i,:], self.hidden_weights.T)\n",
    "            \n",
    "            H[:,i+1,:] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "       \n",
    "        return H, H[:,self.seq_len,:]\n",
    "    \n",
    "    def book_forward(self, X_in):\n",
    "        \n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim)) \n",
    "        \n",
    "        for i in range(self.seq_len):\n",
    "            #ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki',self.input_weights, X_in[:,i,:].T)\n",
    "            hidden_part = np.einsum('ii,ij->ji',self.hidden_weights, H[:,i,:].T)\n",
    "            \n",
    "            H[:,i+1,:] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "       \n",
    "        return H, H[:,self.seq_len,:]\n",
    "    \n",
    "    def backward(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        \n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "        \n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:,self.seq_len,:] = dEdY[:,self.seq_len - 1,:]\n",
    "        \n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            \n",
    "            activation_backward = self.activation.backward(H[:,i,:]).reshape(self.batch_size, self.hidden_dim, 1)\n",
    "        \n",
    "            a = activation_backward * (np.einsum('bh,bi->bhi', H_grad[:,i,:], X[:,i-1,:]))\n",
    "            b = activation_backward * (np.einsum('bh,bk->bhk', H_grad[:,i,:], H[:,i-1,:]))\n",
    "            \n",
    "            dEdW_in += np.sum(activation_backward * (np.einsum('bh,bi->bhi', H_grad[:,i,:], X[:,i-1,:])), axis=0)\n",
    "            dEdW_hh += np.sum(activation_backward * (np.einsum('bh,bk->bhk', H_grad[:,i,:], H[:,i-1,:])), axis=0)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:,i,:]) * H_grad[:,i,:], axis=(0))\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if i > 1:\n",
    "                H_grad[:,i-1,:] = np.einsum('bh,hh->bh', H_grad[:,i,:], self.hidden_weights) * self.activation.backward(H[:,i,:]) + dEdY[:,i-2,:]\n",
    "            else:\n",
    "                H_grad[:,i-1,:] = np.einsum('bh,hh->bh', H_grad[:,i,:], self.hidden_weights) * self.activation.backward(H[:,i,:])\n",
    "        \n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "            \n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        \n",
    "        print(f'self.bias={self.bias}')\n",
    "        \n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "        \n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:,self.seq_len,:] = dEdY[:,self.seq_len - 1,:]\n",
    "        \n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            \n",
    "            for k in range (self.batch_size):\n",
    "                act_grad = np.diag(self.activation.backward(H[k,i,:]))\n",
    "                h_grad = H_grad[k,i,:].reshape(self.hidden_dim, 1)\n",
    "                \n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k,i-1,:].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k,i-1,:].reshape(1, self.hidden_dim)))\n",
    "            \n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:,i,:]) * H_grad[:,i,:], axis=(0))\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if i > 1:\n",
    "                H_grad[:,i-1,:] = np.einsum('bh,hh->bh', H_grad[:,i,:], self.hidden_weights) * self.activation.backward(H[:,i,:]) + dEdY[:,i-2,:]\n",
    "            else:\n",
    "                H_grad[:,i-1,:] = np.einsum('bh,hh->bh', H_grad[:,i,:], self.hidden_weights) * self.activation.backward(H[:,i,:])\n",
    "        \n",
    "        return dEdW_in, dEdW_hh, dEdB_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "endangered-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn forward checker\n",
    "\n",
    "rnn = RnnLayer(4, 5, 3, 2, use_bias=False)\n",
    "rnn1 = RnnLayer(4, 5, 3, 2, use_bias=False)\n",
    "#input dim 4\n",
    "#hidden dim 5\n",
    "#batch 2\n",
    "#timestamps 3\n",
    "rnn1.input_weights = rnn.input_weights\n",
    "rnn1.hidden_weights = rnn.hidden_weights\n",
    "\n",
    "X_in = np.array([[[1,2,1,3],[2,2,3,1],[0,2,3,1]],[[1,3,4,3],[1,2,1,1],[1,0,1,2]]])\n",
    "H, last = rnn.forward(X_in)\n",
    "H1, last1 = rnn1.forward(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "tight-jesus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.bias=[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "dEdY = np.array([[[ 0.34545989,  0.07336296, -0.16346513, -0.06904482,\n",
    "          0.0458759 ],\n",
    "        [ 0.37271336,  0.07915059, -0.17636096, -0.07449179,\n",
    "          0.04949507],\n",
    "        [ 0.35166208,  0.07468007, -0.16639989, -0.07028441,\n",
    "          0.04669953]],\n",
    "\n",
    "       [[ 0.36616935,  0.07776088, -0.17326446, -0.07318388,\n",
    "          0.04862605],\n",
    "        [ 0.33954613,  0.07210709, -0.16066685, -0.06786287,\n",
    "          0.04509058],\n",
    "        [ 0.35872758,  0.07618053, -0.16974315, -0.07169654,\n",
    "          0.04763781]]])\n",
    "\n",
    "Win, Wh, Bin= rnn.backward_checker(X_in, H, dEdY)\n",
    "Win1, Wh1, Bin1 = rnn1.backward(X_in, H1, dEdY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "small-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh1=[[-0.09290856 -0.39063081  1.01463126  1.14421039 -0.36029288]\n",
      " [-0.01613264 -0.07558906  0.19758189  0.22148378 -0.06832331]\n",
      " [ 0.02526107  0.1142049  -0.3143839  -0.35383634  0.09568708]\n",
      " [ 0.01047346  0.0459219  -0.11493675 -0.13019808  0.0440292 ]\n",
      " [-0.00809848 -0.03703844  0.0914215   0.10319943 -0.03601933]]\n",
      "Wh=[[-0.09290856 -0.39063081  1.01463126  1.14421039 -0.36029288]\n",
      " [-0.01613264 -0.07558906  0.19758189  0.22148378 -0.06832331]\n",
      " [ 0.02526107  0.1142049  -0.3143839  -0.35383634  0.09568708]\n",
      " [ 0.01047346  0.0459219  -0.11493675 -0.13019808  0.0440292 ]\n",
      " [-0.00809848 -0.03703844  0.0914215   0.10319943 -0.03601933]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Wh1={Wh1}')\n",
    "print(f'Wh={Wh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "awful-mountain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(3, 3, 4)\n",
      "H_grad.shape=(3, 4, 3)\n",
      "X_e1=[[[3 9 9 6]\n",
      "  [1 3 3 2]\n",
      "  [3 9 9 6]]\n",
      "\n",
      " [[6 9 6 3]\n",
      "  [2 3 2 1]\n",
      "  [6 9 6 3]]\n",
      "\n",
      " [[2 2 4 0]\n",
      "  [1 1 2 0]\n",
      "  [2 2 4 0]]]\n",
      "back=[[0.00986604 0.41997434 0.00986604]\n",
      " [0.00986604 0.41997434 0.00986604]\n",
      " [0.07065082 0.41997434 0.07065082]]\n"
     ]
    }
   ],
   "source": [
    "#dEdW_in += np.einsum('bh,bi->hi', H_grad[:,i,:], X[:,i-1,:])\n",
    "#dEdW_hh += np.einsum('bh,bk->hk', H_grad[:,i,:], H[:,i-1,:])\n",
    "\n",
    "X = np.array([[[1,3,3,2], [1,3,1,3],[2,2,1,1]], [[2,3,2,1],[2,3,1,3],[0,2,1,2]], [[1,1,2,0],[2,2,1,2], [1,2,1,2]]])\n",
    "print(f'X.shape={X.shape}')\n",
    "\n",
    "H_grad = np.array([[[3,3,2], [3,1,3],[2,1,1],[2,1,1]], [[3,2,1],[3,1,3],[0,1,2],[2,1,1]], [[1,2,0],[2,1,2], [2,1,2],[2,1,1]]])\n",
    "print(f'H_grad.shape={H_grad.shape}')\n",
    "\n",
    "X_e1 = np.einsum('bh,bi->bhi', H_grad[:,1,:], X[:,0,:])\n",
    "X_e2 = np.einsum('bh,bi->hi', H_grad[:,1,:], X[:,0,:])\n",
    "print(f'X_e1={X_e1}')\n",
    "\n",
    "act = Tanh()\n",
    "\n",
    "back = act.backward(H_grad[:,1,:])\n",
    "print(f'back={back}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "scenic-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 3 2]\n",
      " [1 3 1 3]\n",
      " [2 2 1 1]]\n",
      "[[0.00986604 0.         0.        ]\n",
      " [0.         0.41997434 0.        ]\n",
      " [0.         0.         0.00986604]]\n",
      "[[0.00986604 0.02959811 0.02959811 0.01973207]\n",
      " [0.41997434 1.25992302 0.41997434 1.25992302]\n",
      " [0.01973207 0.01973207 0.00986604 0.00986604]]\n",
      "[[0.00986604]\n",
      " [0.41997434]\n",
      " [0.00986604]]\n",
      "[[0.00986604 0.02959811 0.02959811 0.01973207]\n",
      " [0.41997434 1.25992302 0.41997434 1.25992302]\n",
      " [0.01973207 0.01973207 0.00986604 0.00986604]]\n",
      "[[0.00986604]\n",
      " [0.41997434]\n",
      " [0.00986604]]\n",
      "[[0.00986604 0.02959811 0.02959811 0.01973207]\n",
      " [0.41997434 1.25992302 0.41997434 1.25992302]\n",
      " [0.01973207 0.01973207 0.00986604 0.00986604]]\n",
      "[[0.00986604 0.02959811 0.02959811 0.01973207]\n",
      " [0.41997434 1.25992302 0.41997434 1.25992302]\n",
      " [0.01973207 0.01973207 0.00986604 0.00986604]]\n"
     ]
    }
   ],
   "source": [
    "n = X[0,:,:]\n",
    "print(n)\n",
    "a = back[0]\n",
    "print(np.diag(a))\n",
    "print(np.dot(np.diag(a),n))\n",
    "\n",
    "print(a.reshape(3,1))\n",
    "print(a.reshape(3,1) * n)\n",
    "print(br[0,:,:])\n",
    "\n",
    "print(br[0,:,:] * n)\n",
    "br1 = br[0,:,:]*X[0,:,:]\n",
    "print(br1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
