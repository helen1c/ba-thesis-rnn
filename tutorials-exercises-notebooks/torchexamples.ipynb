{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "treated-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numerous-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(3, 4, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gothic-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputt = torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liked-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0445,  0.4361, -0.0762],\n",
      "        [-0.3798,  0.1714,  0.2562],\n",
      "        [ 0.1661,  0.3013,  0.2487],\n",
      "        [ 0.3682, -0.0062,  0.2577]], requires_grad=True)\n",
      "tensor([[-0.7067, -0.3054,  0.5206],\n",
      "        [ 1.0049,  1.0786,  0.8055],\n",
      "        [ 0.2857, -0.0968, -1.2853],\n",
      "        [ 0.5373,  0.2709, -0.5338],\n",
      "        [ 1.0436,  0.1004,  0.1597]])\n",
      "tensor([[-0.2043,  0.3494, -0.0799, -0.1241],\n",
      "        [ 0.4536,  0.0096,  0.6922,  0.5709],\n",
      "        [ 0.0685, -0.4543, -0.3013, -0.2255],\n",
      "        [ 0.1827, -0.2944,  0.0381,  0.0586],\n",
      "        [ 0.0780, -0.3382,  0.2433,  0.4248]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(m.weight)\n",
    "print(inputt)\n",
    "print(m(inputt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recovered-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        exps = np.exp(x_in-np.max(x_in, axis=-1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "class Tanh(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.tanh(x_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        # dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(x_in)) ** 2\n",
    "\n",
    "\n",
    "class ReLu(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.maximum(x_in, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        return x_in > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "selected-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        # treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "\n",
    "        # u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            input_part = np.einsum('ij,jk->ik', x_in[:, i, :], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', H[:, i, :], self.hidden_weights.T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def book_forward(self, x_in):\n",
    "\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            # ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki', self.input_weights, x_in[:, i, :].T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', self.hidden_weights, H[:, i, :].T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def backward(self, x, h, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:, self.seq_len - 1, :]\n",
    "\n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            activation_backward = self.activation.backward(h[:, i, :]).reshape(self.batch_size, self.hidden_dim, 1)\n",
    "\n",
    "            dEdW_in += np.sum(activation_backward * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i - 1, :])), axis=0)\n",
    "            dEdW_hh += np.sum(activation_backward * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i - 1, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(h[:, i, :]) * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * self.activation.backward(\n",
    "                    h[:, i, :]) + dEdY[:, i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(h[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        print(f'self.bias={self.bias}')\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:, self.seq_len - 1, :]\n",
    "\n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "\n",
    "            for k in range(self.batch_size):\n",
    "                act_grad = np.diag(self.activation.backward(H[k, i, :]))\n",
    "                h_grad = H_grad[k, i, :].reshape(self.hidden_dim, 1)\n",
    "\n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k, i - 1, :].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k, i - 1, :].reshape(1, self.hidden_dim)))\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:, i, :]) * H_grad[:, i, :], axis=(0))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(H[:, i, :]) + dEdY[:,\n",
    "                                                                                                              i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(H[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "better-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnmine = RnnLayer(3, 2, 3, 5, use_bias=False)\n",
    "rnntorch = nn.RNN(3, 2, bias=False, batch_first=True)\n",
    "rnnmine.input_weights = rnntorch.weight_ih_l0.detach().numpy()\n",
    "rnnmine.hidden_weights = rnntorch.weight_hh_l0.detach().numpy()\n",
    "\n",
    "x = torch.randn(5, 3, 3)\n",
    "x_m = x.detach().numpy()\n",
    "h0 = torch.zeros((1,5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "approved-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputtorch, _ = rnntorch(x, h0)\n",
    "outmine, _ = rnnmine.forward(x_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anticipated-family",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0141, -0.7907],\n",
      "         [-0.7024, -0.2190],\n",
      "         [ 0.8083,  0.0122]],\n",
      "\n",
      "        [[-0.1536, -0.7138],\n",
      "         [-0.1847,  0.6211],\n",
      "         [-0.4310, -0.8088]],\n",
      "\n",
      "        [[-0.2234,  0.1672],\n",
      "         [-0.8791, -0.9268],\n",
      "         [ 0.9770,  0.6179]],\n",
      "\n",
      "        [[ 0.9083,  0.7321],\n",
      "         [-0.9035, -0.9906],\n",
      "         [ 0.2919, -0.8567]],\n",
      "\n",
      "        [[ 0.8316,  0.6995],\n",
      "         [-0.9881, -0.9910],\n",
      "         [ 0.9049,  0.3673]]], dtype=torch.float64)\n",
      "tensor([[[ 0.0141, -0.7907],\n",
      "         [-0.7024, -0.2190],\n",
      "         [ 0.8083,  0.0122]],\n",
      "\n",
      "        [[-0.1536, -0.7138],\n",
      "         [-0.1847,  0.6211],\n",
      "         [-0.4310, -0.8088]],\n",
      "\n",
      "        [[-0.2234,  0.1672],\n",
      "         [-0.8791, -0.9268],\n",
      "         [ 0.9770,  0.6179]],\n",
      "\n",
      "        [[ 0.9083,  0.7321],\n",
      "         [-0.9035, -0.9906],\n",
      "         [ 0.2919, -0.8567]],\n",
      "\n",
      "        [[ 0.8316,  0.6995],\n",
      "         [-0.9881, -0.9910],\n",
      "         [ 0.9049,  0.3673]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "om = torch.from_numpy(outmine[:,1:,:])\n",
    "print(om)\n",
    "print(outputtorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
