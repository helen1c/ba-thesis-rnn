{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enhanced-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "practical-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        exps = np.exp(x_in-np.max(x_in, axis=-1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "class Tanh(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.tanh(x_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        # dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(x_in)) ** 2\n",
    "\n",
    "\n",
    "class ReLu(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.maximum(x_in, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        return x_in > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "native-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        sq = np.sqrt(1. / input_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = np.random.uniform(-sq, sq, (output_dim, input_dim))\n",
    "        if use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, output_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(output_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        return np.tensordot(x_in, self.weights.T, axes=((-1), 0)) + self.bias\n",
    "\n",
    "    def backward(self, de_dy, x_in):\n",
    "        # de_dw = de_dy * dYdW = de_dy * X\n",
    "        # dEdb = de_dy * dYdb = de_dy\n",
    "        # dEdX = de_dy * dYdX = de_dy * W\n",
    "        axis = tuple(range(len(x_in.shape) - 1))\n",
    "        de_dw = np.tensordot(de_dy, x_in, axes=(axis, axis))\n",
    "        de_db = np.sum(de_dy, axis=axis)\n",
    "        de_dx = np.tensordot(de_dy, self.weights, axes=(-1, 0))\n",
    "\n",
    "        return de_dx, de_dw, de_db\n",
    "\n",
    "    def refresh(self, de_dw, de_db, learning_rate):\n",
    "        self.weights = self.weights - learning_rate * de_dw\n",
    "        if self.use_bias:\n",
    "            self.bias = self.bias - learning_rate * de_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accepted-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, y, o):\n",
    "        self.y_pred = Softmax.forward(o)\n",
    "        return np.sum(-y * np.log(self.y_pred + 1e-15))/y.shape[0]\n",
    "\n",
    "    def backward(self, y):\n",
    "        return self.y_pred - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacterial-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        # treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "\n",
    "        # u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            input_part = np.einsum('ij,jk->ik', x_in[:, i, :], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', H[:, i, :], self.hidden_weights.T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def book_forward(self, x_in):\n",
    "\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            # ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki', self.input_weights, x_in[:, i, :].T)\n",
    "            hidden_part = np.einsum('ii,ij->ji', self.hidden_weights, H[:, i, :].T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def backward(self, x, h, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:, self.seq_len - 1, :]\n",
    "\n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            activation_backward = self.activation.backward(h[:, i, :]).reshape(self.batch_size, self.hidden_dim, 1)\n",
    "\n",
    "            dEdW_in += np.sum(activation_backward * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i - 1, :])), axis=0)\n",
    "            dEdW_hh += np.sum(activation_backward * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i - 1, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(h[:, i, :]) * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * self.activation.backward(\n",
    "                    h[:, i, :]) + dEdY[:, i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(h[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        print(f'self.bias={self.bias}')\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:, self.seq_len - 1, :]\n",
    "\n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "\n",
    "            for k in range(self.batch_size):\n",
    "                act_grad = np.diag(self.activation.backward(H[k, i, :]))\n",
    "                h_grad = H_grad[k, i, :].reshape(self.hidden_dim, 1)\n",
    "\n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k, i - 1, :].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k, i - 1, :].reshape(1, self.hidden_dim)))\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:, i, :]) * H_grad[:, i, :], axis=(0))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(H[:, i, :]) + dEdY[:,\n",
    "                                                                                                              i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(H[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compliant-confirmation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "text = ['hey how are you', 'good i am fine', 'have a nice day']\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "# Finding the length of the longest string in our data\n",
    "maxlen = len(max(text, key=len))\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of\n",
    "# the sentence matches the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "    while len(text[i]) < maxlen:\n",
    "        text[i] += ' '\n",
    "\n",
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "\n",
    "    # Remove first character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "\n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cutting-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. epoha- loss: 39.21618111573924\n",
      "11. epoha- loss: 27.870875753443517\n",
      "21. epoha- loss: 15.46392187062306\n",
      "31. epoha- loss: 8.261012277107602\n",
      "41. epoha- loss: 7.313781381838427\n",
      "51. epoha- loss: 10.409192085110115\n",
      "61. epoha- loss: 10.098087795246919\n",
      "71. epoha- loss: 9.487554094467116\n",
      "81. epoha- loss: 15.487353052126139\n",
      "91. epoha- loss: 10.535370474529058\n",
      "101. epoha- loss: 11.430821348635357\n",
      "111. epoha- loss: 15.413992707142597\n",
      "121. epoha- loss: 15.908477178067502\n",
      "131. epoha- loss: 21.371321635449547\n",
      "141. epoha- loss: 21.75236055512248\n",
      "151. epoha- loss: 23.265902117964885\n",
      "161. epoha- loss: 21.251904885385425\n",
      "171. epoha- loss: 18.71500882366192\n",
      "181. epoha- loss: 17.83290787677396\n",
      "191. epoha- loss: 17.694379709557325\n",
      "201. epoha- loss: 17.724944514475027\n",
      "211. epoha- loss: 17.197783612253172\n",
      "221. epoha- loss: 15.888147185795916\n",
      "231. epoha- loss: 15.616534412315545\n",
      "241. epoha- loss: 15.23352146108895\n",
      "251. epoha- loss: 14.038702369586247\n",
      "261. epoha- loss: 13.13454369377692\n",
      "271. epoha- loss: 16.08318249723549\n",
      "281. epoha- loss: 18.61000771926543\n",
      "291. epoha- loss: 29.24840399550851\n",
      "301. epoha- loss: 18.7943330807989\n",
      "311. epoha- loss: 18.04300839557997\n",
      "321. epoha- loss: 32.67180127258598\n",
      "331. epoha- loss: 23.063121647100118\n",
      "341. epoha- loss: 19.683389466379655\n",
      "351. epoha- loss: 23.908081596196478\n",
      "361. epoha- loss: 24.145709194721906\n",
      "371. epoha- loss: 23.135679664116726\n",
      "381. epoha- loss: 26.274396460999444\n",
      "391. epoha- loss: 20.070722249576676\n"
     ]
    }
   ],
   "source": [
    "X = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "T = one_hot_encode(target_seq, dict_size, seq_len, batch_size)\n",
    "\n",
    "hidden_dim = 30\n",
    "\n",
    "rnn = RnnLayer(dict_size, hidden_dim, seq_len, batch_size)\n",
    "dense = DenseLayer(hidden_dim, dict_size)\n",
    "clos = CrossEntropyLoss()\n",
    "n_epochs = 400\n",
    "learning_rate = 0.03\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    H, _ = rnn.forward(X)\n",
    "    out = dense.forward(H[:, 1:, :])\n",
    "    loss = clos.forward(T, out)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i + 1}. epoha- loss: {loss}')\n",
    "\n",
    "    dEdY = clos.backward(T)\n",
    "\n",
    "    de_dx, de_dw, de_db_d = dense.backward(dEdY, H[:, 1:, :])\n",
    "    dEdW_in, dEdW_hh, de_db_r = rnn.backward(X, H, de_dx)\n",
    "\n",
    "    dense.weights = dense.weights - learning_rate * de_dw\n",
    "\n",
    "    if dense.use_bias:\n",
    "        dense.bias = dense.bias - learning_rate * de_db_d\n",
    "    rnn.input_weights = rnn.input_weights - learning_rate * np.clip(dEdW_in, a_min=-1, a_max=1)\n",
    "    rnn.hidden_weights = rnn.hidden_weights - learning_rate * np.clip(dEdW_hh, a_min=-1, a_max=1)\n",
    "    if rnn.use_bias:\n",
    "        rnn.bias = rnn.bias - learning_rate * np.clip(de_db_r, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-theology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
