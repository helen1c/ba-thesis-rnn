{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "objective-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "marine-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the length of the longest string in our data\n",
    "maxlen = len(max(text, key=len))\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of\n",
    "# the sentence matches the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "      text[i] += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiac-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "  input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove first character for target sequence\n",
    "  target_seq.append(text[i][1:])\n",
    "  print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "documented-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "independent-heating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 3\n"
     ]
    }
   ],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "print(seq_len, batch_size)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "solid-israel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inner-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_seq = torch.from_numpy(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cosmetic-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "catholic-liberal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
       " tensor([[12., 15.,  7.,  4.,  0., 14.,  7.,  6.,  9., 12.,  7., 15.,  0.,  8.],\n",
       "         [ 0.,  0., 10.,  7.,  1.,  7.,  6., 11.,  7.,  3.,  1.,  5., 12.,  7.],\n",
       "         [ 6., 13., 12.,  7.,  6.,  7.,  5.,  1.,  2., 12.,  7., 10.,  6., 15.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "humanitarian-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "warming-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "major-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 1000\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accompanied-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000............. Loss: 2.8803\n",
      "Epoch: 2/1000............. Loss: 2.8751\n",
      "Epoch: 3/1000............. Loss: 2.8698\n",
      "Epoch: 4/1000............. Loss: 2.8646\n",
      "Epoch: 5/1000............. Loss: 2.8593\n",
      "Epoch: 6/1000............. Loss: 2.8541\n",
      "Epoch: 7/1000............. Loss: 2.8489\n",
      "Epoch: 8/1000............. Loss: 2.8436\n",
      "Epoch: 9/1000............. Loss: 2.8384\n",
      "Epoch: 10/1000............. Loss: 2.8331\n",
      "Epoch: 11/1000............. Loss: 2.8278\n",
      "Epoch: 12/1000............. Loss: 2.8225\n",
      "Epoch: 13/1000............. Loss: 2.8172\n",
      "Epoch: 14/1000............. Loss: 2.8118\n",
      "Epoch: 15/1000............. Loss: 2.8064\n",
      "Epoch: 16/1000............. Loss: 2.8010\n",
      "Epoch: 17/1000............. Loss: 2.7955\n",
      "Epoch: 18/1000............. Loss: 2.7900\n",
      "Epoch: 19/1000............. Loss: 2.7844\n",
      "Epoch: 20/1000............. Loss: 2.7788\n",
      "Epoch: 21/1000............. Loss: 2.7732\n",
      "Epoch: 22/1000............. Loss: 2.7674\n",
      "Epoch: 23/1000............. Loss: 2.7616\n",
      "Epoch: 24/1000............. Loss: 2.7558\n",
      "Epoch: 25/1000............. Loss: 2.7499\n",
      "Epoch: 26/1000............. Loss: 2.7440\n",
      "Epoch: 27/1000............. Loss: 2.7380\n",
      "Epoch: 28/1000............. Loss: 2.7319\n",
      "Epoch: 29/1000............. Loss: 2.7257\n",
      "Epoch: 30/1000............. Loss: 2.7196\n",
      "Epoch: 31/1000............. Loss: 2.7133\n",
      "Epoch: 32/1000............. Loss: 2.7070\n",
      "Epoch: 33/1000............. Loss: 2.7007\n",
      "Epoch: 34/1000............. Loss: 2.6943\n",
      "Epoch: 35/1000............. Loss: 2.6878\n",
      "Epoch: 36/1000............. Loss: 2.6814\n",
      "Epoch: 37/1000............. Loss: 2.6749\n",
      "Epoch: 38/1000............. Loss: 2.6684\n",
      "Epoch: 39/1000............. Loss: 2.6618\n",
      "Epoch: 40/1000............. Loss: 2.6553\n",
      "Epoch: 41/1000............. Loss: 2.6487\n",
      "Epoch: 42/1000............. Loss: 2.6422\n",
      "Epoch: 43/1000............. Loss: 2.6357\n",
      "Epoch: 44/1000............. Loss: 2.6293\n",
      "Epoch: 45/1000............. Loss: 2.6228\n",
      "Epoch: 46/1000............. Loss: 2.6165\n",
      "Epoch: 47/1000............. Loss: 2.6102\n",
      "Epoch: 48/1000............. Loss: 2.6040\n",
      "Epoch: 49/1000............. Loss: 2.5978\n",
      "Epoch: 50/1000............. Loss: 2.5918\n",
      "Epoch: 51/1000............. Loss: 2.5858\n",
      "Epoch: 52/1000............. Loss: 2.5800\n",
      "Epoch: 53/1000............. Loss: 2.5743\n",
      "Epoch: 54/1000............. Loss: 2.5687\n",
      "Epoch: 55/1000............. Loss: 2.5632\n",
      "Epoch: 56/1000............. Loss: 2.5579\n",
      "Epoch: 57/1000............. Loss: 2.5527\n",
      "Epoch: 58/1000............. Loss: 2.5477\n",
      "Epoch: 59/1000............. Loss: 2.5428\n",
      "Epoch: 60/1000............. Loss: 2.5380\n",
      "Epoch: 61/1000............. Loss: 2.5334\n",
      "Epoch: 62/1000............. Loss: 2.5289\n",
      "Epoch: 63/1000............. Loss: 2.5246\n",
      "Epoch: 64/1000............. Loss: 2.5204\n",
      "Epoch: 65/1000............. Loss: 2.5163\n",
      "Epoch: 66/1000............. Loss: 2.5123\n",
      "Epoch: 67/1000............. Loss: 2.5085\n",
      "Epoch: 68/1000............. Loss: 2.5048\n",
      "Epoch: 69/1000............. Loss: 2.5012\n",
      "Epoch: 70/1000............. Loss: 2.4977\n",
      "Epoch: 71/1000............. Loss: 2.4943\n",
      "Epoch: 72/1000............. Loss: 2.4910\n",
      "Epoch: 73/1000............. Loss: 2.4877\n",
      "Epoch: 74/1000............. Loss: 2.4846\n",
      "Epoch: 75/1000............. Loss: 2.4815\n",
      "Epoch: 76/1000............. Loss: 2.4785\n",
      "Epoch: 77/1000............. Loss: 2.4756\n",
      "Epoch: 78/1000............. Loss: 2.4727\n",
      "Epoch: 79/1000............. Loss: 2.4699\n",
      "Epoch: 80/1000............. Loss: 2.4671\n",
      "Epoch: 81/1000............. Loss: 2.4643\n",
      "Epoch: 82/1000............. Loss: 2.4616\n",
      "Epoch: 83/1000............. Loss: 2.4589\n",
      "Epoch: 84/1000............. Loss: 2.4563\n",
      "Epoch: 85/1000............. Loss: 2.4536\n",
      "Epoch: 86/1000............. Loss: 2.4510\n",
      "Epoch: 87/1000............. Loss: 2.4485\n",
      "Epoch: 88/1000............. Loss: 2.4459\n",
      "Epoch: 89/1000............. Loss: 2.4434\n",
      "Epoch: 90/1000............. Loss: 2.4408\n",
      "Epoch: 91/1000............. Loss: 2.4383\n",
      "Epoch: 92/1000............. Loss: 2.4358\n",
      "Epoch: 93/1000............. Loss: 2.4333\n",
      "Epoch: 94/1000............. Loss: 2.4308\n",
      "Epoch: 95/1000............. Loss: 2.4283\n",
      "Epoch: 96/1000............. Loss: 2.4258\n",
      "Epoch: 97/1000............. Loss: 2.4233\n",
      "Epoch: 98/1000............. Loss: 2.4208\n",
      "Epoch: 99/1000............. Loss: 2.4183\n",
      "Epoch: 100/1000............. Loss: 2.4159\n",
      "Epoch: 101/1000............. Loss: 2.4134\n",
      "Epoch: 102/1000............. Loss: 2.4109\n",
      "Epoch: 103/1000............. Loss: 2.4083\n",
      "Epoch: 104/1000............. Loss: 2.4058\n",
      "Epoch: 105/1000............. Loss: 2.4033\n",
      "Epoch: 106/1000............. Loss: 2.4007\n",
      "Epoch: 107/1000............. Loss: 2.3982\n",
      "Epoch: 108/1000............. Loss: 2.3956\n",
      "Epoch: 109/1000............. Loss: 2.3930\n",
      "Epoch: 110/1000............. Loss: 2.3904\n",
      "Epoch: 111/1000............. Loss: 2.3877\n",
      "Epoch: 112/1000............. Loss: 2.3850\n",
      "Epoch: 113/1000............. Loss: 2.3823\n",
      "Epoch: 114/1000............. Loss: 2.3796\n",
      "Epoch: 115/1000............. Loss: 2.3768\n",
      "Epoch: 116/1000............. Loss: 2.3740\n",
      "Epoch: 117/1000............. Loss: 2.3712\n",
      "Epoch: 118/1000............. Loss: 2.3684\n",
      "Epoch: 119/1000............. Loss: 2.3655\n",
      "Epoch: 120/1000............. Loss: 2.3626\n",
      "Epoch: 121/1000............. Loss: 2.3596\n",
      "Epoch: 122/1000............. Loss: 2.3566\n",
      "Epoch: 123/1000............. Loss: 2.3536\n",
      "Epoch: 124/1000............. Loss: 2.3505\n",
      "Epoch: 125/1000............. Loss: 2.3474\n",
      "Epoch: 126/1000............. Loss: 2.3443\n",
      "Epoch: 127/1000............. Loss: 2.3411\n",
      "Epoch: 128/1000............. Loss: 2.3379\n",
      "Epoch: 129/1000............. Loss: 2.3346\n",
      "Epoch: 130/1000............. Loss: 2.3313\n",
      "Epoch: 131/1000............. Loss: 2.3280\n",
      "Epoch: 132/1000............. Loss: 2.3246\n",
      "Epoch: 133/1000............. Loss: 2.3212\n",
      "Epoch: 134/1000............. Loss: 2.3177\n",
      "Epoch: 135/1000............. Loss: 2.3142\n",
      "Epoch: 136/1000............. Loss: 2.3106\n",
      "Epoch: 137/1000............. Loss: 2.3070\n",
      "Epoch: 138/1000............. Loss: 2.3033\n",
      "Epoch: 139/1000............. Loss: 2.2996\n",
      "Epoch: 140/1000............. Loss: 2.2958\n",
      "Epoch: 141/1000............. Loss: 2.2920\n",
      "Epoch: 142/1000............. Loss: 2.2881\n",
      "Epoch: 143/1000............. Loss: 2.2841\n",
      "Epoch: 144/1000............. Loss: 2.2801\n",
      "Epoch: 145/1000............. Loss: 2.2761\n",
      "Epoch: 146/1000............. Loss: 2.2720\n",
      "Epoch: 147/1000............. Loss: 2.2678\n",
      "Epoch: 148/1000............. Loss: 2.2636\n",
      "Epoch: 149/1000............. Loss: 2.2593\n",
      "Epoch: 150/1000............. Loss: 2.2550\n",
      "Epoch: 151/1000............. Loss: 2.2506\n",
      "Epoch: 152/1000............. Loss: 2.2461\n",
      "Epoch: 153/1000............. Loss: 2.2416\n",
      "Epoch: 154/1000............. Loss: 2.2370\n",
      "Epoch: 155/1000............. Loss: 2.2323\n",
      "Epoch: 156/1000............. Loss: 2.2276\n",
      "Epoch: 157/1000............. Loss: 2.2228\n",
      "Epoch: 158/1000............. Loss: 2.2180\n",
      "Epoch: 159/1000............. Loss: 2.2131\n",
      "Epoch: 160/1000............. Loss: 2.2081\n",
      "Epoch: 161/1000............. Loss: 2.2031\n",
      "Epoch: 162/1000............. Loss: 2.1979\n",
      "Epoch: 163/1000............. Loss: 2.1928\n",
      "Epoch: 164/1000............. Loss: 2.1875\n",
      "Epoch: 165/1000............. Loss: 2.1822\n",
      "Epoch: 166/1000............. Loss: 2.1768\n",
      "Epoch: 167/1000............. Loss: 2.1714\n",
      "Epoch: 168/1000............. Loss: 2.1659\n",
      "Epoch: 169/1000............. Loss: 2.1603\n",
      "Epoch: 170/1000............. Loss: 2.1547\n",
      "Epoch: 171/1000............. Loss: 2.1490\n",
      "Epoch: 172/1000............. Loss: 2.1432\n",
      "Epoch: 173/1000............. Loss: 2.1374\n",
      "Epoch: 174/1000............. Loss: 2.1315\n",
      "Epoch: 175/1000............. Loss: 2.1255\n",
      "Epoch: 176/1000............. Loss: 2.1195\n",
      "Epoch: 177/1000............. Loss: 2.1134\n",
      "Epoch: 178/1000............. Loss: 2.1072\n",
      "Epoch: 179/1000............. Loss: 2.1010\n",
      "Epoch: 180/1000............. Loss: 2.0947\n",
      "Epoch: 181/1000............. Loss: 2.0884\n",
      "Epoch: 182/1000............. Loss: 2.0820\n",
      "Epoch: 183/1000............. Loss: 2.0756\n",
      "Epoch: 184/1000............. Loss: 2.0691\n",
      "Epoch: 185/1000............. Loss: 2.0625\n",
      "Epoch: 186/1000............. Loss: 2.0559\n",
      "Epoch: 187/1000............. Loss: 2.0493\n",
      "Epoch: 188/1000............. Loss: 2.0426\n",
      "Epoch: 189/1000............. Loss: 2.0358\n",
      "Epoch: 190/1000............. Loss: 2.0290\n",
      "Epoch: 191/1000............. Loss: 2.0221\n",
      "Epoch: 192/1000............. Loss: 2.0152\n",
      "Epoch: 193/1000............. Loss: 2.0083\n",
      "Epoch: 194/1000............. Loss: 2.0013\n",
      "Epoch: 195/1000............. Loss: 1.9943\n",
      "Epoch: 196/1000............. Loss: 1.9872\n",
      "Epoch: 197/1000............. Loss: 1.9801\n",
      "Epoch: 198/1000............. Loss: 1.9730\n",
      "Epoch: 199/1000............. Loss: 1.9658\n",
      "Epoch: 200/1000............. Loss: 1.9586\n",
      "Epoch: 201/1000............. Loss: 1.9514\n",
      "Epoch: 202/1000............. Loss: 1.9441\n",
      "Epoch: 203/1000............. Loss: 1.9368\n",
      "Epoch: 204/1000............. Loss: 1.9295\n",
      "Epoch: 205/1000............. Loss: 1.9222\n",
      "Epoch: 206/1000............. Loss: 1.9148\n",
      "Epoch: 207/1000............. Loss: 1.9074\n",
      "Epoch: 208/1000............. Loss: 1.9001\n",
      "Epoch: 209/1000............. Loss: 1.8926\n",
      "Epoch: 210/1000............. Loss: 1.8852\n",
      "Epoch: 211/1000............. Loss: 1.8778\n",
      "Epoch: 212/1000............. Loss: 1.8703\n",
      "Epoch: 213/1000............. Loss: 1.8628\n",
      "Epoch: 214/1000............. Loss: 1.8554\n",
      "Epoch: 215/1000............. Loss: 1.8479\n",
      "Epoch: 216/1000............. Loss: 1.8404\n",
      "Epoch: 217/1000............. Loss: 1.8329\n",
      "Epoch: 218/1000............. Loss: 1.8255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219/1000............. Loss: 1.8180\n",
      "Epoch: 220/1000............. Loss: 1.8105\n",
      "Epoch: 221/1000............. Loss: 1.8030\n",
      "Epoch: 222/1000............. Loss: 1.7955\n",
      "Epoch: 223/1000............. Loss: 1.7881\n",
      "Epoch: 224/1000............. Loss: 1.7806\n",
      "Epoch: 225/1000............. Loss: 1.7732\n",
      "Epoch: 226/1000............. Loss: 1.7658\n",
      "Epoch: 227/1000............. Loss: 1.7583\n",
      "Epoch: 228/1000............. Loss: 1.7509\n",
      "Epoch: 229/1000............. Loss: 1.7436\n",
      "Epoch: 230/1000............. Loss: 1.7362\n",
      "Epoch: 231/1000............. Loss: 1.7288\n",
      "Epoch: 232/1000............. Loss: 1.7215\n",
      "Epoch: 233/1000............. Loss: 1.7142\n",
      "Epoch: 234/1000............. Loss: 1.7069\n",
      "Epoch: 235/1000............. Loss: 1.6996\n",
      "Epoch: 236/1000............. Loss: 1.6924\n",
      "Epoch: 237/1000............. Loss: 1.6851\n",
      "Epoch: 238/1000............. Loss: 1.6779\n",
      "Epoch: 239/1000............. Loss: 1.6708\n",
      "Epoch: 240/1000............. Loss: 1.6636\n",
      "Epoch: 241/1000............. Loss: 1.6565\n",
      "Epoch: 242/1000............. Loss: 1.6494\n",
      "Epoch: 243/1000............. Loss: 1.6423\n",
      "Epoch: 244/1000............. Loss: 1.6352\n",
      "Epoch: 245/1000............. Loss: 1.6282\n",
      "Epoch: 246/1000............. Loss: 1.6212\n",
      "Epoch: 247/1000............. Loss: 1.6142\n",
      "Epoch: 248/1000............. Loss: 1.6072\n",
      "Epoch: 249/1000............. Loss: 1.6003\n",
      "Epoch: 250/1000............. Loss: 1.5934\n",
      "Epoch: 251/1000............. Loss: 1.5865\n",
      "Epoch: 252/1000............. Loss: 1.5797\n",
      "Epoch: 253/1000............. Loss: 1.5728\n",
      "Epoch: 254/1000............. Loss: 1.5660\n",
      "Epoch: 255/1000............. Loss: 1.5593\n",
      "Epoch: 256/1000............. Loss: 1.5525\n",
      "Epoch: 257/1000............. Loss: 1.5458\n",
      "Epoch: 258/1000............. Loss: 1.5391\n",
      "Epoch: 259/1000............. Loss: 1.5324\n",
      "Epoch: 260/1000............. Loss: 1.5257\n",
      "Epoch: 261/1000............. Loss: 1.5191\n",
      "Epoch: 262/1000............. Loss: 1.5125\n",
      "Epoch: 263/1000............. Loss: 1.5059\n",
      "Epoch: 264/1000............. Loss: 1.4994\n",
      "Epoch: 265/1000............. Loss: 1.4928\n",
      "Epoch: 266/1000............. Loss: 1.4863\n",
      "Epoch: 267/1000............. Loss: 1.4798\n",
      "Epoch: 268/1000............. Loss: 1.4733\n",
      "Epoch: 269/1000............. Loss: 1.4669\n",
      "Epoch: 270/1000............. Loss: 1.4605\n",
      "Epoch: 271/1000............. Loss: 1.4541\n",
      "Epoch: 272/1000............. Loss: 1.4477\n",
      "Epoch: 273/1000............. Loss: 1.4413\n",
      "Epoch: 274/1000............. Loss: 1.4350\n",
      "Epoch: 275/1000............. Loss: 1.4287\n",
      "Epoch: 276/1000............. Loss: 1.4224\n",
      "Epoch: 277/1000............. Loss: 1.4161\n",
      "Epoch: 278/1000............. Loss: 1.4098\n",
      "Epoch: 279/1000............. Loss: 1.4036\n",
      "Epoch: 280/1000............. Loss: 1.3974\n",
      "Epoch: 281/1000............. Loss: 1.3912\n",
      "Epoch: 282/1000............. Loss: 1.3850\n",
      "Epoch: 283/1000............. Loss: 1.3788\n",
      "Epoch: 284/1000............. Loss: 1.3727\n",
      "Epoch: 285/1000............. Loss: 1.3666\n",
      "Epoch: 286/1000............. Loss: 1.3605\n",
      "Epoch: 287/1000............. Loss: 1.3544\n",
      "Epoch: 288/1000............. Loss: 1.3484\n",
      "Epoch: 289/1000............. Loss: 1.3424\n",
      "Epoch: 290/1000............. Loss: 1.3363\n",
      "Epoch: 291/1000............. Loss: 1.3304\n",
      "Epoch: 292/1000............. Loss: 1.3244\n",
      "Epoch: 293/1000............. Loss: 1.3184\n",
      "Epoch: 294/1000............. Loss: 1.3125\n",
      "Epoch: 295/1000............. Loss: 1.3066\n",
      "Epoch: 296/1000............. Loss: 1.3007\n",
      "Epoch: 297/1000............. Loss: 1.2948\n",
      "Epoch: 298/1000............. Loss: 1.2890\n",
      "Epoch: 299/1000............. Loss: 1.2832\n",
      "Epoch: 300/1000............. Loss: 1.2774\n",
      "Epoch: 301/1000............. Loss: 1.2716\n",
      "Epoch: 302/1000............. Loss: 1.2658\n",
      "Epoch: 303/1000............. Loss: 1.2601\n",
      "Epoch: 304/1000............. Loss: 1.2544\n",
      "Epoch: 305/1000............. Loss: 1.2487\n",
      "Epoch: 306/1000............. Loss: 1.2430\n",
      "Epoch: 307/1000............. Loss: 1.2373\n",
      "Epoch: 308/1000............. Loss: 1.2317\n",
      "Epoch: 309/1000............. Loss: 1.2261\n",
      "Epoch: 310/1000............. Loss: 1.2205\n",
      "Epoch: 311/1000............. Loss: 1.2149\n",
      "Epoch: 312/1000............. Loss: 1.2094\n",
      "Epoch: 313/1000............. Loss: 1.2039\n",
      "Epoch: 314/1000............. Loss: 1.1984\n",
      "Epoch: 315/1000............. Loss: 1.1929\n",
      "Epoch: 316/1000............. Loss: 1.1875\n",
      "Epoch: 317/1000............. Loss: 1.1820\n",
      "Epoch: 318/1000............. Loss: 1.1766\n",
      "Epoch: 319/1000............. Loss: 1.1712\n",
      "Epoch: 320/1000............. Loss: 1.1659\n",
      "Epoch: 321/1000............. Loss: 1.1605\n",
      "Epoch: 322/1000............. Loss: 1.1552\n",
      "Epoch: 323/1000............. Loss: 1.1499\n",
      "Epoch: 324/1000............. Loss: 1.1446\n",
      "Epoch: 325/1000............. Loss: 1.1394\n",
      "Epoch: 326/1000............. Loss: 1.1342\n",
      "Epoch: 327/1000............. Loss: 1.1289\n",
      "Epoch: 328/1000............. Loss: 1.1238\n",
      "Epoch: 329/1000............. Loss: 1.1186\n",
      "Epoch: 330/1000............. Loss: 1.1135\n",
      "Epoch: 331/1000............. Loss: 1.1084\n",
      "Epoch: 332/1000............. Loss: 1.1033\n",
      "Epoch: 333/1000............. Loss: 1.0982\n",
      "Epoch: 334/1000............. Loss: 1.0932\n",
      "Epoch: 335/1000............. Loss: 1.0882\n",
      "Epoch: 336/1000............. Loss: 1.0832\n",
      "Epoch: 337/1000............. Loss: 1.0782\n",
      "Epoch: 338/1000............. Loss: 1.0733\n",
      "Epoch: 339/1000............. Loss: 1.0684\n",
      "Epoch: 340/1000............. Loss: 1.0635\n",
      "Epoch: 341/1000............. Loss: 1.0586\n",
      "Epoch: 342/1000............. Loss: 1.0538\n",
      "Epoch: 343/1000............. Loss: 1.0489\n",
      "Epoch: 344/1000............. Loss: 1.0442\n",
      "Epoch: 345/1000............. Loss: 1.0394\n",
      "Epoch: 346/1000............. Loss: 1.0346\n",
      "Epoch: 347/1000............. Loss: 1.0299\n",
      "Epoch: 348/1000............. Loss: 1.0252\n",
      "Epoch: 349/1000............. Loss: 1.0206\n",
      "Epoch: 350/1000............. Loss: 1.0159\n",
      "Epoch: 351/1000............. Loss: 1.0113\n",
      "Epoch: 352/1000............. Loss: 1.0067\n",
      "Epoch: 353/1000............. Loss: 1.0022\n",
      "Epoch: 354/1000............. Loss: 0.9976\n",
      "Epoch: 355/1000............. Loss: 0.9931\n",
      "Epoch: 356/1000............. Loss: 0.9886\n",
      "Epoch: 357/1000............. Loss: 0.9842\n",
      "Epoch: 358/1000............. Loss: 0.9797\n",
      "Epoch: 359/1000............. Loss: 0.9753\n",
      "Epoch: 360/1000............. Loss: 0.9709\n",
      "Epoch: 361/1000............. Loss: 0.9666\n",
      "Epoch: 362/1000............. Loss: 0.9622\n",
      "Epoch: 363/1000............. Loss: 0.9579\n",
      "Epoch: 364/1000............. Loss: 0.9536\n",
      "Epoch: 365/1000............. Loss: 0.9494\n",
      "Epoch: 366/1000............. Loss: 0.9451\n",
      "Epoch: 367/1000............. Loss: 0.9409\n",
      "Epoch: 368/1000............. Loss: 0.9367\n",
      "Epoch: 369/1000............. Loss: 0.9326\n",
      "Epoch: 370/1000............. Loss: 0.9284\n",
      "Epoch: 371/1000............. Loss: 0.9243\n",
      "Epoch: 372/1000............. Loss: 0.9202\n",
      "Epoch: 373/1000............. Loss: 0.9162\n",
      "Epoch: 374/1000............. Loss: 0.9121\n",
      "Epoch: 375/1000............. Loss: 0.9081\n",
      "Epoch: 376/1000............. Loss: 0.9041\n",
      "Epoch: 377/1000............. Loss: 0.9001\n",
      "Epoch: 378/1000............. Loss: 0.8962\n",
      "Epoch: 379/1000............. Loss: 0.8923\n",
      "Epoch: 380/1000............. Loss: 0.8884\n",
      "Epoch: 381/1000............. Loss: 0.8845\n",
      "Epoch: 382/1000............. Loss: 0.8806\n",
      "Epoch: 383/1000............. Loss: 0.8768\n",
      "Epoch: 384/1000............. Loss: 0.8730\n",
      "Epoch: 385/1000............. Loss: 0.8692\n",
      "Epoch: 386/1000............. Loss: 0.8655\n",
      "Epoch: 387/1000............. Loss: 0.8617\n",
      "Epoch: 388/1000............. Loss: 0.8580\n",
      "Epoch: 389/1000............. Loss: 0.8543\n",
      "Epoch: 390/1000............. Loss: 0.8506\n",
      "Epoch: 391/1000............. Loss: 0.8470\n",
      "Epoch: 392/1000............. Loss: 0.8433\n",
      "Epoch: 393/1000............. Loss: 0.8397\n",
      "Epoch: 394/1000............. Loss: 0.8361\n",
      "Epoch: 395/1000............. Loss: 0.8326\n",
      "Epoch: 396/1000............. Loss: 0.8290\n",
      "Epoch: 397/1000............. Loss: 0.8255\n",
      "Epoch: 398/1000............. Loss: 0.8220\n",
      "Epoch: 399/1000............. Loss: 0.8185\n",
      "Epoch: 400/1000............. Loss: 0.8151\n",
      "Epoch: 401/1000............. Loss: 0.8116\n",
      "Epoch: 402/1000............. Loss: 0.8082\n",
      "Epoch: 403/1000............. Loss: 0.8048\n",
      "Epoch: 404/1000............. Loss: 0.8014\n",
      "Epoch: 405/1000............. Loss: 0.7980\n",
      "Epoch: 406/1000............. Loss: 0.7947\n",
      "Epoch: 407/1000............. Loss: 0.7914\n",
      "Epoch: 408/1000............. Loss: 0.7881\n",
      "Epoch: 409/1000............. Loss: 0.7848\n",
      "Epoch: 410/1000............. Loss: 0.7815\n",
      "Epoch: 411/1000............. Loss: 0.7782\n",
      "Epoch: 412/1000............. Loss: 0.7750\n",
      "Epoch: 413/1000............. Loss: 0.7718\n",
      "Epoch: 414/1000............. Loss: 0.7686\n",
      "Epoch: 415/1000............. Loss: 0.7654\n",
      "Epoch: 416/1000............. Loss: 0.7623\n",
      "Epoch: 417/1000............. Loss: 0.7591\n",
      "Epoch: 418/1000............. Loss: 0.7560\n",
      "Epoch: 419/1000............. Loss: 0.7529\n",
      "Epoch: 420/1000............. Loss: 0.7498\n",
      "Epoch: 421/1000............. Loss: 0.7467\n",
      "Epoch: 422/1000............. Loss: 0.7437\n",
      "Epoch: 423/1000............. Loss: 0.7406\n",
      "Epoch: 424/1000............. Loss: 0.7376\n",
      "Epoch: 425/1000............. Loss: 0.7346\n",
      "Epoch: 426/1000............. Loss: 0.7316\n",
      "Epoch: 427/1000............. Loss: 0.7287\n",
      "Epoch: 428/1000............. Loss: 0.7257\n",
      "Epoch: 429/1000............. Loss: 0.7228\n",
      "Epoch: 430/1000............. Loss: 0.7199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 431/1000............. Loss: 0.7170\n",
      "Epoch: 432/1000............. Loss: 0.7141\n",
      "Epoch: 433/1000............. Loss: 0.7112\n",
      "Epoch: 434/1000............. Loss: 0.7083\n",
      "Epoch: 435/1000............. Loss: 0.7055\n",
      "Epoch: 436/1000............. Loss: 0.7027\n",
      "Epoch: 437/1000............. Loss: 0.6998\n",
      "Epoch: 438/1000............. Loss: 0.6971\n",
      "Epoch: 439/1000............. Loss: 0.6943\n",
      "Epoch: 440/1000............. Loss: 0.6915\n",
      "Epoch: 441/1000............. Loss: 0.6887\n",
      "Epoch: 442/1000............. Loss: 0.6860\n",
      "Epoch: 443/1000............. Loss: 0.6833\n",
      "Epoch: 444/1000............. Loss: 0.6806\n",
      "Epoch: 445/1000............. Loss: 0.6779\n",
      "Epoch: 446/1000............. Loss: 0.6752\n",
      "Epoch: 447/1000............. Loss: 0.6725\n",
      "Epoch: 448/1000............. Loss: 0.6699\n",
      "Epoch: 449/1000............. Loss: 0.6672\n",
      "Epoch: 450/1000............. Loss: 0.6646\n",
      "Epoch: 451/1000............. Loss: 0.6620\n",
      "Epoch: 452/1000............. Loss: 0.6594\n",
      "Epoch: 453/1000............. Loss: 0.6568\n",
      "Epoch: 454/1000............. Loss: 0.6543\n",
      "Epoch: 455/1000............. Loss: 0.6517\n",
      "Epoch: 456/1000............. Loss: 0.6491\n",
      "Epoch: 457/1000............. Loss: 0.6466\n",
      "Epoch: 458/1000............. Loss: 0.6441\n",
      "Epoch: 459/1000............. Loss: 0.6416\n",
      "Epoch: 460/1000............. Loss: 0.6391\n",
      "Epoch: 461/1000............. Loss: 0.6366\n",
      "Epoch: 462/1000............. Loss: 0.6341\n",
      "Epoch: 463/1000............. Loss: 0.6317\n",
      "Epoch: 464/1000............. Loss: 0.6292\n",
      "Epoch: 465/1000............. Loss: 0.6268\n",
      "Epoch: 466/1000............. Loss: 0.6244\n",
      "Epoch: 467/1000............. Loss: 0.6220\n",
      "Epoch: 468/1000............. Loss: 0.6196\n",
      "Epoch: 469/1000............. Loss: 0.6172\n",
      "Epoch: 470/1000............. Loss: 0.6148\n",
      "Epoch: 471/1000............. Loss: 0.6125\n",
      "Epoch: 472/1000............. Loss: 0.6101\n",
      "Epoch: 473/1000............. Loss: 0.6078\n",
      "Epoch: 474/1000............. Loss: 0.6054\n",
      "Epoch: 475/1000............. Loss: 0.6031\n",
      "Epoch: 476/1000............. Loss: 0.6008\n",
      "Epoch: 477/1000............. Loss: 0.5985\n",
      "Epoch: 478/1000............. Loss: 0.5963\n",
      "Epoch: 479/1000............. Loss: 0.5940\n",
      "Epoch: 480/1000............. Loss: 0.5917\n",
      "Epoch: 481/1000............. Loss: 0.5895\n",
      "Epoch: 482/1000............. Loss: 0.5872\n",
      "Epoch: 483/1000............. Loss: 0.5850\n",
      "Epoch: 484/1000............. Loss: 0.5828\n",
      "Epoch: 485/1000............. Loss: 0.5806\n",
      "Epoch: 486/1000............. Loss: 0.5784\n",
      "Epoch: 487/1000............. Loss: 0.5762\n",
      "Epoch: 488/1000............. Loss: 0.5740\n",
      "Epoch: 489/1000............. Loss: 0.5719\n",
      "Epoch: 490/1000............. Loss: 0.5697\n",
      "Epoch: 491/1000............. Loss: 0.5676\n",
      "Epoch: 492/1000............. Loss: 0.5654\n",
      "Epoch: 493/1000............. Loss: 0.5633\n",
      "Epoch: 494/1000............. Loss: 0.5612\n",
      "Epoch: 495/1000............. Loss: 0.5591\n",
      "Epoch: 496/1000............. Loss: 0.5570\n",
      "Epoch: 497/1000............. Loss: 0.5549\n",
      "Epoch: 498/1000............. Loss: 0.5528\n",
      "Epoch: 499/1000............. Loss: 0.5508\n",
      "Epoch: 500/1000............. Loss: 0.5487\n",
      "Epoch: 501/1000............. Loss: 0.5467\n",
      "Epoch: 502/1000............. Loss: 0.5446\n",
      "Epoch: 503/1000............. Loss: 0.5426\n",
      "Epoch: 504/1000............. Loss: 0.5406\n",
      "Epoch: 505/1000............. Loss: 0.5386\n",
      "Epoch: 506/1000............. Loss: 0.5366\n",
      "Epoch: 507/1000............. Loss: 0.5346\n",
      "Epoch: 508/1000............. Loss: 0.5326\n",
      "Epoch: 509/1000............. Loss: 0.5306\n",
      "Epoch: 510/1000............. Loss: 0.5287\n",
      "Epoch: 511/1000............. Loss: 0.5267\n",
      "Epoch: 512/1000............. Loss: 0.5248\n",
      "Epoch: 513/1000............. Loss: 0.5228\n",
      "Epoch: 514/1000............. Loss: 0.5209\n",
      "Epoch: 515/1000............. Loss: 0.5190\n",
      "Epoch: 516/1000............. Loss: 0.5171\n",
      "Epoch: 517/1000............. Loss: 0.5152\n",
      "Epoch: 518/1000............. Loss: 0.5133\n",
      "Epoch: 519/1000............. Loss: 0.5114\n",
      "Epoch: 520/1000............. Loss: 0.5095\n",
      "Epoch: 521/1000............. Loss: 0.5076\n",
      "Epoch: 522/1000............. Loss: 0.5058\n",
      "Epoch: 523/1000............. Loss: 0.5039\n",
      "Epoch: 524/1000............. Loss: 0.5021\n",
      "Epoch: 525/1000............. Loss: 0.5003\n",
      "Epoch: 526/1000............. Loss: 0.4984\n",
      "Epoch: 527/1000............. Loss: 0.4966\n",
      "Epoch: 528/1000............. Loss: 0.4948\n",
      "Epoch: 529/1000............. Loss: 0.4930\n",
      "Epoch: 530/1000............. Loss: 0.4912\n",
      "Epoch: 531/1000............. Loss: 0.4894\n",
      "Epoch: 532/1000............. Loss: 0.4877\n",
      "Epoch: 533/1000............. Loss: 0.4859\n",
      "Epoch: 534/1000............. Loss: 0.4841\n",
      "Epoch: 535/1000............. Loss: 0.4824\n",
      "Epoch: 536/1000............. Loss: 0.4806\n",
      "Epoch: 537/1000............. Loss: 0.4789\n",
      "Epoch: 538/1000............. Loss: 0.4772\n",
      "Epoch: 539/1000............. Loss: 0.4754\n",
      "Epoch: 540/1000............. Loss: 0.4737\n",
      "Epoch: 541/1000............. Loss: 0.4720\n",
      "Epoch: 542/1000............. Loss: 0.4703\n",
      "Epoch: 543/1000............. Loss: 0.4686\n",
      "Epoch: 544/1000............. Loss: 0.4670\n",
      "Epoch: 545/1000............. Loss: 0.4653\n",
      "Epoch: 546/1000............. Loss: 0.4636\n",
      "Epoch: 547/1000............. Loss: 0.4620\n",
      "Epoch: 548/1000............. Loss: 0.4603\n",
      "Epoch: 549/1000............. Loss: 0.4587\n",
      "Epoch: 550/1000............. Loss: 0.4570\n",
      "Epoch: 551/1000............. Loss: 0.4554\n",
      "Epoch: 552/1000............. Loss: 0.4538\n",
      "Epoch: 553/1000............. Loss: 0.4522\n",
      "Epoch: 554/1000............. Loss: 0.4505\n",
      "Epoch: 555/1000............. Loss: 0.4489\n",
      "Epoch: 556/1000............. Loss: 0.4473\n",
      "Epoch: 557/1000............. Loss: 0.4458\n",
      "Epoch: 558/1000............. Loss: 0.4442\n",
      "Epoch: 559/1000............. Loss: 0.4426\n",
      "Epoch: 560/1000............. Loss: 0.4410\n",
      "Epoch: 561/1000............. Loss: 0.4395\n",
      "Epoch: 562/1000............. Loss: 0.4379\n",
      "Epoch: 563/1000............. Loss: 0.4364\n",
      "Epoch: 564/1000............. Loss: 0.4348\n",
      "Epoch: 565/1000............. Loss: 0.4333\n",
      "Epoch: 566/1000............. Loss: 0.4318\n",
      "Epoch: 567/1000............. Loss: 0.4303\n",
      "Epoch: 568/1000............. Loss: 0.4288\n",
      "Epoch: 569/1000............. Loss: 0.4273\n",
      "Epoch: 570/1000............. Loss: 0.4258\n",
      "Epoch: 571/1000............. Loss: 0.4243\n",
      "Epoch: 572/1000............. Loss: 0.4228\n",
      "Epoch: 573/1000............. Loss: 0.4213\n",
      "Epoch: 574/1000............. Loss: 0.4198\n",
      "Epoch: 575/1000............. Loss: 0.4184\n",
      "Epoch: 576/1000............. Loss: 0.4169\n",
      "Epoch: 577/1000............. Loss: 0.4155\n",
      "Epoch: 578/1000............. Loss: 0.4140\n",
      "Epoch: 579/1000............. Loss: 0.4126\n",
      "Epoch: 580/1000............. Loss: 0.4111\n",
      "Epoch: 581/1000............. Loss: 0.4097\n",
      "Epoch: 582/1000............. Loss: 0.4083\n",
      "Epoch: 583/1000............. Loss: 0.4069\n",
      "Epoch: 584/1000............. Loss: 0.4055\n",
      "Epoch: 585/1000............. Loss: 0.4041\n",
      "Epoch: 586/1000............. Loss: 0.4027\n",
      "Epoch: 587/1000............. Loss: 0.4013\n",
      "Epoch: 588/1000............. Loss: 0.3999\n",
      "Epoch: 589/1000............. Loss: 0.3985\n",
      "Epoch: 590/1000............. Loss: 0.3972\n",
      "Epoch: 591/1000............. Loss: 0.3958\n",
      "Epoch: 592/1000............. Loss: 0.3944\n",
      "Epoch: 593/1000............. Loss: 0.3931\n",
      "Epoch: 594/1000............. Loss: 0.3918\n",
      "Epoch: 595/1000............. Loss: 0.3904\n",
      "Epoch: 596/1000............. Loss: 0.3891\n",
      "Epoch: 597/1000............. Loss: 0.3878\n",
      "Epoch: 598/1000............. Loss: 0.3864\n",
      "Epoch: 599/1000............. Loss: 0.3851\n",
      "Epoch: 600/1000............. Loss: 0.3838\n",
      "Epoch: 601/1000............. Loss: 0.3825\n",
      "Epoch: 602/1000............. Loss: 0.3812\n",
      "Epoch: 603/1000............. Loss: 0.3799\n",
      "Epoch: 604/1000............. Loss: 0.3786\n",
      "Epoch: 605/1000............. Loss: 0.3773\n",
      "Epoch: 606/1000............. Loss: 0.3761\n",
      "Epoch: 607/1000............. Loss: 0.3748\n",
      "Epoch: 608/1000............. Loss: 0.3735\n",
      "Epoch: 609/1000............. Loss: 0.3723\n",
      "Epoch: 610/1000............. Loss: 0.3710\n",
      "Epoch: 611/1000............. Loss: 0.3698\n",
      "Epoch: 612/1000............. Loss: 0.3685\n",
      "Epoch: 613/1000............. Loss: 0.3673\n",
      "Epoch: 614/1000............. Loss: 0.3661\n",
      "Epoch: 615/1000............. Loss: 0.3648\n",
      "Epoch: 616/1000............. Loss: 0.3636\n",
      "Epoch: 617/1000............. Loss: 0.3624\n",
      "Epoch: 618/1000............. Loss: 0.3612\n",
      "Epoch: 619/1000............. Loss: 0.3600\n",
      "Epoch: 620/1000............. Loss: 0.3588\n",
      "Epoch: 621/1000............. Loss: 0.3576\n",
      "Epoch: 622/1000............. Loss: 0.3564\n",
      "Epoch: 623/1000............. Loss: 0.3552\n",
      "Epoch: 624/1000............. Loss: 0.3541\n",
      "Epoch: 625/1000............. Loss: 0.3529\n",
      "Epoch: 626/1000............. Loss: 0.3517\n",
      "Epoch: 627/1000............. Loss: 0.3506\n",
      "Epoch: 628/1000............. Loss: 0.3494\n",
      "Epoch: 629/1000............. Loss: 0.3483\n",
      "Epoch: 630/1000............. Loss: 0.3471\n",
      "Epoch: 631/1000............. Loss: 0.3460\n",
      "Epoch: 632/1000............. Loss: 0.3449\n",
      "Epoch: 633/1000............. Loss: 0.3437\n",
      "Epoch: 634/1000............. Loss: 0.3426\n",
      "Epoch: 635/1000............. Loss: 0.3415\n",
      "Epoch: 636/1000............. Loss: 0.3404\n",
      "Epoch: 637/1000............. Loss: 0.3393\n",
      "Epoch: 638/1000............. Loss: 0.3382\n",
      "Epoch: 639/1000............. Loss: 0.3371\n",
      "Epoch: 640/1000............. Loss: 0.3360\n",
      "Epoch: 641/1000............. Loss: 0.3349\n",
      "Epoch: 642/1000............. Loss: 0.3338\n",
      "Epoch: 643/1000............. Loss: 0.3327\n",
      "Epoch: 644/1000............. Loss: 0.3316\n",
      "Epoch: 645/1000............. Loss: 0.3306\n",
      "Epoch: 646/1000............. Loss: 0.3295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 647/1000............. Loss: 0.3285\n",
      "Epoch: 648/1000............. Loss: 0.3274\n",
      "Epoch: 649/1000............. Loss: 0.3264\n",
      "Epoch: 650/1000............. Loss: 0.3253\n",
      "Epoch: 651/1000............. Loss: 0.3243\n",
      "Epoch: 652/1000............. Loss: 0.3232\n",
      "Epoch: 653/1000............. Loss: 0.3222\n",
      "Epoch: 654/1000............. Loss: 0.3212\n",
      "Epoch: 655/1000............. Loss: 0.3202\n",
      "Epoch: 656/1000............. Loss: 0.3191\n",
      "Epoch: 657/1000............. Loss: 0.3181\n",
      "Epoch: 658/1000............. Loss: 0.3171\n",
      "Epoch: 659/1000............. Loss: 0.3161\n",
      "Epoch: 660/1000............. Loss: 0.3151\n",
      "Epoch: 661/1000............. Loss: 0.3141\n",
      "Epoch: 662/1000............. Loss: 0.3132\n",
      "Epoch: 663/1000............. Loss: 0.3122\n",
      "Epoch: 664/1000............. Loss: 0.3112\n",
      "Epoch: 665/1000............. Loss: 0.3102\n",
      "Epoch: 666/1000............. Loss: 0.3093\n",
      "Epoch: 667/1000............. Loss: 0.3083\n",
      "Epoch: 668/1000............. Loss: 0.3073\n",
      "Epoch: 669/1000............. Loss: 0.3064\n",
      "Epoch: 670/1000............. Loss: 0.3054\n",
      "Epoch: 671/1000............. Loss: 0.3045\n",
      "Epoch: 672/1000............. Loss: 0.3035\n",
      "Epoch: 673/1000............. Loss: 0.3026\n",
      "Epoch: 674/1000............. Loss: 0.3017\n",
      "Epoch: 675/1000............. Loss: 0.3007\n",
      "Epoch: 676/1000............. Loss: 0.2998\n",
      "Epoch: 677/1000............. Loss: 0.2989\n",
      "Epoch: 678/1000............. Loss: 0.2980\n",
      "Epoch: 679/1000............. Loss: 0.2971\n",
      "Epoch: 680/1000............. Loss: 0.2962\n",
      "Epoch: 681/1000............. Loss: 0.2953\n",
      "Epoch: 682/1000............. Loss: 0.2944\n",
      "Epoch: 683/1000............. Loss: 0.2935\n",
      "Epoch: 684/1000............. Loss: 0.2926\n",
      "Epoch: 685/1000............. Loss: 0.2917\n",
      "Epoch: 686/1000............. Loss: 0.2908\n",
      "Epoch: 687/1000............. Loss: 0.2899\n",
      "Epoch: 688/1000............. Loss: 0.2891\n",
      "Epoch: 689/1000............. Loss: 0.2882\n",
      "Epoch: 690/1000............. Loss: 0.2873\n",
      "Epoch: 691/1000............. Loss: 0.2865\n",
      "Epoch: 692/1000............. Loss: 0.2856\n",
      "Epoch: 693/1000............. Loss: 0.2847\n",
      "Epoch: 694/1000............. Loss: 0.2839\n",
      "Epoch: 695/1000............. Loss: 0.2831\n",
      "Epoch: 696/1000............. Loss: 0.2822\n",
      "Epoch: 697/1000............. Loss: 0.2814\n",
      "Epoch: 698/1000............. Loss: 0.2805\n",
      "Epoch: 699/1000............. Loss: 0.2797\n",
      "Epoch: 700/1000............. Loss: 0.2789\n",
      "Epoch: 701/1000............. Loss: 0.2781\n",
      "Epoch: 702/1000............. Loss: 0.2772\n",
      "Epoch: 703/1000............. Loss: 0.2764\n",
      "Epoch: 704/1000............. Loss: 0.2756\n",
      "Epoch: 705/1000............. Loss: 0.2748\n",
      "Epoch: 706/1000............. Loss: 0.2740\n",
      "Epoch: 707/1000............. Loss: 0.2732\n",
      "Epoch: 708/1000............. Loss: 0.2724\n",
      "Epoch: 709/1000............. Loss: 0.2716\n",
      "Epoch: 710/1000............. Loss: 0.2708\n",
      "Epoch: 711/1000............. Loss: 0.2701\n",
      "Epoch: 712/1000............. Loss: 0.2693\n",
      "Epoch: 713/1000............. Loss: 0.2685\n",
      "Epoch: 714/1000............. Loss: 0.2677\n",
      "Epoch: 715/1000............. Loss: 0.2670\n",
      "Epoch: 716/1000............. Loss: 0.2662\n",
      "Epoch: 717/1000............. Loss: 0.2654\n",
      "Epoch: 718/1000............. Loss: 0.2647\n",
      "Epoch: 719/1000............. Loss: 0.2639\n",
      "Epoch: 720/1000............. Loss: 0.2632\n",
      "Epoch: 721/1000............. Loss: 0.2624\n",
      "Epoch: 722/1000............. Loss: 0.2617\n",
      "Epoch: 723/1000............. Loss: 0.2609\n",
      "Epoch: 724/1000............. Loss: 0.2602\n",
      "Epoch: 725/1000............. Loss: 0.2595\n",
      "Epoch: 726/1000............. Loss: 0.2587\n",
      "Epoch: 727/1000............. Loss: 0.2580\n",
      "Epoch: 728/1000............. Loss: 0.2573\n",
      "Epoch: 729/1000............. Loss: 0.2565\n",
      "Epoch: 730/1000............. Loss: 0.2558\n",
      "Epoch: 731/1000............. Loss: 0.2551\n",
      "Epoch: 732/1000............. Loss: 0.2544\n",
      "Epoch: 733/1000............. Loss: 0.2537\n",
      "Epoch: 734/1000............. Loss: 0.2530\n",
      "Epoch: 735/1000............. Loss: 0.2523\n",
      "Epoch: 736/1000............. Loss: 0.2516\n",
      "Epoch: 737/1000............. Loss: 0.2509\n",
      "Epoch: 738/1000............. Loss: 0.2502\n",
      "Epoch: 739/1000............. Loss: 0.2495\n",
      "Epoch: 740/1000............. Loss: 0.2488\n",
      "Epoch: 741/1000............. Loss: 0.2481\n",
      "Epoch: 742/1000............. Loss: 0.2475\n",
      "Epoch: 743/1000............. Loss: 0.2468\n",
      "Epoch: 744/1000............. Loss: 0.2461\n",
      "Epoch: 745/1000............. Loss: 0.2454\n",
      "Epoch: 746/1000............. Loss: 0.2448\n",
      "Epoch: 747/1000............. Loss: 0.2441\n",
      "Epoch: 748/1000............. Loss: 0.2434\n",
      "Epoch: 749/1000............. Loss: 0.2428\n",
      "Epoch: 750/1000............. Loss: 0.2421\n",
      "Epoch: 751/1000............. Loss: 0.2415\n",
      "Epoch: 752/1000............. Loss: 0.2408\n",
      "Epoch: 753/1000............. Loss: 0.2402\n",
      "Epoch: 754/1000............. Loss: 0.2395\n",
      "Epoch: 755/1000............. Loss: 0.2389\n",
      "Epoch: 756/1000............. Loss: 0.2382\n",
      "Epoch: 757/1000............. Loss: 0.2376\n",
      "Epoch: 758/1000............. Loss: 0.2370\n",
      "Epoch: 759/1000............. Loss: 0.2363\n",
      "Epoch: 760/1000............. Loss: 0.2357\n",
      "Epoch: 761/1000............. Loss: 0.2351\n",
      "Epoch: 762/1000............. Loss: 0.2345\n",
      "Epoch: 763/1000............. Loss: 0.2338\n",
      "Epoch: 764/1000............. Loss: 0.2332\n",
      "Epoch: 765/1000............. Loss: 0.2326\n",
      "Epoch: 766/1000............. Loss: 0.2320\n",
      "Epoch: 767/1000............. Loss: 0.2314\n",
      "Epoch: 768/1000............. Loss: 0.2308\n",
      "Epoch: 769/1000............. Loss: 0.2302\n",
      "Epoch: 770/1000............. Loss: 0.2296\n",
      "Epoch: 771/1000............. Loss: 0.2290\n",
      "Epoch: 772/1000............. Loss: 0.2284\n",
      "Epoch: 773/1000............. Loss: 0.2278\n",
      "Epoch: 774/1000............. Loss: 0.2272\n",
      "Epoch: 775/1000............. Loss: 0.2266\n",
      "Epoch: 776/1000............. Loss: 0.2260\n",
      "Epoch: 777/1000............. Loss: 0.2254\n",
      "Epoch: 778/1000............. Loss: 0.2248\n",
      "Epoch: 779/1000............. Loss: 0.2243\n",
      "Epoch: 780/1000............. Loss: 0.2237\n",
      "Epoch: 781/1000............. Loss: 0.2231\n",
      "Epoch: 782/1000............. Loss: 0.2225\n",
      "Epoch: 783/1000............. Loss: 0.2219\n",
      "Epoch: 784/1000............. Loss: 0.2214\n",
      "Epoch: 785/1000............. Loss: 0.2208\n",
      "Epoch: 786/1000............. Loss: 0.2202\n",
      "Epoch: 787/1000............. Loss: 0.2197\n",
      "Epoch: 788/1000............. Loss: 0.2191\n",
      "Epoch: 789/1000............. Loss: 0.2186\n",
      "Epoch: 790/1000............. Loss: 0.2180\n",
      "Epoch: 791/1000............. Loss: 0.2174\n",
      "Epoch: 792/1000............. Loss: 0.2169\n",
      "Epoch: 793/1000............. Loss: 0.2163\n",
      "Epoch: 794/1000............. Loss: 0.2158\n",
      "Epoch: 795/1000............. Loss: 0.2153\n",
      "Epoch: 796/1000............. Loss: 0.2147\n",
      "Epoch: 797/1000............. Loss: 0.2142\n",
      "Epoch: 798/1000............. Loss: 0.2136\n",
      "Epoch: 799/1000............. Loss: 0.2131\n",
      "Epoch: 800/1000............. Loss: 0.2125\n",
      "Epoch: 801/1000............. Loss: 0.2120\n",
      "Epoch: 802/1000............. Loss: 0.2115\n",
      "Epoch: 803/1000............. Loss: 0.2110\n",
      "Epoch: 804/1000............. Loss: 0.2104\n",
      "Epoch: 805/1000............. Loss: 0.2099\n",
      "Epoch: 806/1000............. Loss: 0.2094\n",
      "Epoch: 807/1000............. Loss: 0.2088\n",
      "Epoch: 808/1000............. Loss: 0.2083\n",
      "Epoch: 809/1000............. Loss: 0.2078\n",
      "Epoch: 810/1000............. Loss: 0.2073\n",
      "Epoch: 811/1000............. Loss: 0.2068\n",
      "Epoch: 812/1000............. Loss: 0.2063\n",
      "Epoch: 813/1000............. Loss: 0.2057\n",
      "Epoch: 814/1000............. Loss: 0.2052\n",
      "Epoch: 815/1000............. Loss: 0.2047\n",
      "Epoch: 816/1000............. Loss: 0.2042\n",
      "Epoch: 817/1000............. Loss: 0.2037\n",
      "Epoch: 818/1000............. Loss: 0.2032\n",
      "Epoch: 819/1000............. Loss: 0.2027\n",
      "Epoch: 820/1000............. Loss: 0.2022\n",
      "Epoch: 821/1000............. Loss: 0.2017\n",
      "Epoch: 822/1000............. Loss: 0.2012\n",
      "Epoch: 823/1000............. Loss: 0.2007\n",
      "Epoch: 824/1000............. Loss: 0.2002\n",
      "Epoch: 825/1000............. Loss: 0.1997\n",
      "Epoch: 826/1000............. Loss: 0.1992\n",
      "Epoch: 827/1000............. Loss: 0.1988\n",
      "Epoch: 828/1000............. Loss: 0.1983\n",
      "Epoch: 829/1000............. Loss: 0.1978\n",
      "Epoch: 830/1000............. Loss: 0.1973\n",
      "Epoch: 831/1000............. Loss: 0.1968\n",
      "Epoch: 832/1000............. Loss: 0.1963\n",
      "Epoch: 833/1000............. Loss: 0.1959\n",
      "Epoch: 834/1000............. Loss: 0.1954\n",
      "Epoch: 835/1000............. Loss: 0.1949\n",
      "Epoch: 836/1000............. Loss: 0.1944\n",
      "Epoch: 837/1000............. Loss: 0.1940\n",
      "Epoch: 838/1000............. Loss: 0.1935\n",
      "Epoch: 839/1000............. Loss: 0.1930\n",
      "Epoch: 840/1000............. Loss: 0.1925\n",
      "Epoch: 841/1000............. Loss: 0.1921\n",
      "Epoch: 842/1000............. Loss: 0.1916\n",
      "Epoch: 843/1000............. Loss: 0.1911\n",
      "Epoch: 844/1000............. Loss: 0.1907\n",
      "Epoch: 845/1000............. Loss: 0.1902\n",
      "Epoch: 846/1000............. Loss: 0.1898\n",
      "Epoch: 847/1000............. Loss: 0.1893\n",
      "Epoch: 848/1000............. Loss: 0.1889\n",
      "Epoch: 849/1000............. Loss: 0.1884\n",
      "Epoch: 850/1000............. Loss: 0.1879\n",
      "Epoch: 851/1000............. Loss: 0.1875\n",
      "Epoch: 852/1000............. Loss: 0.1870\n",
      "Epoch: 853/1000............. Loss: 0.1866\n",
      "Epoch: 854/1000............. Loss: 0.1861\n",
      "Epoch: 855/1000............. Loss: 0.1857\n",
      "Epoch: 856/1000............. Loss: 0.1852\n",
      "Epoch: 857/1000............. Loss: 0.1848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 858/1000............. Loss: 0.1844\n",
      "Epoch: 859/1000............. Loss: 0.1839\n",
      "Epoch: 860/1000............. Loss: 0.1835\n",
      "Epoch: 861/1000............. Loss: 0.1830\n",
      "Epoch: 862/1000............. Loss: 0.1826\n",
      "Epoch: 863/1000............. Loss: 0.1822\n",
      "Epoch: 864/1000............. Loss: 0.1817\n",
      "Epoch: 865/1000............. Loss: 0.1813\n",
      "Epoch: 866/1000............. Loss: 0.1809\n",
      "Epoch: 867/1000............. Loss: 0.1804\n",
      "Epoch: 868/1000............. Loss: 0.1800\n",
      "Epoch: 869/1000............. Loss: 0.1796\n",
      "Epoch: 870/1000............. Loss: 0.1792\n",
      "Epoch: 871/1000............. Loss: 0.1787\n",
      "Epoch: 872/1000............. Loss: 0.1783\n",
      "Epoch: 873/1000............. Loss: 0.1779\n",
      "Epoch: 874/1000............. Loss: 0.1775\n",
      "Epoch: 875/1000............. Loss: 0.1770\n",
      "Epoch: 876/1000............. Loss: 0.1766\n",
      "Epoch: 877/1000............. Loss: 0.1762\n",
      "Epoch: 878/1000............. Loss: 0.1758\n",
      "Epoch: 879/1000............. Loss: 0.1754\n",
      "Epoch: 880/1000............. Loss: 0.1750\n",
      "Epoch: 881/1000............. Loss: 0.1746\n",
      "Epoch: 882/1000............. Loss: 0.1741\n",
      "Epoch: 883/1000............. Loss: 0.1737\n",
      "Epoch: 884/1000............. Loss: 0.1733\n",
      "Epoch: 885/1000............. Loss: 0.1729\n",
      "Epoch: 886/1000............. Loss: 0.1725\n",
      "Epoch: 887/1000............. Loss: 0.1721\n",
      "Epoch: 888/1000............. Loss: 0.1717\n",
      "Epoch: 889/1000............. Loss: 0.1713\n",
      "Epoch: 890/1000............. Loss: 0.1709\n",
      "Epoch: 891/1000............. Loss: 0.1705\n",
      "Epoch: 892/1000............. Loss: 0.1701\n",
      "Epoch: 893/1000............. Loss: 0.1697\n",
      "Epoch: 894/1000............. Loss: 0.1693\n",
      "Epoch: 895/1000............. Loss: 0.1689\n",
      "Epoch: 896/1000............. Loss: 0.1685\n",
      "Epoch: 897/1000............. Loss: 0.1682\n",
      "Epoch: 898/1000............. Loss: 0.1678\n",
      "Epoch: 899/1000............. Loss: 0.1674\n",
      "Epoch: 900/1000............. Loss: 0.1670\n",
      "Epoch: 901/1000............. Loss: 0.1666\n",
      "Epoch: 902/1000............. Loss: 0.1662\n",
      "Epoch: 903/1000............. Loss: 0.1658\n",
      "Epoch: 904/1000............. Loss: 0.1655\n",
      "Epoch: 905/1000............. Loss: 0.1651\n",
      "Epoch: 906/1000............. Loss: 0.1647\n",
      "Epoch: 907/1000............. Loss: 0.1643\n",
      "Epoch: 908/1000............. Loss: 0.1639\n",
      "Epoch: 909/1000............. Loss: 0.1636\n",
      "Epoch: 910/1000............. Loss: 0.1632\n",
      "Epoch: 911/1000............. Loss: 0.1628\n",
      "Epoch: 912/1000............. Loss: 0.1625\n",
      "Epoch: 913/1000............. Loss: 0.1621\n",
      "Epoch: 914/1000............. Loss: 0.1617\n",
      "Epoch: 915/1000............. Loss: 0.1613\n",
      "Epoch: 916/1000............. Loss: 0.1610\n",
      "Epoch: 917/1000............. Loss: 0.1606\n",
      "Epoch: 918/1000............. Loss: 0.1603\n",
      "Epoch: 919/1000............. Loss: 0.1599\n",
      "Epoch: 920/1000............. Loss: 0.1595\n",
      "Epoch: 921/1000............. Loss: 0.1592\n",
      "Epoch: 922/1000............. Loss: 0.1588\n",
      "Epoch: 923/1000............. Loss: 0.1585\n",
      "Epoch: 924/1000............. Loss: 0.1581\n",
      "Epoch: 925/1000............. Loss: 0.1578\n",
      "Epoch: 926/1000............. Loss: 0.1574\n",
      "Epoch: 927/1000............. Loss: 0.1570\n",
      "Epoch: 928/1000............. Loss: 0.1567\n",
      "Epoch: 929/1000............. Loss: 0.1564\n",
      "Epoch: 930/1000............. Loss: 0.1560\n",
      "Epoch: 931/1000............. Loss: 0.1557\n",
      "Epoch: 932/1000............. Loss: 0.1553\n",
      "Epoch: 933/1000............. Loss: 0.1550\n",
      "Epoch: 934/1000............. Loss: 0.1546\n",
      "Epoch: 935/1000............. Loss: 0.1543\n",
      "Epoch: 936/1000............. Loss: 0.1539\n",
      "Epoch: 937/1000............. Loss: 0.1536\n",
      "Epoch: 938/1000............. Loss: 0.1533\n",
      "Epoch: 939/1000............. Loss: 0.1529\n",
      "Epoch: 940/1000............. Loss: 0.1526\n",
      "Epoch: 941/1000............. Loss: 0.1523\n",
      "Epoch: 942/1000............. Loss: 0.1519\n",
      "Epoch: 943/1000............. Loss: 0.1516\n",
      "Epoch: 944/1000............. Loss: 0.1513\n",
      "Epoch: 945/1000............. Loss: 0.1509\n",
      "Epoch: 946/1000............. Loss: 0.1506\n",
      "Epoch: 947/1000............. Loss: 0.1503\n",
      "Epoch: 948/1000............. Loss: 0.1500\n",
      "Epoch: 949/1000............. Loss: 0.1496\n",
      "Epoch: 950/1000............. Loss: 0.1493\n",
      "Epoch: 951/1000............. Loss: 0.1490\n",
      "Epoch: 952/1000............. Loss: 0.1487\n",
      "Epoch: 953/1000............. Loss: 0.1484\n",
      "Epoch: 954/1000............. Loss: 0.1480\n",
      "Epoch: 955/1000............. Loss: 0.1477\n",
      "Epoch: 956/1000............. Loss: 0.1474\n",
      "Epoch: 957/1000............. Loss: 0.1471\n",
      "Epoch: 958/1000............. Loss: 0.1468\n",
      "Epoch: 959/1000............. Loss: 0.1465\n",
      "Epoch: 960/1000............. Loss: 0.1462\n",
      "Epoch: 961/1000............. Loss: 0.1459\n",
      "Epoch: 962/1000............. Loss: 0.1456\n",
      "Epoch: 963/1000............. Loss: 0.1452\n",
      "Epoch: 964/1000............. Loss: 0.1449\n",
      "Epoch: 965/1000............. Loss: 0.1446\n",
      "Epoch: 966/1000............. Loss: 0.1443\n",
      "Epoch: 967/1000............. Loss: 0.1440\n",
      "Epoch: 968/1000............. Loss: 0.1437\n",
      "Epoch: 969/1000............. Loss: 0.1434\n",
      "Epoch: 970/1000............. Loss: 0.1431\n",
      "Epoch: 971/1000............. Loss: 0.1428\n",
      "Epoch: 972/1000............. Loss: 0.1425\n",
      "Epoch: 973/1000............. Loss: 0.1422\n",
      "Epoch: 974/1000............. Loss: 0.1419\n",
      "Epoch: 975/1000............. Loss: 0.1417\n",
      "Epoch: 976/1000............. Loss: 0.1414\n",
      "Epoch: 977/1000............. Loss: 0.1411\n",
      "Epoch: 978/1000............. Loss: 0.1408\n",
      "Epoch: 979/1000............. Loss: 0.1405\n",
      "Epoch: 980/1000............. Loss: 0.1402\n",
      "Epoch: 981/1000............. Loss: 0.1399\n",
      "Epoch: 982/1000............. Loss: 0.1396\n",
      "Epoch: 983/1000............. Loss: 0.1394\n",
      "Epoch: 984/1000............. Loss: 0.1391\n",
      "Epoch: 985/1000............. Loss: 0.1388\n",
      "Epoch: 986/1000............. Loss: 0.1385\n",
      "Epoch: 987/1000............. Loss: 0.1382\n",
      "Epoch: 988/1000............. Loss: 0.1380\n",
      "Epoch: 989/1000............. Loss: 0.1377\n",
      "Epoch: 990/1000............. Loss: 0.1374\n",
      "Epoch: 991/1000............. Loss: 0.1371\n",
      "Epoch: 992/1000............. Loss: 0.1368\n",
      "Epoch: 993/1000............. Loss: 0.1366\n",
      "Epoch: 994/1000............. Loss: 0.1363\n",
      "Epoch: 995/1000............. Loss: 0.1360\n",
      "Epoch: 996/1000............. Loss: 0.1358\n",
      "Epoch: 997/1000............. Loss: 0.1355\n",
      "Epoch: 998/1000............. Loss: 0.1352\n",
      "Epoch: 999/1000............. Loss: 0.1350\n",
      "Epoch: 1000/1000............. Loss: 0.1347\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "    print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "protected-makeup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[[1,2,3],[2,3,1]], [[-1,2,0],[2,-3,3]], [[-1,-2,3],[-2,3,2]]])\n",
    "np.mean(np.sum(X, axis=(1,2), keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "liquid-french",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = np.zeros((2,2))\n",
    "dl = np.array([[np.nan, 0],[np.nan, 0]])\n",
    "\n",
    "np.power(2,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
