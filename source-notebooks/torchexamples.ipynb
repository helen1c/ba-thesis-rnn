{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "treated-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "numerous-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(3, 4, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gothic-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputt = torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "liked-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4793,  0.2916, -0.5416],\n",
      "        [-0.1635, -0.2718, -0.0553],\n",
      "        [-0.3974, -0.4508,  0.3561],\n",
      "        [ 0.3549, -0.2679, -0.0435]], requires_grad=True)\n",
      "tensor([[-0.8689, -0.1350, -0.6070],\n",
      "        [-0.1299, -0.5284, -0.6637],\n",
      "        [-0.4502,  0.3287,  1.4166],\n",
      "        [ 0.8028, -1.2446, -1.7689],\n",
      "        [ 1.0773,  0.6362, -0.2447]])\n",
      "tensor([[-0.1271,  0.2123,  0.1899, -0.2458],\n",
      "        [ 0.1431,  0.2015,  0.0535,  0.1243],\n",
      "        [-0.8871, -0.0940,  0.5352, -0.3094],\n",
      "        [ 0.9798,  0.3048, -0.3879,  0.6952],\n",
      "        [ 0.8344, -0.3355, -0.8021,  0.2225]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(m.weight)\n",
    "print(inputt)\n",
    "print(m(inputt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "careful-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        exps = np.exp(x_in-np.max(x_in, axis=-1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "class Tanh(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.tanh(x_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        # dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(x_in)) ** 2\n",
    "\n",
    "\n",
    "class ReLu(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.maximum(x_in, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        return x_in > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "selected-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        # treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "\n",
    "        # u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            input_part = np.einsum('ij,jk->ik', x_in[:, i, :], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', H[:, i, :], self.hidden_weights.T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def book_forward(self, x_in):\n",
    "\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            # ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki', self.input_weights, x_in[:, i, :].T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', self.hidden_weights, H[:, i, :].T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def backward(self, x, h, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:, self.seq_len - 1, :]\n",
    "\n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            activation_backward = self.activation.backward(h[:, i, :]).reshape(self.batch_size, self.hidden_dim, 1)\n",
    "\n",
    "            dEdW_in += np.sum(activation_backward * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i - 1, :])), axis=0)\n",
    "            dEdW_hh += np.sum(activation_backward * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i - 1, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(h[:, i, :]) * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :], self.hidden_weights) * self.activation.backward(\n",
    "                    h[:, i, :]) + dEdY[:, i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(h[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "\n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        print(f'self.bias={self.bias}')\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:, self.seq_len - 1, :]\n",
    "\n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "\n",
    "            for k in range(self.batch_size):\n",
    "                act_grad = np.diag(self.activation.backward(H[k, i, :]))\n",
    "                h_grad = H_grad[k, i, :].reshape(self.hidden_dim, 1)\n",
    "\n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k, i - 1, :].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k, i - 1, :].reshape(1, self.hidden_dim)))\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:, i, :]) * H_grad[:, i, :], axis=(0))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(H[:, i, :]) + dEdY[:,\n",
    "                                                                                                              i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * self.activation.backward(H[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "better-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnmine = RnnLayer(3, 2, 3, 5, use_bias=False)\n",
    "rnntorch = nn.RNN(3, 2, bias=False, batch_first=True)\n",
    "rnnmine.input_weights = rnntorch.weight_ih_l0.detach().numpy()\n",
    "rnnmine.hidden_weights = rnntorch.weight_hh_l0.detach().numpy()\n",
    "\n",
    "x = torch.randn(5, 3, 3)\n",
    "x_m = x.detach().numpy()\n",
    "h0 = torch.zeros((1,5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "comic-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1871, -1.4239,  0.3179],\n",
      "         [-0.1685, -0.3000,  0.5422],\n",
      "         [-0.9485, -0.4403,  0.4206]],\n",
      "\n",
      "        [[ 1.1973,  0.1080,  0.4636],\n",
      "         [ 0.4488, -0.6182, -1.0582],\n",
      "         [ 0.5242,  0.0701,  0.0330]],\n",
      "\n",
      "        [[-1.5357, -0.4836,  0.7346],\n",
      "         [-0.0235,  0.2072,  0.5150],\n",
      "         [ 0.2925,  1.5732, -0.2112]],\n",
      "\n",
      "        [[-1.1987,  0.3369, -1.6119],\n",
      "         [-0.2858,  1.4730, -0.2773],\n",
      "         [-0.9517, -0.6675, -0.9721]],\n",
      "\n",
      "        [[ 0.8934,  0.0963, -0.8608],\n",
      "         [ 0.0765,  1.4835, -0.6139],\n",
      "         [ 0.1046,  0.2719,  0.9029]]]) [[[ 0.1871342  -1.4239417   0.31785467]\n",
      "  [-0.16854592 -0.30003515  0.54215044]\n",
      "  [-0.9485377  -0.44030938  0.42060366]]\n",
      "\n",
      " [[ 1.1972725   0.10804261  0.4635505 ]\n",
      "  [ 0.4488446  -0.61821663 -1.0582086 ]\n",
      "  [ 0.52422094  0.07005763  0.03298781]]\n",
      "\n",
      " [[-1.535706   -0.48362377  0.73459995]\n",
      "  [-0.02351362  0.20716505  0.5150445 ]\n",
      "  [ 0.29249704  1.5731645  -0.21118361]]\n",
      "\n",
      " [[-1.1987295   0.3369196  -1.611908  ]\n",
      "  [-0.2858458   1.4730457  -0.27725837]\n",
      "  [-0.951713   -0.66752315 -0.9721032 ]]\n",
      "\n",
      " [[ 0.893384    0.09626313 -0.8607675 ]\n",
      "  [ 0.07646935  1.4835321  -0.61390233]\n",
      "  [ 0.10455976  0.271896    0.90294373]]]\n"
     ]
    }
   ],
   "source": [
    "print(x, x_m)\n",
    "\n",
    "outputtorch, _ = rnntorch(x, h0)\n",
    "outmine, _ = rnnmine.forward(x_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "polish-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6995,  0.4176],\n",
      "         [ 0.5998,  0.2281],\n",
      "         [ 0.7656,  0.2997]],\n",
      "\n",
      "        [[-0.3584, -0.0712],\n",
      "         [-0.4717, -0.2834],\n",
      "         [-0.3494, -0.0279]],\n",
      "\n",
      "        [[ 0.8946,  0.6687],\n",
      "         [ 0.3980,  0.1242],\n",
      "         [-0.7973, -0.5998]],\n",
      "\n",
      "        [[-0.4900, -0.5242],\n",
      "         [-0.7639, -0.4490],\n",
      "         [ 0.1207,  0.0573]],\n",
      "\n",
      "        [[-0.7586, -0.5609],\n",
      "         [-0.8823, -0.5290],\n",
      "         [ 0.1609,  0.4728]]], dtype=torch.float64)\n",
      "tensor([[[ 0.6995,  0.4176],\n",
      "         [ 0.5998,  0.2281],\n",
      "         [ 0.7656,  0.2997]],\n",
      "\n",
      "        [[-0.3584, -0.0712],\n",
      "         [-0.4717, -0.2834],\n",
      "         [-0.3494, -0.0279]],\n",
      "\n",
      "        [[ 0.8946,  0.6687],\n",
      "         [ 0.3980,  0.1242],\n",
      "         [-0.7973, -0.5998]],\n",
      "\n",
      "        [[-0.4900, -0.5242],\n",
      "         [-0.7639, -0.4490],\n",
      "         [ 0.1207,  0.0573]],\n",
      "\n",
      "        [[-0.7586, -0.5609],\n",
      "         [-0.8823, -0.5290],\n",
      "         [ 0.1609,  0.4728]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "om = torch.from_numpy(outmine[:,1:,:])\n",
    "print(om)\n",
    "print(outputtorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
