{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "treated-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "standard-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        exps = np.exp(x_in-np.max(x_in, axis=-1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "class Tanh(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.tanh(x_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        # dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (x_in) ** 2\n",
    "\n",
    "\n",
    "class ReLu(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x_in):\n",
    "        return np.maximum(x_in, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x_in):\n",
    "        return x_in > 0\n",
    "    \n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "\n",
    "    def forward(self, y, o):\n",
    "        self.y_pred = Softmax.forward(o)\n",
    "        return np.sum(-y * np.log(self.y_pred + 1e-15))/y.shape[0]\n",
    "\n",
    "    def backward(self, y):\n",
    "        return (self.y_pred - y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "engaging-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(hidden_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        # treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "\n",
    "        # u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            input_part = np.einsum('ij,jk->ik', x_in[:, i, :], self.input_weights.T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', H[:, i, :], self.hidden_weights.T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def book_forward(self, x_in):\n",
    "\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            # ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki', self.input_weights, x_in[:, i, :].T)\n",
    "            hidden_part = np.einsum('ij,jk->ik', self.hidden_weights, H[:, i, :].T)\n",
    "\n",
    "            H[:, i + 1, :] = self.activation.forward(input_part + hidden_part + self.bias)\n",
    "\n",
    "        return H, H[:, self.seq_len, :]\n",
    "\n",
    "    def backward(self, x, h, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "        \n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:,self.seq_len - 1, :]\n",
    "        \n",
    "\n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            \n",
    "            activation_backward = self.activation.backward(h[:, i, :])\n",
    "            back_reshaped = activation_backward.reshape(self.batch_size, self.hidden_dim, 1)\n",
    "            \n",
    "            dEdW_in += np.sum(back_reshaped * (np.einsum('bh,bi->bhi', H_grad[:, i, :], x[:, i - 1, :])), axis=0)\n",
    "            dEdW_hh += np.sum(back_reshaped * (np.einsum('bh,bk->bhk', H_grad[:, i, :], h[:, i - 1, :])), axis=0)\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(h[:, i, :]) * H_grad[:, i, :], axis=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * activation_backward + dEdY[:, i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.einsum('bh,hk->bk', H_grad[:, i, :],\n",
    "                                                self.hidden_weights) * activation_backward\n",
    "\n",
    "    \n",
    "        return dEdW_in, dEdW_hh, dEdB_in, H_grad\n",
    "\n",
    "    def backward_checker(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "\n",
    "        dEdB_in = np.zeros_like(self.bias)\n",
    "\n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:, self.seq_len, :] = dEdY[:, self.seq_len - 1, :]\n",
    "        \n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "\n",
    "            for k in range(self.batch_size):\n",
    "                act_grad = np.diag(self.activation.backward(H[k, i, :]))\n",
    "                h_grad = H_grad[k, i, :].reshape(self.hidden_dim, 1)\n",
    "\n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k, i - 1, :].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k, i - 1, :].reshape(1, self.hidden_dim)))\n",
    "\n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:, i, :]) * H_grad[:, i, :], axis=(0))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i > 1:\n",
    "                H_grad[:, i - 1, :] = np.dot(H_grad[:, i, :],self.hidden_weights) * self.activation.backward(H[:, i, :]) + dEdY[:,i - 2, :]\n",
    "            else:\n",
    "                H_grad[:, i - 1, :] = np.dot(H_grad[:, i, :],self.hidden_weights) * self.activation.backward(H[:, i, :])\n",
    "\n",
    "        return dEdW_in, dEdW_hh, dEdB_in\n",
    "    \n",
    "    \n",
    "class DenseLayer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        sq = np.sqrt(1. / input_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = np.random.uniform(-sq, sq, (output_dim, input_dim))\n",
    "        if use_bias:\n",
    "            self.bias = np.random.uniform(-sq, sq, output_dim)\n",
    "        else:\n",
    "            self.bias = np.zeros(output_dim)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        return np.tensordot(x_in, self.weights.T, axes=((-1), 0)) + self.bias\n",
    "\n",
    "    def backward(self, de_dy, x_in):\n",
    "        # de_dw = de_dy * dYdW = de_dy * X\n",
    "        # dEdb = de_dy * dYdb = de_dy\n",
    "        # dEdX = de_dy * dYdX = de_dy * W\n",
    "        axis = tuple(range(len(x_in.shape) - 1))\n",
    "        de_dw = np.tensordot(de_dy, x_in, axes=(axis, axis))\n",
    "        de_db = np.sum(de_dy, axis=axis)\n",
    "        de_dx = np.tensordot(de_dy, self.weights, axes=(-1, 0))\n",
    "\n",
    "        return de_dx, de_dw, de_db\n",
    "\n",
    "    def refresh(self, de_dw, de_db, learning_rate):\n",
    "        self.weights = self.weights - learning_rate * de_dw\n",
    "        if self.use_bias:\n",
    "            self.bias = self.bias - learning_rate * de_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-sarah",
   "metadata": {},
   "source": [
    "## Softmax, Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aging-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "\n",
    "x = torch.randn(N, num_classes) # logits\n",
    "y = torch.randint(num_classes, (N,)) # labels\n",
    "\n",
    "x_ = x.numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()] # one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regional-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax check: True\n"
     ]
    }
   ],
   "source": [
    "softmax_out = F.softmax(x, dim=-1)\n",
    "softmax_out\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax_out_ = softmax.forward(x_)\n",
    "\n",
    "print('Softmax check:', np.isclose(softmax_out, softmax_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "focused-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss check: True\n"
     ]
    }
   ],
   "source": [
    "cel = nn.CrossEntropyLoss()\n",
    "loss = cel(x, y).item()\n",
    "\n",
    "cel_ = CrossEntropyLoss()\n",
    "loss_ = cel_.forward(y_, x_)\n",
    "loss_\n",
    "\n",
    "print('Cross entropy loss check:', np.isclose(loss, loss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-display",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numerous-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer forward check:  True\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "num_classes = 4\n",
    "seq_len = 3\n",
    "\n",
    "x = torch.randn(N, seq_len, requires_grad=True)\n",
    "y = torch.randint(num_classes, (N,))\n",
    "\n",
    "x_ = x.detach().numpy()\n",
    "y_ = np.eye(num_classes)[y.numpy()]\n",
    "\n",
    "linear = nn.Linear(seq_len, num_classes)\n",
    "linear_ = DenseLayer(seq_len, num_classes)\n",
    "\n",
    "linear_.weights = linear.weight.detach().numpy()\n",
    "linear_.bias = linear.bias.detach().numpy()\n",
    "\n",
    "lin_out = linear(x)\n",
    "lin_out_ = linear_.forward(x_)\n",
    "\n",
    "print('Linear layer forward check: ', np.isclose(lin_out.detach().numpy(), lin_out_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "minor-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TORCH] dE/dy:\n",
      "tensor([[-0.1788,  0.0606,  0.0739,  0.0443],\n",
      "        [ 0.0529,  0.0595,  0.0175, -0.1299],\n",
      "        [ 0.0990,  0.0464, -0.1736,  0.0282],\n",
      "        [-0.1731,  0.0731,  0.0470,  0.0531],\n",
      "        [ 0.0177,  0.0843, -0.1569,  0.0549]])\n",
      "\n",
      "[TORCH] dE/dW:\n",
      "tensor([[ 0.6432,  0.0048,  0.0285],\n",
      "        [-0.1244,  0.0718, -0.0022],\n",
      "        [-0.1545, -0.1517, -0.0556],\n",
      "        [-0.3643,  0.0751,  0.0292]])\n",
      "\n",
      "[TORCH] dE/dB:\n",
      "tensor([-0.1823,  0.3239, -0.1921,  0.0505])\n",
      "\n",
      "[TORCH] dE/dX:\n",
      "tensor([[-0.0961, -0.0593, -0.1335],\n",
      "        [ 0.0058,  0.0971,  0.0600],\n",
      "        [ 0.1024,  0.0162,  0.0155],\n",
      "        [-0.0838, -0.0608, -0.1402],\n",
      "        [ 0.0650, -0.0128, -0.0527]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = cel(lin_out, y)\n",
    "lin_out.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "print(f'[TORCH] dE/dy:\\n{lin_out.grad}\\n')\n",
    "print(f'[TORCH] dE/dW:\\n{linear.weight.grad}\\n')\n",
    "print(f'[TORCH] dE/dB:\\n{linear.bias.grad}\\n')\n",
    "print(f'[TORCH] dE/dX:\\n{x.grad}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gothic-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = cel_.forward(y_, lin_out_)\n",
    "loss_\n",
    "\n",
    "de_dy = cel_.backward(y_)\n",
    "de_dx, de_dw, de_db = linear_.backward(de_dy, x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liked-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dy:\n",
      "[[-0.17883323  0.0606169   0.07393674  0.0442796 ]\n",
      " [ 0.05292938  0.05950016  0.0174878  -0.12991735]\n",
      " [ 0.09900548  0.04641851 -0.17358001  0.02815602]\n",
      " [-0.17313426  0.0730741   0.04697702  0.05308315]\n",
      " [ 0.01770099  0.08429919 -0.15692319  0.05492302]]\n",
      "\n",
      "[CUSTOM] dE/dW:\n",
      "[[ 0.64319172  0.00482929  0.02852225]\n",
      " [-0.12438113  0.07183421 -0.0021866 ]\n",
      " [-0.15453876 -0.15172726 -0.05558313]\n",
      " [-0.36427187  0.07506376  0.02924749]]\n",
      "\n",
      "[CUSTOM] dE/dB:\n",
      "[-0.18233163  0.32390886 -0.19210164  0.05052444]\n",
      "\n",
      "[CUSTOM] dE/dX:\n",
      "[[-0.09606687 -0.05928671 -0.13354808]\n",
      " [ 0.00578424  0.09710226  0.06000554]\n",
      " [ 0.10241285  0.01616698  0.01546191]\n",
      " [-0.08383915 -0.06081502 -0.14015159]\n",
      " [ 0.06502276 -0.0128443  -0.05272639]]\n",
      "\n",
      "Check dE/dy: True\n",
      "Check dE/dX: True\n",
      "Check dE/dW: True\n",
      "Check dE/dB: True\n"
     ]
    }
   ],
   "source": [
    "print(f'[CUSTOM] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[CUSTOM] dE/dW:\\n{de_dw}\\n')\n",
    "print(f'[CUSTOM] dE/dB:\\n{de_db}\\n')\n",
    "print(f'[CUSTOM] dE/dX:\\n{de_dx}\\n')\n",
    "\n",
    "print('Check dE/dy:', np.isclose(lin_out.grad, de_dy).all())\n",
    "print('Check dE/dX:', np.isclose(x.grad, de_dx).all())\n",
    "print('Check dE/dW:', np.isclose(linear.weight.grad, de_dw).all())\n",
    "print('Check dE/dB:', np.isclose(linear.bias.grad, de_db).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-briefs",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "coordinated-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN layer forward check:  True\n",
      "RNN layer forward check last hidden:  True\n"
     ]
    }
   ],
   "source": [
    "#N = 5\n",
    "#emb_dim = 6\n",
    "#seq_len = 3\n",
    "#hidden_dim = 8\n",
    "\n",
    "N = 5\n",
    "emb_dim = 6\n",
    "seq_len = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "\n",
    "x = torch.randn(N, seq_len, emb_dim, requires_grad=True)\n",
    "\n",
    "x_ = x.detach().numpy()\n",
    "\n",
    "rnn = nn.RNN(emb_dim, hidden_dim, bias=False, batch_first=True)\n",
    "rnn_ = RnnLayer(emb_dim, hidden_dim, seq_len, N, use_bias=False)\n",
    "rnn_.input_weights = rnn.weight_ih_l0.detach().numpy()\n",
    "rnn_.hidden_weights = rnn.weight_hh_l0.detach().numpy()\n",
    "\n",
    "rnn_out, h_n = rnn(x)\n",
    "rnn_out_, h_n_ = rnn_.forward(x_)\n",
    "rnn_out__ = rnn_out_[:, 1:, :] # remove zeros prepended to every hidden output\n",
    "\n",
    "print('RNN layer forward check: ', np.isclose(rnn_out.detach().numpy(), rnn_out__).all())\n",
    "print('RNN layer forward check last hidden: ', np.isclose(h_n.detach().numpy(), h_n_).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "surprised-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TORCH] dE/dWih:\n",
      "tensor([[ 1.5632,  1.0950, -0.4356, -0.9051, -0.5419,  1.1497],\n",
      "        [ 3.6365, -3.2831, -3.2712, -3.2545,  0.7571, -1.6923],\n",
      "        [-1.8961, -0.0933,  1.3080,  1.0940,  1.2342, -0.6066],\n",
      "        [ 0.0826, -0.9934, -0.1278,  4.1134, -1.2327,  0.5618],\n",
      "        [-3.4931, -2.2032,  3.2736,  0.8773, -2.4627,  1.8469]])\n",
      "\n",
      "[TORCH] dE/dWhh:\n",
      "tensor([[-1.6476,  0.5321, -0.9045, -0.2720,  1.3781],\n",
      "        [ 2.0258, -1.3684,  0.7980,  1.7109, -1.7800],\n",
      "        [-0.9835,  0.2355,  0.1630, -0.1145,  0.3965],\n",
      "        [ 1.4600, -0.2265, -0.3678, -1.5220,  0.7827],\n",
      "        [ 0.0067, -1.2813,  0.6293,  1.6404, -0.5236]])\n",
      "\n",
      "[TORCH] dE/dy:\n",
      "tensor([[[ 0.3413, -0.5822, -0.7120,  0.6790,  0.0666],\n",
      "         [ 1.4542,  1.0778, -1.0873, -0.4267, -1.2978],\n",
      "         [-0.7192, -1.3991, -0.6228,  0.9242,  1.2328]],\n",
      "\n",
      "        [[-0.8214,  1.8177, -0.7652,  1.4600,  0.6283],\n",
      "         [ 0.0898, -2.4753, -0.2170,  0.2810, -1.3683],\n",
      "         [-0.1085, -0.2659, -0.8700,  0.3504, -1.1689]],\n",
      "\n",
      "        [[ 1.2876,  0.3426, -0.5702,  0.1755, -0.4855],\n",
      "         [ 0.0400,  0.3988,  0.0745, -0.4096,  1.0581],\n",
      "         [-1.0882,  1.4621, -0.1467,  1.1866,  0.4832]],\n",
      "\n",
      "        [[-0.8845,  1.3315, -0.7308, -1.6760,  1.2480],\n",
      "         [ 0.6129, -0.8395, -0.0189, -0.3082, -0.3225],\n",
      "         [-2.5475,  1.0408, -0.0484,  1.2282, -0.9485]],\n",
      "\n",
      "        [[-0.8320,  1.2423, -1.8611,  1.3935,  0.0627],\n",
      "         [ 0.5447, -0.3186, -1.3441, -0.7116,  0.8054],\n",
      "         [ 1.3127, -1.3813,  0.2935,  0.8563,  0.0585]]])\n",
      "\n",
      "[TORCH] dE/dH:\n",
      "tensor([[[ 0.3413, -0.5822, -0.7120,  0.6790,  0.0666],\n",
      "         [ 1.4542,  1.0778, -1.0873, -0.4267, -1.2978],\n",
      "         [-0.7192, -1.3991, -0.6228,  0.9242,  1.2328]],\n",
      "\n",
      "        [[-0.8214,  1.8177, -0.7652,  1.4600,  0.6283],\n",
      "         [ 0.0898, -2.4753, -0.2170,  0.2810, -1.3683],\n",
      "         [-0.1085, -0.2659, -0.8700,  0.3504, -1.1689]],\n",
      "\n",
      "        [[ 1.2876,  0.3426, -0.5702,  0.1755, -0.4855],\n",
      "         [ 0.0400,  0.3988,  0.0745, -0.4096,  1.0581],\n",
      "         [-1.0882,  1.4621, -0.1467,  1.1866,  0.4832]],\n",
      "\n",
      "        [[-0.8845,  1.3315, -0.7308, -1.6760,  1.2480],\n",
      "         [ 0.6129, -0.8395, -0.0189, -0.3082, -0.3225],\n",
      "         [-2.5475,  1.0408, -0.0484,  1.2282, -0.9485]],\n",
      "\n",
      "        [[-0.8320,  1.2423, -1.8611,  1.3935,  0.0627],\n",
      "         [ 0.5447, -0.3186, -1.3441, -0.7116,  0.8054],\n",
      "         [ 1.3127, -1.3813,  0.2935,  0.8563,  0.0585]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "de_dy = torch.randn(N, seq_len, hidden_dim)\n",
    "de_dy_ = de_dy.numpy()\n",
    "\n",
    "rnn_out.retain_grad()\n",
    "rnn_out.backward(de_dy)\n",
    "\n",
    "print(f'[TORCH] dE/dWih:\\n{rnn.weight_ih_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dWhh:\\n{rnn.weight_hh_l0.grad}\\n')\n",
    "print(f'[TORCH] dE/dy:\\n{de_dy}\\n')\n",
    "print(f'[TORCH] dE/dH:\\n{rnn_out.grad}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fourth-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUSTOM] dE/dWih:\n",
      "[[ 2.1704273   0.80655396 -0.09895559 -2.1960852  -0.11273439  1.4121459 ]\n",
      " [ 4.2564716  -3.3818326  -2.3150346  -4.151105    0.9410493  -2.1321788 ]\n",
      " [-1.9509997  -1.5498486   1.4614779  -0.16966856  0.7720219  -0.7037502 ]\n",
      " [ 0.5028647  -1.0100005   0.544974    3.7390335  -1.0582447   0.28043902]\n",
      " [-3.0213704  -2.2303386   2.9346206   0.82597715 -2.3038645   0.96782213]]\n",
      "\n",
      "[CUSTOM] dE/dWhh:\n",
      "[[-1.8016164   0.69000906 -1.0969379  -0.02718778  1.4229884 ]\n",
      " [ 1.7044684  -0.9310839   0.7106786   2.1803424  -1.7890654 ]\n",
      " [-0.34787923 -0.4033661   0.47439235 -0.1280304   0.04756025]\n",
      " [ 1.0414937   0.22979382 -0.34883544 -1.0666538   0.7155496 ]\n",
      " [ 0.22342995 -1.3912995   0.52715844  1.5331726  -0.42903402]]\n",
      "\n",
      "[CUSTOM] dE/dH:\n",
      "[[[ 0.62494165 -0.55459172 -0.28033203  1.00214668 -0.27574827]\n",
      "  [ 1.19031345  0.33171065 -0.99559586 -0.96420243 -0.70657765]\n",
      "  [-0.7191968  -1.39909518 -0.62279123  0.9242416   1.23278213]]\n",
      "\n",
      " [[-0.18648617  1.57574717 -0.05813043  1.71692212  1.57767315]\n",
      "  [ 0.20969913 -2.27117378 -0.20996425  0.56526862 -0.54694728]\n",
      "  [-0.10852648 -0.26593593 -0.87001717  0.35040766 -1.16885531]]\n",
      "\n",
      " [[ 0.83187438  0.24053414 -0.61652077 -0.96794606 -1.11257193]\n",
      "  [-1.03510659  0.65607161 -0.1477217  -1.19324568  1.33536477]\n",
      "  [-1.08822286  1.46208763 -0.14669025  1.1866169   0.48316348]]\n",
      "\n",
      " [[-1.01760483  1.22157542 -0.57477803 -1.81427547  1.29391034]\n",
      "  [ 0.19890326  0.07346827 -1.00684183 -0.32710375 -0.14151049]\n",
      "  [-2.54745102  1.0407747  -0.04841707  1.22820568 -0.94846749]]\n",
      "\n",
      " [[-0.8831011   0.51768363 -1.74639872  1.15127267  0.02450909]\n",
      "  [ 0.71552088 -0.87657642 -0.89439625 -0.43032086  0.91524771]\n",
      "  [ 1.31267607 -1.38132727  0.29348511  0.85634613  0.05853518]]]\n",
      "\n",
      "RNN layer gradient check dEdW_in:  False\n",
      "RNN layer gradient check dEdW_hh:  False\n",
      "RNN layer gradient check dEdH:  False\n"
     ]
    }
   ],
   "source": [
    "dEdW_in, dEdW_hh, _, H_grad = rnn_.backward(x_, rnn_out_, de_dy_)\n",
    "\n",
    "print(f'[CUSTOM] dE/dWih:\\n{dEdW_in}\\n')\n",
    "print(f'[CUSTOM] dE/dWhh:\\n{dEdW_hh}\\n')\n",
    "print(f'[CUSTOM] dE/dH:\\n{H_grad[:,1:,:]}\\n')\n",
    "\n",
    "print('RNN layer gradient check dEdW_in: ', np.isclose(rnn.weight_ih_l0.grad.numpy(), dEdW_in).all())\n",
    "print('RNN layer gradient check dEdW_hh: ', np.isclose(rnn.weight_hh_l0.grad.numpy(), dEdW_hh).all())\n",
    "print('RNN layer gradient check dEdH: ', np.isclose(rnn_out.grad.numpy(), H_grad[:,1:,:]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-registrar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-warning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
