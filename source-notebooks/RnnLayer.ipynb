{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "valuable-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "obvious-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(object):\n",
    "    \n",
    "    def forward(self, X_in):\n",
    "        return np.tanh(X_in)\n",
    "    \n",
    "    def backward(self, X_in):\n",
    "        #dEdX = dEdY * dYdX = dEdY * 1 - (tanh(X))^2\n",
    "        return 1 - (np.tanh(X_in))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accompanied-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu(object):\n",
    "    \n",
    "    def forward(self, X_in):\n",
    "        return np.maximum(X_in, 0)\n",
    "    \n",
    "    def backward(self, X_in):\n",
    "        dYdX = (X_in > 0)  \n",
    "        return dYdX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "retained-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnLayer(object):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, batch_size, use_bias=True, activation=Tanh):\n",
    "        sq = np.sqrt(1. / hidden_dim)\n",
    "        self.use_bias = use_bias\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation()\n",
    "        self.input_weights = np.random.uniform(-sq, sq, (hidden_dim, input_dim))\n",
    "        self.hidden_weights = np.random.uniform(-sq, sq, (hidden_dim, hidden_dim))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.hidden_bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "            self.input_bias = np.random.uniform(-sq, sq, hidden_dim)\n",
    "        else:\n",
    "            self.hidden_bias = np.zeros((hidden_dim))\n",
    "            self.input_bias = np.zeros((hidden_dim))\n",
    "        \n",
    "    def forward(self, X_in):        \n",
    "        #treba li dodati provjeru je li X_in stvarno ima sekvencu jednaku seq_len?\n",
    "        #treba li dodati provjeru je li X_in prva koordinata jednaka batch_size\n",
    "        \n",
    "        #u ovom slucaju sam pretpostavio da je za sve inpute, pocetno stanje 0 u 0. vremenskom trenutku\n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim)) \n",
    "        \n",
    "        for i in range(self.seq_len):\n",
    "            \n",
    "            input_part = np.einsum('ij,jk->ik', X_in[:,i,:], self.input_weights.T) + self.input_bias\n",
    "            hidden_part = np.einsum('ij,jj->ij', H[:,i,:], self.hidden_weights.T) + self.hidden_bias\n",
    "            \n",
    "            H[:,i+1,:] = self.activation.forward(input_part + hidden_part)\n",
    "       \n",
    "        return H, H[:,self.seq_len,:]\n",
    "    \n",
    "    def book_forward(self, X_in):\n",
    "        \n",
    "        H = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim)) \n",
    "        \n",
    "        for i in range(self.seq_len):\n",
    "            #ovdje dobivam transponirano iz mog forwarda, ali sam u einsum zamijenio vrijednosti, tako da zapravo dobijem isto\n",
    "            input_part = np.einsum('ij,jk->ki',self.input_weights, X_in[:,i,:].T) + self.input_bias\n",
    "            hidden_part = np.einsum('ii,ij->ji',self.hidden_weights, H[:,i,:].T) + self.hidden_bias\n",
    "            \n",
    "            H[:,i+1,:] = self.activation.forward(input_part + hidden_part)\n",
    "       \n",
    "        return H, H[:,self.seq_len,:]\n",
    "        \n",
    "            \n",
    "    def backward(self, X, H, dEdY):\n",
    "        dEdW_in = np.zeros_like(self.input_weights)\n",
    "        dEdW_hh = np.zeros_like(self.hidden_weights)\n",
    "        \n",
    "        print(f'self.hiddan_bias={self.hidden_bias}')\n",
    "        print(f'self.input_bias={self.input_bias}')\n",
    "        \n",
    "        dEdB_in = np.zeros_like(self.input_bias)\n",
    "        dEdB_h = np.zeros_like(self.hidden_bias)\n",
    "        \n",
    "        H_grad = np.zeros((self.batch_size, self.seq_len + 1, self.hidden_dim))\n",
    "        H_grad[:,self.seq_len,:] = dEdY[:,self.seq_len - 1,:]\n",
    "        \n",
    "        for i in range(self.seq_len, 0, -1):\n",
    "            \n",
    "            #ovo pitaj!!!\n",
    "            #ako ovako racunam, onda imam problem jer odjednom poracunam doprinos svakog primjera, a tek kasnije moram nadodati\n",
    "            #gradijent aktivacijske funkcije\n",
    "            #dEdW_in += np.einsum('bh,bi->hi', H_grad[:,i,:], X[:,i-1,:])\n",
    "            #dEdW_hh += np.einsum('bh,bk->hk', H_grad[:,i,:], H[:,i-1,:])\n",
    "            \n",
    "            #onda je ovo drugi pristup, treba provjerit s Josipom jel ok\n",
    "            for k in range (self.batch_size):\n",
    "                act_grad = np.diag(self.activation.backward(H[k,i,:]))\n",
    "                h_grad = H_grad[k,i,:].reshape(self.hidden_dim, 1)\n",
    "                \n",
    "                dEdW_in += np.dot(act_grad, np.dot(h_grad, X[k,i-1,:].reshape(1, self.input_dim)))\n",
    "                dEdW_hh += np.dot(act_grad, np.dot(h_grad, H[k,i-1,:].reshape(1, self.hidden_dim)))\n",
    "            \n",
    "            if self.use_bias:\n",
    "                dEdB_in += np.sum(self.activation.backward(H[:,i,:]) * H_grad[:,i,:], axis=(0))\n",
    "                #mislim da ovdje nije potrebno imati oba biasa, mislim na početku se random postave,\n",
    "                #ali ovdje uvijek računamo iste vrijednosti\n",
    "                dEdB_h = dEdB_in\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            #ovo pitaj !!!!\n",
    "            \n",
    "            if i > 1:\n",
    "                H_grad[:,i-1,:] = np.einsum('bh,hh->bh', H_grad[:,i,:], self.hidden_weights) * self.activation.backward(H[:,i,:]) + dEdY[:,i-2,:]\n",
    "            else:\n",
    "                H_grad[:,i-1,:] = np.einsum('bh,hh->bh', H_grad[:,i,:], self.hidden_weights) * self.activation.backward(H[:,i,:])\n",
    "        \n",
    "        return dEdW_in, dEdW_hh, dEdB_in, dEdB_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "endangered-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn forward checker\n",
    "\n",
    "rnn = RnnLayer(4, 5, 3, 2)\n",
    "#input dim 4\n",
    "#hidden dim 5\n",
    "#batch 2\n",
    "#timestamps 3\n",
    "\n",
    "X_in = np.array([[[1,2,1,3],[2,2,3,1],[0,2,3,1]],[[1,3,4,3],[1,2,1,1],[1,0,1,2]]])\n",
    "H, last = rnn.forward(X_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "tight-jesus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.hiddan_bias=[-0.29658073 -0.35963977 -0.03723587  0.21223004  0.02764134]\n",
      "self.input_bias=[-0.27178528  0.08742101 -0.04558765 -0.20821289  0.26399787]\n"
     ]
    }
   ],
   "source": [
    "dEdY = np.array([[[ 0.34545989,  0.07336296, -0.16346513, -0.06904482,\n",
    "          0.0458759 ],\n",
    "        [ 0.37271336,  0.07915059, -0.17636096, -0.07449179,\n",
    "          0.04949507],\n",
    "        [ 0.35166208,  0.07468007, -0.16639989, -0.07028441,\n",
    "          0.04669953]],\n",
    "\n",
    "       [[ 0.36616935,  0.07776088, -0.17326446, -0.07318388,\n",
    "          0.04862605],\n",
    "        [ 0.33954613,  0.07210709, -0.16066685, -0.06786287,\n",
    "          0.04509058],\n",
    "        [ 0.35872758,  0.07618053, -0.16974315, -0.07169654,\n",
    "          0.04763781]]])\n",
    "\n",
    "Win, Wh, Bin, Bh = rnn.backward(X_in, H, dEdY)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "small-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin=[ 2.11390974  0.34241985 -0.57601489 -0.32735228  0.16412215]\n",
      "Bh=[ 2.11390974  0.34241985 -0.57601489 -0.32735228  0.16412215]\n",
      "Win=(5, 4)\n",
      "Wh=(5, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f'Bin={Bin}')\n",
    "print(f'Bh={Bh}')\n",
    "print(f'Win={Win.shape}')\n",
    "print(f'Wh={Wh.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "electric-edgar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [0, 0, 3]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(np.array([1,2,3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
